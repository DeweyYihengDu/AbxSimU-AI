{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SMILES->Graph: 100%|██████████| 13524/13524 [00:13<00:00, 992.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feat dim = 47 | Edge feat dim = 10 | N graphs = 13524\n",
      "[AE] Epoch 001 | recon MSE 0.02076 | lr 1.00e-03\n",
      "[AE] Epoch 002 | recon MSE 0.00463 | lr 1.00e-03\n",
      "[AE] Epoch 003 | recon MSE 0.00259 | lr 1.00e-03\n",
      "[AE] Epoch 004 | recon MSE 0.00179 | lr 1.00e-03\n",
      "[AE] Epoch 005 | recon MSE 0.00137 | lr 1.00e-03\n",
      "[AE] Epoch 006 | recon MSE 0.00114 | lr 1.00e-03\n",
      "[AE] Epoch 007 | recon MSE 0.00101 | lr 1.00e-03\n",
      "[AE] Epoch 008 | recon MSE 0.00093 | lr 1.00e-03\n",
      "[AE] Epoch 009 | recon MSE 0.00088 | lr 1.00e-03\n",
      "[AE] Epoch 010 | recon MSE 0.00094 | lr 1.00e-03\n",
      "[AE] Epoch 011 | recon MSE 0.00084 | lr 1.00e-03\n",
      "[AE] Epoch 012 | recon MSE 0.00081 | lr 1.00e-03\n",
      "[AE] Epoch 013 | recon MSE 0.00085 | lr 1.00e-03\n",
      "[AE] Epoch 014 | recon MSE 0.00078 | lr 1.00e-03\n",
      "[AE] Epoch 015 | recon MSE 0.00076 | lr 1.00e-03\n",
      "[AE] Epoch 016 | recon MSE 0.00076 | lr 1.00e-03\n",
      "[AE] Epoch 017 | recon MSE 0.00076 | lr 1.00e-03\n",
      "[AE] Epoch 018 | recon MSE 0.00075 | lr 1.00e-03\n",
      "[AE] Epoch 019 | recon MSE 0.00074 | lr 1.00e-03\n",
      "[AE] Epoch 020 | recon MSE 0.00074 | lr 1.00e-03\n",
      "[AE] Epoch 021 | recon MSE 0.00086 | lr 1.00e-03\n",
      "[AE] Epoch 022 | recon MSE 0.00074 | lr 1.00e-03\n",
      "[AE] Epoch 023 | recon MSE 0.00072 | lr 1.00e-03\n",
      "[AE] Epoch 024 | recon MSE 0.00072 | lr 1.00e-03\n",
      "[AE] Epoch 025 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 026 | recon MSE 0.00072 | lr 1.00e-03\n",
      "[AE] Epoch 027 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 028 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 029 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 030 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 031 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 032 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 033 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 034 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 035 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 036 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 037 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 038 | recon MSE 0.00073 | lr 1.00e-03\n",
      "[AE] Epoch 039 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 040 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 041 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 042 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 043 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 044 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 045 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 046 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 047 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 048 | recon MSE 0.00075 | lr 1.00e-03\n",
      "[AE] Epoch 049 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 050 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 051 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 052 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 053 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 054 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 055 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 056 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 057 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 058 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 059 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 060 | recon MSE 0.00067 | lr 1.00e-03\n",
      "After undersampling by 'nearest' (cosine) similarity: pos=470 neg=470 total=940\n",
      "[Fold 1] Ep 001 | train loss 0.6449 | val AP 0.9519 | lr 2.00e-03\n",
      "[Fold 1] Ep 002 | train loss 0.4425 | val AP 0.8957 | lr 2.00e-03\n",
      "[Fold 1] Ep 003 | train loss 0.3317 | val AP 0.9455 | lr 2.00e-03\n",
      "[Fold 1] Ep 004 | train loss 0.3259 | val AP 0.9562 | lr 2.00e-03\n",
      "[Fold 1] Ep 005 | train loss 0.2732 | val AP 0.9662 | lr 2.00e-03\n",
      "[Fold 1] Ep 006 | train loss 0.2855 | val AP 0.9499 | lr 2.00e-03\n",
      "[Fold 1] Ep 007 | train loss 0.2741 | val AP 0.8777 | lr 2.00e-03\n",
      "[Fold 1] Ep 008 | train loss 0.2363 | val AP 0.9364 | lr 2.00e-03\n",
      "[Fold 1] Ep 009 | train loss 0.2535 | val AP 0.9432 | lr 2.00e-03\n",
      "[Fold 1] Ep 010 | train loss 0.2505 | val AP 0.9241 | lr 2.00e-03\n",
      "[Fold 1] Ep 011 | train loss 0.2229 | val AP 0.9244 | lr 1.00e-03\n",
      "[Fold 1] Ep 012 | train loss 0.2201 | val AP 0.9361 | lr 1.00e-03\n",
      "[Fold 1] Ep 013 | train loss 0.1992 | val AP 0.9430 | lr 1.00e-03\n",
      "[Fold 1] Ep 014 | train loss 0.1986 | val AP 0.9497 | lr 1.00e-03\n",
      "[Fold 1] Ep 015 | train loss 0.1619 | val AP 0.9541 | lr 1.00e-03\n",
      "[Fold 1] Ep 016 | train loss 0.1580 | val AP 0.9542 | lr 1.00e-03\n",
      "[Fold 1] Ep 017 | train loss 0.1769 | val AP 0.9618 | lr 5.00e-04\n",
      "[Fold 1] Ep 018 | train loss 0.1558 | val AP 0.9534 | lr 5.00e-04\n",
      "[Fold 1] Ep 019 | train loss 0.1381 | val AP 0.9565 | lr 5.00e-04\n",
      "[Fold 1] Ep 020 | train loss 0.1336 | val AP 0.9567 | lr 5.00e-04\n",
      "[Fold 1] Ep 021 | train loss 0.1621 | val AP 0.9548 | lr 5.00e-04\n",
      "[Fold 1] Ep 022 | train loss 0.1483 | val AP 0.9522 | lr 5.00e-04\n",
      "[Fold 1] Ep 023 | train loss 0.1373 | val AP 0.9527 | lr 2.50e-04\n",
      "[Fold 1] Ep 024 | train loss 0.1340 | val AP 0.9570 | lr 2.50e-04\n",
      "[Fold 1] Ep 025 | train loss 0.1249 | val AP 0.9455 | lr 2.50e-04\n",
      "[Fold 1] Ep 026 | train loss 0.1189 | val AP 0.9559 | lr 2.50e-04\n",
      "[Fold 1] Ep 027 | train loss 0.1053 | val AP 0.9530 | lr 2.50e-04\n",
      "[Fold 1] Ep 028 | train loss 0.1253 | val AP 0.9569 | lr 2.50e-04\n",
      "[Fold 1] Ep 029 | train loss 0.1133 | val AP 0.9514 | lr 1.25e-04\n",
      "[Fold 1] Ep 030 | train loss 0.0999 | val AP 0.9554 | lr 1.25e-04\n",
      "[Fold 1] Early stop.\n",
      "[Fold 1] Test metrics: {'ROC-AUC': 0.9580126754187415, 'PRC-AUC': 0.9497681064353753, 'Accuracy': 0.9361702127659575, 'F1': 0.9361702127659575, 'MCC': 0.8723404255319149, 'Recall': 0.9361702127659575, 'Precision': 0.9361702127659575, 'False Positives': 6, 'False Positive Rate': np.float64(0.06382978723404255)}\n",
      "[Fold 2] Ep 001 | train loss 0.6479 | val AP 0.6986 | lr 2.00e-03\n",
      "[Fold 2] Ep 002 | train loss 0.4665 | val AP 0.8958 | lr 2.00e-03\n",
      "[Fold 2] Ep 003 | train loss 0.3352 | val AP 0.8958 | lr 2.00e-03\n",
      "[Fold 2] Ep 004 | train loss 0.2670 | val AP 0.9216 | lr 2.00e-03\n",
      "[Fold 2] Ep 005 | train loss 0.2964 | val AP 0.7691 | lr 2.00e-03\n",
      "[Fold 2] Ep 006 | train loss 0.3085 | val AP 0.9668 | lr 2.00e-03\n",
      "[Fold 2] Ep 007 | train loss 0.2368 | val AP 0.9019 | lr 2.00e-03\n",
      "[Fold 2] Ep 008 | train loss 0.2654 | val AP 0.9266 | lr 2.00e-03\n",
      "[Fold 2] Ep 009 | train loss 0.2212 | val AP 0.9488 | lr 2.00e-03\n",
      "[Fold 2] Ep 010 | train loss 0.2184 | val AP 0.8884 | lr 2.00e-03\n",
      "[Fold 2] Ep 011 | train loss 0.2155 | val AP 0.9496 | lr 2.00e-03\n",
      "[Fold 2] Ep 012 | train loss 0.2299 | val AP 0.9620 | lr 1.00e-03\n",
      "[Fold 2] Ep 013 | train loss 0.2006 | val AP 0.9610 | lr 1.00e-03\n",
      "[Fold 2] Ep 014 | train loss 0.2152 | val AP 0.9507 | lr 1.00e-03\n",
      "[Fold 2] Ep 015 | train loss 0.1580 | val AP 0.9728 | lr 1.00e-03\n",
      "[Fold 2] Ep 016 | train loss 0.1574 | val AP 0.9628 | lr 1.00e-03\n",
      "[Fold 2] Ep 017 | train loss 0.1728 | val AP 0.9623 | lr 1.00e-03\n",
      "[Fold 2] Ep 018 | train loss 0.1484 | val AP 0.9545 | lr 1.00e-03\n",
      "[Fold 2] Ep 019 | train loss 0.1634 | val AP 0.9594 | lr 1.00e-03\n",
      "[Fold 2] Ep 020 | train loss 0.1315 | val AP 0.9454 | lr 1.00e-03\n",
      "[Fold 2] Ep 021 | train loss 0.1371 | val AP 0.9691 | lr 5.00e-04\n",
      "[Fold 2] Ep 022 | train loss 0.1203 | val AP 0.9686 | lr 5.00e-04\n",
      "[Fold 2] Ep 023 | train loss 0.1161 | val AP 0.9587 | lr 5.00e-04\n",
      "[Fold 2] Ep 024 | train loss 0.1009 | val AP 0.9706 | lr 5.00e-04\n",
      "[Fold 2] Ep 025 | train loss 0.1031 | val AP 0.9647 | lr 5.00e-04\n",
      "[Fold 2] Ep 026 | train loss 0.1054 | val AP 0.9707 | lr 5.00e-04\n",
      "[Fold 2] Ep 027 | train loss 0.0870 | val AP 0.9718 | lr 2.50e-04\n",
      "[Fold 2] Ep 028 | train loss 0.0992 | val AP 0.9735 | lr 2.50e-04\n",
      "[Fold 2] Ep 029 | train loss 0.0931 | val AP 0.9731 | lr 2.50e-04\n",
      "[Fold 2] Ep 030 | train loss 0.0876 | val AP 0.9731 | lr 2.50e-04\n",
      "[Fold 2] Ep 031 | train loss 0.0803 | val AP 0.9739 | lr 2.50e-04\n",
      "[Fold 2] Ep 032 | train loss 0.0984 | val AP 0.9735 | lr 2.50e-04\n",
      "[Fold 2] Ep 033 | train loss 0.0874 | val AP 0.9708 | lr 2.50e-04\n",
      "[Fold 2] Ep 034 | train loss 0.0771 | val AP 0.9740 | lr 2.50e-04\n",
      "[Fold 2] Ep 035 | train loss 0.0799 | val AP 0.9754 | lr 2.50e-04\n",
      "[Fold 2] Ep 036 | train loss 0.0735 | val AP 0.9720 | lr 2.50e-04\n",
      "[Fold 2] Ep 037 | train loss 0.0951 | val AP 0.9715 | lr 2.50e-04\n",
      "[Fold 2] Ep 038 | train loss 0.0813 | val AP 0.9715 | lr 2.50e-04\n",
      "[Fold 2] Ep 039 | train loss 0.0810 | val AP 0.9706 | lr 2.50e-04\n",
      "[Fold 2] Ep 040 | train loss 0.0672 | val AP 0.9766 | lr 2.50e-04\n",
      "[Fold 2] Ep 041 | train loss 0.0739 | val AP 0.9719 | lr 2.50e-04\n",
      "[Fold 2] Ep 042 | train loss 0.0736 | val AP 0.9686 | lr 2.50e-04\n",
      "[Fold 2] Ep 043 | train loss 0.0969 | val AP 0.9736 | lr 2.50e-04\n",
      "[Fold 2] Ep 044 | train loss 0.0915 | val AP 0.9699 | lr 2.50e-04\n",
      "[Fold 2] Ep 045 | train loss 0.0668 | val AP 0.9707 | lr 2.50e-04\n",
      "[Fold 2] Ep 046 | train loss 0.0859 | val AP 0.9743 | lr 1.25e-04\n",
      "[Fold 2] Ep 047 | train loss 0.0716 | val AP 0.9723 | lr 1.25e-04\n",
      "[Fold 2] Ep 048 | train loss 0.0686 | val AP 0.9693 | lr 1.25e-04\n",
      "[Fold 2] Ep 049 | train loss 0.0688 | val AP 0.9707 | lr 1.25e-04\n",
      "[Fold 2] Ep 050 | train loss 0.0643 | val AP 0.9686 | lr 1.25e-04\n",
      "[Fold 2] Ep 051 | train loss 0.0749 | val AP 0.9709 | lr 1.25e-04\n",
      "[Fold 2] Ep 052 | train loss 0.0799 | val AP 0.9777 | lr 1.25e-04\n",
      "[Fold 2] Ep 053 | train loss 0.0682 | val AP 0.9776 | lr 1.25e-04\n",
      "[Fold 2] Ep 054 | train loss 0.0723 | val AP 0.9720 | lr 1.25e-04\n",
      "[Fold 2] Ep 055 | train loss 0.0771 | val AP 0.9699 | lr 1.25e-04\n",
      "[Fold 2] Ep 056 | train loss 0.0539 | val AP 0.9722 | lr 1.25e-04\n",
      "[Fold 2] Ep 057 | train loss 0.0827 | val AP 0.9748 | lr 1.25e-04\n",
      "[Fold 2] Ep 058 | train loss 0.0634 | val AP 0.9755 | lr 6.25e-05\n",
      "[Fold 2] Ep 059 | train loss 0.0796 | val AP 0.9743 | lr 6.25e-05\n",
      "[Fold 2] Ep 060 | train loss 0.0713 | val AP 0.9732 | lr 6.25e-05\n",
      "[Fold 2] Ep 061 | train loss 0.0738 | val AP 0.9731 | lr 6.25e-05\n",
      "[Fold 2] Ep 062 | train loss 0.0727 | val AP 0.9740 | lr 6.25e-05\n",
      "[Fold 2] Ep 063 | train loss 0.0596 | val AP 0.9749 | lr 6.25e-05\n",
      "[Fold 2] Ep 064 | train loss 0.0925 | val AP 0.9749 | lr 3.13e-05\n",
      "[Fold 2] Ep 065 | train loss 0.0839 | val AP 0.9739 | lr 3.13e-05\n",
      "[Fold 2] Ep 066 | train loss 0.0654 | val AP 0.9747 | lr 3.13e-05\n",
      "[Fold 2] Ep 067 | train loss 0.0687 | val AP 0.9747 | lr 3.13e-05\n",
      "[Fold 2] Early stop.\n",
      "[Fold 2] Test metrics: {'ROC-AUC': 0.9425079221367134, 'PRC-AUC': 0.954169996040974, 'Accuracy': 0.851063829787234, 'F1': 0.8372093023255814, 'MCC': 0.7125253031944253, 'Recall': 0.7659574468085106, 'Precision': 0.9230769230769231, 'False Positives': 6, 'False Positive Rate': np.float64(0.06382978723404255)}\n",
      "[Fold 3] Ep 001 | train loss 0.6285 | val AP 0.8954 | lr 2.00e-03\n",
      "[Fold 3] Ep 002 | train loss 0.4083 | val AP 0.9029 | lr 2.00e-03\n",
      "[Fold 3] Ep 003 | train loss 0.3126 | val AP 0.9378 | lr 2.00e-03\n",
      "[Fold 3] Ep 004 | train loss 0.3185 | val AP 0.8520 | lr 2.00e-03\n",
      "[Fold 3] Ep 005 | train loss 0.2617 | val AP 0.8503 | lr 2.00e-03\n",
      "[Fold 3] Ep 006 | train loss 0.3288 | val AP 0.8846 | lr 2.00e-03\n",
      "[Fold 3] Ep 007 | train loss 0.2449 | val AP 0.9154 | lr 2.00e-03\n",
      "[Fold 3] Ep 008 | train loss 0.2380 | val AP 0.9389 | lr 2.00e-03\n",
      "[Fold 3] Ep 009 | train loss 0.2591 | val AP 0.9077 | lr 2.00e-03\n",
      "[Fold 3] Ep 010 | train loss 0.2110 | val AP 0.9339 | lr 2.00e-03\n",
      "[Fold 3] Ep 011 | train loss 0.2153 | val AP 0.9544 | lr 2.00e-03\n",
      "[Fold 3] Ep 012 | train loss 0.2114 | val AP 0.9403 | lr 2.00e-03\n",
      "[Fold 3] Ep 013 | train loss 0.1849 | val AP 0.9577 | lr 2.00e-03\n",
      "[Fold 3] Ep 014 | train loss 0.1886 | val AP 0.9498 | lr 2.00e-03\n",
      "[Fold 3] Ep 015 | train loss 0.2290 | val AP 0.9305 | lr 2.00e-03\n",
      "[Fold 3] Ep 016 | train loss 0.1772 | val AP 0.9399 | lr 2.00e-03\n",
      "[Fold 3] Ep 017 | train loss 0.1899 | val AP 0.9475 | lr 2.00e-03\n",
      "[Fold 3] Ep 018 | train loss 0.1652 | val AP 0.9409 | lr 2.00e-03\n",
      "[Fold 3] Ep 019 | train loss 0.1560 | val AP 0.9158 | lr 1.00e-03\n",
      "[Fold 3] Ep 020 | train loss 0.1483 | val AP 0.9631 | lr 1.00e-03\n",
      "[Fold 3] Ep 021 | train loss 0.1763 | val AP 0.9424 | lr 1.00e-03\n",
      "[Fold 3] Ep 022 | train loss 0.1277 | val AP 0.9547 | lr 1.00e-03\n",
      "[Fold 3] Ep 023 | train loss 0.1404 | val AP 0.9562 | lr 1.00e-03\n",
      "[Fold 3] Ep 024 | train loss 0.1186 | val AP 0.9336 | lr 1.00e-03\n",
      "[Fold 3] Ep 025 | train loss 0.1177 | val AP 0.9502 | lr 1.00e-03\n",
      "[Fold 3] Ep 026 | train loss 0.1036 | val AP 0.9379 | lr 5.00e-04\n",
      "[Fold 3] Ep 027 | train loss 0.1100 | val AP 0.9503 | lr 5.00e-04\n",
      "[Fold 3] Ep 028 | train loss 0.1002 | val AP 0.9389 | lr 5.00e-04\n",
      "[Fold 3] Ep 029 | train loss 0.0851 | val AP 0.9344 | lr 5.00e-04\n",
      "[Fold 3] Ep 030 | train loss 0.1061 | val AP 0.9339 | lr 5.00e-04\n",
      "[Fold 3] Ep 031 | train loss 0.0906 | val AP 0.9401 | lr 5.00e-04\n",
      "[Fold 3] Ep 032 | train loss 0.0916 | val AP 0.9352 | lr 2.50e-04\n",
      "[Fold 3] Ep 033 | train loss 0.0765 | val AP 0.9404 | lr 2.50e-04\n",
      "[Fold 3] Ep 034 | train loss 0.0852 | val AP 0.9426 | lr 2.50e-04\n",
      "[Fold 3] Ep 035 | train loss 0.0980 | val AP 0.9401 | lr 2.50e-04\n",
      "[Fold 3] Early stop.\n",
      "[Fold 3] Test metrics: {'ROC-AUC': 0.9689904934359439, 'PRC-AUC': 0.9651921301716772, 'Accuracy': 0.9042553191489362, 'F1': 0.898876404494382, 'MCC': 0.8131249357707345, 'Recall': 0.851063829787234, 'Precision': 0.9523809523809523, 'False Positives': 4, 'False Positive Rate': np.float64(0.0425531914893617)}\n",
      "[Fold 4] Ep 001 | train loss 0.6299 | val AP 0.5181 | lr 2.00e-03\n",
      "[Fold 4] Ep 002 | train loss 0.4660 | val AP 0.9422 | lr 2.00e-03\n",
      "[Fold 4] Ep 003 | train loss 0.3805 | val AP 0.9369 | lr 2.00e-03\n",
      "[Fold 4] Ep 004 | train loss 0.3105 | val AP 0.8578 | lr 2.00e-03\n",
      "[Fold 4] Ep 005 | train loss 0.2739 | val AP 0.8848 | lr 2.00e-03\n",
      "[Fold 4] Ep 006 | train loss 0.2596 | val AP 0.9587 | lr 2.00e-03\n",
      "[Fold 4] Ep 007 | train loss 0.2675 | val AP 0.8680 | lr 2.00e-03\n",
      "[Fold 4] Ep 008 | train loss 0.2637 | val AP 0.9409 | lr 2.00e-03\n",
      "[Fold 4] Ep 009 | train loss 0.2488 | val AP 0.9552 | lr 2.00e-03\n",
      "[Fold 4] Ep 010 | train loss 0.2236 | val AP 0.9692 | lr 2.00e-03\n",
      "[Fold 4] Ep 011 | train loss 0.2158 | val AP 0.9710 | lr 2.00e-03\n",
      "[Fold 4] Ep 012 | train loss 0.2469 | val AP 0.9837 | lr 2.00e-03\n",
      "[Fold 4] Ep 013 | train loss 0.2113 | val AP 0.9335 | lr 2.00e-03\n",
      "[Fold 4] Ep 014 | train loss 0.2394 | val AP 0.9722 | lr 2.00e-03\n",
      "[Fold 4] Ep 015 | train loss 0.2164 | val AP 0.9496 | lr 2.00e-03\n",
      "[Fold 4] Ep 016 | train loss 0.1965 | val AP 0.9799 | lr 2.00e-03\n",
      "[Fold 4] Ep 017 | train loss 0.2052 | val AP 0.9064 | lr 2.00e-03\n",
      "[Fold 4] Ep 018 | train loss 0.2212 | val AP 0.9585 | lr 1.00e-03\n",
      "[Fold 4] Ep 019 | train loss 0.1870 | val AP 0.9884 | lr 1.00e-03\n",
      "[Fold 4] Ep 020 | train loss 0.1711 | val AP 0.9700 | lr 1.00e-03\n",
      "[Fold 4] Ep 021 | train loss 0.1850 | val AP 0.9933 | lr 1.00e-03\n",
      "[Fold 4] Ep 022 | train loss 0.1610 | val AP 0.9503 | lr 1.00e-03\n",
      "[Fold 4] Ep 023 | train loss 0.1529 | val AP 0.9849 | lr 1.00e-03\n",
      "[Fold 4] Ep 024 | train loss 0.1519 | val AP 0.9541 | lr 1.00e-03\n",
      "[Fold 4] Ep 025 | train loss 0.1495 | val AP 0.9944 | lr 1.00e-03\n",
      "[Fold 4] Ep 026 | train loss 0.1443 | val AP 0.9746 | lr 1.00e-03\n",
      "[Fold 4] Ep 027 | train loss 0.1511 | val AP 0.9702 | lr 1.00e-03\n",
      "[Fold 4] Ep 028 | train loss 0.1620 | val AP 0.9874 | lr 1.00e-03\n",
      "[Fold 4] Ep 029 | train loss 0.1457 | val AP 0.9859 | lr 1.00e-03\n",
      "[Fold 4] Ep 030 | train loss 0.1233 | val AP 0.9927 | lr 1.00e-03\n",
      "[Fold 4] Ep 031 | train loss 0.1362 | val AP 0.9877 | lr 5.00e-04\n",
      "[Fold 4] Ep 032 | train loss 0.1240 | val AP 0.9818 | lr 5.00e-04\n",
      "[Fold 4] Ep 033 | train loss 0.1074 | val AP 0.9803 | lr 5.00e-04\n",
      "[Fold 4] Ep 034 | train loss 0.1238 | val AP 0.9863 | lr 5.00e-04\n",
      "[Fold 4] Ep 035 | train loss 0.1027 | val AP 0.9877 | lr 5.00e-04\n",
      "[Fold 4] Ep 036 | train loss 0.1002 | val AP 0.9913 | lr 5.00e-04\n",
      "[Fold 4] Ep 037 | train loss 0.1048 | val AP 0.9903 | lr 2.50e-04\n",
      "[Fold 4] Ep 038 | train loss 0.0950 | val AP 0.9869 | lr 2.50e-04\n",
      "[Fold 4] Ep 039 | train loss 0.0827 | val AP 0.9861 | lr 2.50e-04\n",
      "[Fold 4] Ep 040 | train loss 0.0853 | val AP 0.9866 | lr 2.50e-04\n",
      "[Fold 4] Early stop.\n",
      "[Fold 4] Test metrics: {'ROC-AUC': 0.9701222272521502, 'PRC-AUC': 0.9781530793338535, 'Accuracy': 0.9521276595744681, 'F1': 0.9518716577540107, 'MCC': 0.9043064923087151, 'Recall': 0.9468085106382979, 'Precision': 0.956989247311828, 'False Positives': 4, 'False Positive Rate': np.float64(0.0425531914893617)}\n",
      "[Fold 5] Ep 001 | train loss 0.6603 | val AP 0.8641 | lr 2.00e-03\n",
      "[Fold 5] Ep 002 | train loss 0.4813 | val AP 0.9298 | lr 2.00e-03\n",
      "[Fold 5] Ep 003 | train loss 0.3483 | val AP 0.9404 | lr 2.00e-03\n",
      "[Fold 5] Ep 004 | train loss 0.3215 | val AP 0.8916 | lr 2.00e-03\n",
      "[Fold 5] Ep 005 | train loss 0.3043 | val AP 0.9043 | lr 2.00e-03\n",
      "[Fold 5] Ep 006 | train loss 0.2852 | val AP 0.9092 | lr 2.00e-03\n",
      "[Fold 5] Ep 007 | train loss 0.2710 | val AP 0.9453 | lr 2.00e-03\n",
      "[Fold 5] Ep 008 | train loss 0.2415 | val AP 0.9282 | lr 2.00e-03\n",
      "[Fold 5] Ep 009 | train loss 0.2744 | val AP 0.9708 | lr 2.00e-03\n",
      "[Fold 5] Ep 010 | train loss 0.2645 | val AP 0.7711 | lr 2.00e-03\n",
      "[Fold 5] Ep 011 | train loss 0.2150 | val AP 0.9639 | lr 2.00e-03\n",
      "[Fold 5] Ep 012 | train loss 0.2153 | val AP 0.9495 | lr 2.00e-03\n",
      "[Fold 5] Ep 013 | train loss 0.2098 | val AP 0.9575 | lr 2.00e-03\n",
      "[Fold 5] Ep 014 | train loss 0.2139 | val AP 0.9791 | lr 2.00e-03\n",
      "[Fold 5] Ep 015 | train loss 0.2011 | val AP 0.9495 | lr 2.00e-03\n",
      "[Fold 5] Ep 016 | train loss 0.1835 | val AP 0.9690 | lr 2.00e-03\n",
      "[Fold 5] Ep 017 | train loss 0.1751 | val AP 0.9476 | lr 2.00e-03\n",
      "[Fold 5] Ep 018 | train loss 0.1928 | val AP 0.9608 | lr 2.00e-03\n",
      "[Fold 5] Ep 019 | train loss 0.1756 | val AP 0.9670 | lr 2.00e-03\n",
      "[Fold 5] Ep 020 | train loss 0.1628 | val AP 0.9282 | lr 1.00e-03\n",
      "[Fold 5] Ep 021 | train loss 0.1531 | val AP 0.9388 | lr 1.00e-03\n",
      "[Fold 5] Ep 022 | train loss 0.1454 | val AP 0.9413 | lr 1.00e-03\n",
      "[Fold 5] Ep 023 | train loss 0.1364 | val AP 0.9387 | lr 1.00e-03\n",
      "[Fold 5] Ep 024 | train loss 0.1362 | val AP 0.9612 | lr 1.00e-03\n",
      "[Fold 5] Ep 025 | train loss 0.1378 | val AP 0.9533 | lr 1.00e-03\n",
      "[Fold 5] Ep 026 | train loss 0.1451 | val AP 0.9651 | lr 5.00e-04\n",
      "[Fold 5] Ep 027 | train loss 0.1177 | val AP 0.9539 | lr 5.00e-04\n",
      "[Fold 5] Ep 028 | train loss 0.1191 | val AP 0.9477 | lr 5.00e-04\n",
      "[Fold 5] Ep 029 | train loss 0.1196 | val AP 0.9636 | lr 5.00e-04\n",
      "[Fold 5] Ep 030 | train loss 0.1210 | val AP 0.9623 | lr 5.00e-04\n",
      "[Fold 5] Early stop.\n",
      "[Fold 5] Test metrics: {'ROC-AUC': 0.9660479855138072, 'PRC-AUC': 0.9723779608130719, 'Accuracy': 0.9148936170212766, 'F1': 0.9139784946236559, 'MCC': 0.8299751174897799, 'Recall': 0.9042553191489362, 'Precision': 0.9239130434782609, 'False Positives': 7, 'False Positive Rate': np.float64(0.07446808510638298)}\n",
      "\n",
      "========== Overall (5-fold aggregated) ==========\n",
      "ROC-AUC: 0.955220\n",
      "PRC-AUC: 0.960153\n",
      "Accuracy: 0.911702\n",
      "F1: 0.908891\n",
      "MCC: 0.824976\n",
      "Recall: 0.880851\n",
      "Precision: 0.938776\n",
      "False Positives: 27\n",
      "False Positive Rate: 0.057447\n",
      "Best fold model saved to: best_gine_model.pth\n",
      "\n",
      "Per-fold metrics -> cv_per_fold.csv\n",
      "Overall metrics -> cv_results.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "GINE + Graph Autoencoder (GAE) undersampling pipeline for molecular activity prediction.\n",
    "\n",
    "Pipeline\n",
    "--------\n",
    "1) RDKit: convert SMILES -> molecular graphs with rich node/edge features\n",
    "2) Train a Graph Autoencoder (reconstruct node features) to obtain graph embeddings\n",
    "3) Undersample negatives by similarity to the positive centroid in embedding space:\n",
    "   - 'nearest'  : pick the most similar negatives (smallest difference)  [DEFAULT]\n",
    "   - 'farthest' : pick the most dissimilar negatives (largest difference)\n",
    "   Supports cosine (default) or euclidean distance; negative:positive ~ 1:1 (configurable)\n",
    "4) Train a GINE-based graph classifier with Stratified 5-Fold cross-validation\n",
    "5) Report and save metrics per fold and overall:\n",
    "   ROC-AUC, PRC-AUC, Accuracy, F1, MCC, Recall, Precision, False Positives, False Positive Rate\n",
    "6) Save best-fold model weights and optional embeddings CSV\n",
    "\n",
    "Requirements\n",
    "------------\n",
    "- rdkit\n",
    "- torch, torch_geometric (>= 2.2 recommended)\n",
    "- scikit-learn, pandas, numpy, tqdm\n",
    "\n",
    "Input\n",
    "-----\n",
    "A CSV with at least two columns:\n",
    "- 'smiles' : SMILES string\n",
    "- 'antibiotic_activity' : binary label {0,1}\n",
    "\n",
    "Usage\n",
    "-----\n",
    "Command line:\n",
    "    python train_gcn_gae_pipeline.py --csv /path/to/data.csv \\\n",
    "        --pick nearest --metric cosine --ratio 1.0\n",
    "\n",
    "Jupyter / IDE:\n",
    "    This script uses `parse_known_args()` to ignore extra kernel args.\n",
    "    Alternatively, call:\n",
    "        from types import SimpleNamespace\n",
    "        main(SimpleNamespace(csv='/path/to/data.csv', pick='nearest', metric='cosine', ratio=1.0))\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# PyG\n",
    "from torch_geometric.data import Data\n",
    "try:\n",
    "    from torch_geometric.loader import DataLoader\n",
    "except Exception:  # backward compatibility with very old PyG versions\n",
    "    from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool, GINEConv, BatchNorm\n",
    "\n",
    "# RDKit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    matthews_corrcoef,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration (edit as needed)\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "DEFAULT_CSV = \"./data/raw_data.csv\"\n",
    "RESULT_CSV = \"cv_results.csv\"\n",
    "FOLD_DETAIL_CSV = \"cv_per_fold.csv\"\n",
    "BEST_MODEL_PATH = \"best_gine_model.pth\"\n",
    "EMBED_CSV = \"graph_embeddings.csv\"\n",
    "\n",
    "MAX_WORKERS = min(os.cpu_count() or 0, 30)  # keep <= 30 CPUs if you parallelize elsewhere\n",
    "BATCH_SIZE_AE = 64\n",
    "BATCH_SIZE_CLS = 64\n",
    "EPOCHS_AE = 60\n",
    "EPOCHS_CLS = 120\n",
    "PATIENCE = 15\n",
    "LR_AE = 1e-3\n",
    "LR_CLS = 2e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "DROPOUT = 0.2\n",
    "HIDDEN = 128\n",
    "NUM_GINE = 3                  # number of encoder layers\n",
    "VAL_SPLIT = 0.10              # split from the training fold for early stopping\n",
    "DIST_METRIC = \"cosine\"        # 'cosine' or 'euclidean' (default used if not overriden by CLI)\n",
    "NEG_POS_RATIO = 1.0           # negative:positive = 1:1 by default\n",
    "DEFAULT_PICK = \"nearest\"      # 'nearest' (most similar negatives) or 'farthest' (most dissimilar)\n",
    "\n",
    "# Common atomic numbers; everything else goes to \"other\"\n",
    "COMMON_Z = [1, 5, 6, 7, 8, 9, 14, 15, 16, 17, 19, 11, 12, 20, 26, 29, 30, 35, 53]\n",
    "# H,B,C,N,O,F,Si,P,S,Cl,K,Na,Mg,Ca,Fe,Cu,Zn,Br,I\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def set_seed(seed: int = SEED) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def one_hot(val, choices):\n",
    "    vec = [0] * len(choices)\n",
    "    if val in choices:\n",
    "        vec[choices.index(val)] = 1\n",
    "    return vec\n",
    "\n",
    "\n",
    "def atom_features(atom: Chem.rdchem.Atom) -> List[float]:\n",
    "    \"\"\"\n",
    "    Node features:\n",
    "      - atomic number: one-hot (COMMON_Z + 'other')\n",
    "      - degree: one-hot [0..5]\n",
    "      - hybridization: one-hot {sp, sp2, sp3, sp3d, sp3d2, other}\n",
    "      - formal charge: one-hot [-2..2]\n",
    "      - total hydrogens: one-hot [0..4]\n",
    "      - aromatic (bool)\n",
    "      - in ring (bool)\n",
    "      - chirality tag: one-hot {unspecified, CW, CCW}\n",
    "    \"\"\"\n",
    "    z = atom.GetAtomicNum()\n",
    "    z_onehot = one_hot(z if z in COMMON_Z else -1, COMMON_Z + [-1])\n",
    "\n",
    "    degree = atom.GetTotalDegree()\n",
    "    degree_onehot = one_hot(min(degree, 5), list(range(6)))\n",
    "\n",
    "    hyb = atom.GetHybridization()\n",
    "    hyb_choices = [\n",
    "        Chem.rdchem.HybridizationType.SP,\n",
    "        Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3,\n",
    "        Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2,\n",
    "    ]\n",
    "    hyb_onehot = one_hot(hyb if hyb in hyb_choices else None, hyb_choices + [None])\n",
    "\n",
    "    charge = int(atom.GetFormalCharge())\n",
    "    charge = max(-2, min(2, charge))\n",
    "    charge_onehot = one_hot(charge, [-2, -1, 0, 1, 2])\n",
    "\n",
    "    num_h = min(atom.GetTotalNumHs(), 4)\n",
    "    num_h_onehot = one_hot(num_h, [0, 1, 2, 3, 4])\n",
    "\n",
    "    aromatic = [int(atom.GetIsAromatic())]\n",
    "    ring = [int(atom.IsInRing())]\n",
    "\n",
    "    chiral_tag = atom.GetChiralTag()\n",
    "    chiral_choices = [\n",
    "        Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "    ]\n",
    "    chiral_onehot = one_hot(\n",
    "        chiral_tag if chiral_tag in chiral_choices else Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "        chiral_choices,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        z_onehot\n",
    "        + degree_onehot\n",
    "        + hyb_onehot\n",
    "        + charge_onehot\n",
    "        + num_h_onehot\n",
    "        + aromatic\n",
    "        + ring\n",
    "        + chiral_onehot\n",
    "    )\n",
    "\n",
    "\n",
    "def bond_features(bond: Chem.rdchem.Bond) -> List[float]:\n",
    "    \"\"\"\n",
    "    Edge features:\n",
    "      - bond type: one-hot {single,double,triple,aromatic,other}\n",
    "      - conjugated (bool)\n",
    "      - in ring (bool)\n",
    "      - stereo: one-hot {none, Z, E}\n",
    "    \"\"\"\n",
    "    bt = bond.GetBondType()\n",
    "    bt_choices = [\n",
    "        Chem.BondType.SINGLE,\n",
    "        Chem.BondType.DOUBLE,\n",
    "        Chem.BondType.TRIPLE,\n",
    "        Chem.BondType.AROMATIC,\n",
    "    ]\n",
    "    bt_onehot = one_hot(bt if bt in bt_choices else None, bt_choices + [None])\n",
    "\n",
    "    conj = [int(bond.GetIsConjugated())]\n",
    "    ring = [int(bond.IsInRing())]\n",
    "\n",
    "    stereo = bond.GetStereo()\n",
    "    stereo_choices = [\n",
    "        Chem.rdchem.BondStereo.STEREONONE,\n",
    "        Chem.rdchem.BondStereo.STEREOZ,\n",
    "        Chem.rdchem.BondStereo.STEREOE,\n",
    "    ]\n",
    "    stereo_onehot = one_hot(\n",
    "        stereo if stereo in stereo_choices else Chem.rdchem.BondStereo.STEREONONE, stereo_choices\n",
    "    )\n",
    "\n",
    "    return bt_onehot + conj + ring + stereo_onehot\n",
    "\n",
    "\n",
    "def smiles_to_graph(smiles: str):\n",
    "    \"\"\"Build a PyG `Data` object from a SMILES string.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    # Keep aromatic flags; 2D coords are sufficient for this pipeline\n",
    "    Chem.Kekulize(mol, clearAromaticFlags=False)\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "\n",
    "    # Node features\n",
    "    x = [atom_features(a) for a in mol.GetAtoms()]\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "    # Edges + edge features (bidirectional)\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        f = bond_features(b)\n",
    "        edge_index.append([i, j]); edge_attr.append(f)\n",
    "        edge_index.append([j, i]); edge_attr.append(f)\n",
    "\n",
    "    if len(edge_index) == 0:\n",
    "        # Rare edge-less molecule\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        # Build an empty edge_attr with correct feature width by probing a dummy bond\n",
    "        dummy = Chem.MolFromSmiles(\"CC\").GetBonds()[0]\n",
    "        feat_w = len(bond_features(dummy))\n",
    "        edge_attr = torch.empty((0, feat_w), dtype=torch.float)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "# -----------------------------\n",
    "# Models\n",
    "# -----------------------------\n",
    "def mlp(in_dim: int, out_dim: int) -> nn.Sequential:\n",
    "    return nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Linear(out_dim, out_dim))\n",
    "\n",
    "\n",
    "class GINEEncoder(nn.Module):\n",
    "    \"\"\"GINE encoder with BatchNorm, dropout and a light residual connection.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, edge_dim: int, hidden: int = HIDDEN, num_layers: int = NUM_GINE, dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # First layer\n",
    "        self.convs.append(GINEConv(mlp(in_dim, hidden), edge_dim=edge_dim))\n",
    "        self.bns.append(BatchNorm(hidden))\n",
    "\n",
    "        # Subsequent layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GINEConv(mlp(hidden, hidden), edge_dim=edge_dim))\n",
    "            self.bns.append(BatchNorm(hidden))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        h = x\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            h_res = h\n",
    "            h = conv(h, edge_index, edge_attr)\n",
    "            h = bn(h)\n",
    "            h = F.relu(h)\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            # Tiny residual when shape matches\n",
    "            if h_res.shape == h.shape:\n",
    "                h = h + 0.1 * h_res\n",
    "        return h  # node embeddings\n",
    "\n",
    "\n",
    "class GraphAE(nn.Module):\n",
    "    \"\"\"Graph Autoencoder: node encoder -> reconstruct node features; returns graph embedding.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, edge_dim: int, hidden: int = HIDDEN, num_layers: int = NUM_GINE, dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.encoder = GINEEncoder(in_dim, edge_dim, hidden, num_layers, dropout)\n",
    "        self.decoder = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, in_dim))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        h = self.encoder(x, edge_index, edge_attr)  # [N, hidden]\n",
    "        x_hat = self.decoder(h)                     # [N, in_dim]\n",
    "        g = global_mean_pool(h, batch)              # [B, hidden] graph embedding\n",
    "        return x_hat, g\n",
    "\n",
    "    def encode_nodes(self, x, edge_index, edge_attr):\n",
    "        return self.encoder(x, edge_index, edge_attr)\n",
    "\n",
    "\n",
    "class GINEClassifier(nn.Module):\n",
    "    \"\"\"Graph-level classifier; encoder can be initialized from a trained AE.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, edge_dim: int, hidden: int = HIDDEN, num_layers: int = NUM_GINE, dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.encoder = GINEEncoder(in_dim, edge_dim, hidden, num_layers, dropout)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        h = self.encoder(x, edge_index, edge_attr)\n",
    "        g = global_mean_pool(h, batch)\n",
    "        logit = self.head(g).view(-1)\n",
    "        return logit\n",
    "\n",
    "    def load_from_ae(self, ae: GraphAE):\n",
    "        self.encoder.load_state_dict(ae.encoder.state_dict(), strict=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Training & evaluation helpers\n",
    "# -----------------------------\n",
    "def train_ae(ae: GraphAE, loader: DataLoader, device, epochs: int = EPOCHS_AE, lr: float = LR_AE,\n",
    "             wd: float = WEIGHT_DECAY, patience: int = PATIENCE) -> GraphAE:\n",
    "    \"\"\"Train GraphAE with node feature reconstruction loss (MSE).\"\"\"\n",
    "    ae = ae.to(device)\n",
    "    opt = torch.optim.Adam(ae.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    # ReduceLROnPlateau: some torch versions don't support 'verbose'\n",
    "    try:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=5, verbose=False)\n",
    "    except TypeError:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=5)\n",
    "\n",
    "    best_loss, bad = float(\"inf\"), 0\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        ae.train()\n",
    "        total = 0.0\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            x_hat, _ = ae(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            loss = F.mse_loss(x_hat, data.x)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(ae.parameters(), 2.0)\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "\n",
    "        mean_loss = total / max(len(loader), 1)\n",
    "        scheduler.step(mean_loss)\n",
    "        lr_now = opt.param_groups[0][\"lr\"]\n",
    "        print(f\"[AE] Epoch {ep:03d} | recon MSE {mean_loss:.5f} | lr {lr_now:.2e}\")\n",
    "\n",
    "        if mean_loss < best_loss - 1e-5:\n",
    "            best_loss, bad = mean_loss, 0\n",
    "            torch.save(ae.state_dict(), \"best_graph_ae.pth\")\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience and ep >= 20:\n",
    "                print(\"[AE] Early stop.\")\n",
    "                break\n",
    "\n",
    "    ae.load_state_dict(torch.load(\"best_graph_ae.pth\", map_location=device))\n",
    "    ae.eval()\n",
    "    return ae\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_graph_embeddings(ae: GraphAE, loader: DataLoader, device) -> np.ndarray:\n",
    "    \"\"\"Return graph embeddings [N_graphs, hidden] from a trained AE encoder.\"\"\"\n",
    "    ae.eval()\n",
    "    embs = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        h = ae.encode_nodes(data.x, data.edge_index, data.edge_attr)\n",
    "        g = global_mean_pool(h, data.batch)  # [B, hidden]\n",
    "        embs.append(g.cpu().numpy())\n",
    "    return np.concatenate(embs, axis=0)\n",
    "\n",
    "\n",
    "def select_negatives_by_similarity(\n",
    "    embs: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    metric: str = DIST_METRIC,     # 'cosine' or 'euclidean'\n",
    "    ratio: float = NEG_POS_RATIO,  # negatives : positives\n",
    "    pick: str = DEFAULT_PICK       # 'nearest' (most similar) or 'farthest' (most dissimilar)\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Select negatives relative to the positive centroid in the embedding space.\n",
    "\n",
    "    Behavior\n",
    "    --------\n",
    "    - cosine:    higher similarity -> more similar; lower -> more dissimilar\n",
    "    - euclidean: smaller distance  -> more similar; larger -> more dissimilar\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mask : (N,) bool array indicating retained samples (all positives + selected negatives)\n",
    "    \"\"\"\n",
    "    assert pick in (\"nearest\", \"farthest\"), \"pick must be 'nearest' or 'farthest'\"\n",
    "\n",
    "    pos_idx = np.where(labels == 1)[0]\n",
    "    neg_idx = np.where(labels == 0)[0]\n",
    "    if len(pos_idx) == 0 or len(neg_idx) == 0:\n",
    "        return np.ones_like(labels, dtype=bool)\n",
    "\n",
    "    pos_centroid = embs[pos_idx].mean(axis=0, keepdims=True)  # [1, d]\n",
    "    neg_embs = embs[neg_idx]                                  # [K, d]\n",
    "\n",
    "    if metric == \"cosine\":\n",
    "        a = neg_embs / (np.linalg.norm(neg_embs, axis=1, keepdims=True) + 1e-9)\n",
    "        b = pos_centroid / (np.linalg.norm(pos_centroid, axis=1, keepdims=True) + 1e-9)\n",
    "        sim = (a @ b.T).reshape(-1)  # larger = more similar\n",
    "        if pick == \"nearest\":\n",
    "            order = np.argsort(-sim)   # descending  -> most similar first\n",
    "        else:\n",
    "            order = np.argsort(sim)    # ascending   -> most dissimilar first\n",
    "    elif metric == \"euclidean\":\n",
    "        dist = np.linalg.norm(neg_embs - pos_centroid, axis=1)  # smaller = more similar\n",
    "        if pick == \"nearest\":\n",
    "            order = np.argsort(dist)   # ascending  -> most similar first\n",
    "        else:\n",
    "            order = np.argsort(-dist)  # descending -> most dissimilar first\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'cosine' or 'euclidean'\")\n",
    "\n",
    "    k = int(round(len(pos_idx) * ratio))\n",
    "    chosen_neg = neg_idx[order[:k]]\n",
    "\n",
    "    mask = np.zeros_like(labels, dtype=bool)\n",
    "    mask[pos_idx] = True\n",
    "    mask[chosen_neg] = True\n",
    "    return mask\n",
    "\n",
    "\n",
    "def train_one_epoch_cls(model: nn.Module, loader: DataLoader, optimizer, criterion, device) -> float:\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        y = data.y.view(-1).to(device)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "    return total / max(len(loader), 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(model: nn.Module, loader: DataLoader, device):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        logits = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        prob = torch.sigmoid(logits)\n",
    "        ys.append(data.y.view(-1).cpu().numpy())\n",
    "        ps.append(prob.cpu().numpy())\n",
    "    y_true = np.concatenate(ys) if ys else np.array([])\n",
    "    y_pred = np.concatenate(ps) if ps else np.array([])\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def calc_metrics(y_true: np.ndarray, y_prob: np.ndarray, threshold: float = 0.5):\n",
    "    \"\"\"Return all required metrics, including FP and FPR.\"\"\"\n",
    "    if y_true.size == 0:\n",
    "        keys = [\n",
    "            \"ROC-AUC\", \"PRC-AUC\", \"Accuracy\", \"F1\", \"MCC\",\n",
    "            \"Recall\", \"Precision\", \"False Positives\", \"False Positive Rate\",\n",
    "        ]\n",
    "        return {k: float(\"nan\") for k in keys}\n",
    "\n",
    "    y_hat = (y_prob >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat, labels=[0, 1]).ravel()\n",
    "    fpr = fp / max((fp + tn), 1)\n",
    "\n",
    "    return {\n",
    "        \"ROC-AUC\": roc_auc_score(y_true, y_prob),\n",
    "        \"PRC-AUC\": average_precision_score(y_true, y_prob),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_hat),\n",
    "        \"F1\": f1_score(y_true, y_hat, zero_division=0),\n",
    "        \"MCC\": matthews_corrcoef(y_true, y_hat),\n",
    "        \"Recall\": recall_score(y_true, y_hat, zero_division=0),\n",
    "        \"Precision\": precision_score(y_true, y_hat, zero_division=0),\n",
    "        \"False Positives\": int(fp),\n",
    "        \"False Positive Rate\": fpr,\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main(args) -> None:\n",
    "    set_seed(SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 1) Load data\n",
    "    df = pd.read_csv(args.csv)\n",
    "    assert \"smiles\" in df.columns and \"antibiotic_activity\" in df.columns, \\\n",
    "        \"CSV must contain columns 'smiles' and 'antibiotic_activity'.\"\n",
    "    smiles = df[\"smiles\"].astype(str).tolist()\n",
    "    labels = df[\"antibiotic_activity\"].astype(int).to_numpy()\n",
    "\n",
    "    # 2) Build graphs\n",
    "    data_list: List[Data] = []\n",
    "    drop_idx = []\n",
    "    for i, smi in enumerate(tqdm(smiles, desc=\"SMILES->Graph\")):\n",
    "        g = smiles_to_graph(smi)\n",
    "        if g is None or g.x.numel() == 0:\n",
    "            drop_idx.append(i)\n",
    "            continue\n",
    "        g.y = torch.tensor([labels[i]], dtype=torch.float32)\n",
    "        data_list.append(g)\n",
    "\n",
    "    if len(data_list) == 0:\n",
    "        raise RuntimeError(\"No valid molecules after SMILES->graph conversion.\")\n",
    "\n",
    "    if drop_idx:\n",
    "        print(f\"Warning: {len(drop_idx)} SMILES failed to convert and were skipped.\")\n",
    "        labels = np.delete(labels, drop_idx, axis=0)\n",
    "\n",
    "    in_dim = data_list[0].x.size(1)\n",
    "    edge_dim = data_list[0].edge_attr.size(1) if data_list[0].edge_attr is not None else 0\n",
    "    print(f\"Node feat dim = {in_dim} | Edge feat dim = {edge_dim} | N graphs = {len(data_list)}\")\n",
    "\n",
    "    # 3) Train Graph AE on ALL samples\n",
    "    ae_loader = DataLoader(data_list, batch_size=BATCH_SIZE_AE, shuffle=True, num_workers=0)\n",
    "    ae = GraphAE(in_dim, edge_dim, hidden=HIDDEN, num_layers=NUM_GINE, dropout=DROPOUT)\n",
    "    ae = train_ae(ae, ae_loader, device)\n",
    "\n",
    "    # 4) Get embeddings and perform similarity-based undersampling\n",
    "    eval_loader = DataLoader(data_list, batch_size=BATCH_SIZE_AE, shuffle=False, num_workers=0)\n",
    "    embs = get_graph_embeddings(ae, eval_loader, device)  # [N, hidden]\n",
    "    pd.DataFrame(embs).to_csv(EMBED_CSV, index=False)\n",
    "\n",
    "    pick_mode = getattr(args, \"pick\", DEFAULT_PICK)\n",
    "    dist_metric = getattr(args, \"metric\", DIST_METRIC)\n",
    "    ratio = float(getattr(args, \"ratio\", NEG_POS_RATIO))\n",
    "\n",
    "    mask = select_negatives_by_similarity(\n",
    "        embs, labels, metric=dist_metric, ratio=ratio, pick=pick_mode\n",
    "    )\n",
    "    data_balanced = [d for d, m in zip(data_list, mask) if m]\n",
    "    labels_balanced = labels[mask]\n",
    "    print(\n",
    "        f\"After undersampling by '{pick_mode}' ({dist_metric}) similarity: \"\n",
    "        f\"pos={labels_balanced.sum()} neg={(1 - labels_balanced).sum()} total={len(data_balanced)}\"\n",
    "    )\n",
    "\n",
    "    # 5) 5-fold CV training\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    fold_metrics = []\n",
    "    all_true, all_prob = [], []\n",
    "    best_score = -1.0\n",
    "\n",
    "    for fold, (tr_idx, te_idx) in enumerate(skf.split(np.arange(len(data_balanced)), labels_balanced), 1):\n",
    "        train_subset = [data_balanced[i] for i in tr_idx]\n",
    "        test_subset = [data_balanced[i] for i in te_idx]\n",
    "        y_train = labels_balanced[tr_idx]\n",
    "        y_test = labels_balanced[te_idx]\n",
    "\n",
    "        # validation split from training fold for early stopping\n",
    "        tr_part, val_part, _, _ = train_test_split(\n",
    "            train_subset, y_train, test_size=VAL_SPLIT, stratify=y_train, random_state=SEED\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(tr_part, batch_size=BATCH_SIZE_CLS, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_part, batch_size=BATCH_SIZE_CLS, shuffle=False, num_workers=0)\n",
    "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE_CLS, shuffle=False, num_workers=0)\n",
    "\n",
    "        model = GINEClassifier(in_dim, edge_dim, hidden=HIDDEN, num_layers=NUM_GINE, dropout=DROPOUT).to(device)\n",
    "        model.load_from_ae(ae)\n",
    "\n",
    "        # class imbalance guard (should be close to 1:1 after undersampling)\n",
    "        pos_count = float((y_train == 1).sum())\n",
    "        neg_count = float((y_train == 0).sum())\n",
    "        pos_weight = torch.tensor([(neg_count / max(pos_count, 1.0))], device=device, dtype=torch.float32)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR_CLS, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=5)\n",
    "\n",
    "        best_val, bad = -1.0, 0\n",
    "        best_state = None\n",
    "\n",
    "        for ep in range(1, EPOCHS_CLS + 1):\n",
    "            loss = train_one_epoch_cls(model, train_loader, optimizer, criterion, device)\n",
    "            yv, pv = infer(model, val_loader, device)\n",
    "            val_ap = average_precision_score(yv, pv) if yv.size > 0 else 0.0  # PRC-AUC for early stopping\n",
    "            scheduler.step(val_ap)\n",
    "            print(\n",
    "                f\"[Fold {fold}] Ep {ep:03d} | train loss {loss:.4f} | val AP {val_ap:.4f} \"\n",
    "                f\"| lr {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "            )\n",
    "\n",
    "            if val_ap > best_val + 1e-5:\n",
    "                best_val, bad = val_ap, 0\n",
    "                best_state = model.state_dict()\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= PATIENCE and ep >= 30:\n",
    "                    print(f\"[Fold {fold}] Early stop.\")\n",
    "                    break\n",
    "\n",
    "        # test with best validation state\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        yt, pt = infer(model, test_loader, device)\n",
    "        all_true.append(yt)\n",
    "        all_prob.append(pt)\n",
    "        m = calc_metrics(yt, pt, threshold=0.5)\n",
    "        fold_metrics.append({\"Fold\": fold, **m})\n",
    "        print(f\"[Fold {fold}] Test metrics: {m}\")\n",
    "\n",
    "        # keep best model by ROC-AUC\n",
    "        if m[\"ROC-AUC\"] > best_score:\n",
    "            best_score = m[\"ROC-AUC\"]\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "\n",
    "    # 6) Save per-fold and overall results\n",
    "    pd.DataFrame(fold_metrics).to_csv(FOLD_DETAIL_CSV, index=False)\n",
    "\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_prob = np.concatenate(all_prob)\n",
    "    overall_metrics = calc_metrics(all_true, all_prob, threshold=0.5)\n",
    "\n",
    "    print(\"\\n========== Overall (5-fold aggregated) ==========\")\n",
    "    for k, v in overall_metrics.items():\n",
    "        print(f\"{k}: {v:.6f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "    print(f\"Best fold model saved to: {BEST_MODEL_PATH}\")\n",
    "\n",
    "    # Single-row summary for easy comparison across models\n",
    "    out_row = {\n",
    "        \"Model\": f\"GINE(AE init) + {pick_mode} undersampling ({dist_metric}), ratio={ratio}\",\n",
    "        **overall_metrics,\n",
    "    }\n",
    "    pd.DataFrame([out_row]).to_csv(RESULT_CSV, index=False)\n",
    "    print(f\"\\nPer-fold metrics -> {FOLD_DETAIL_CSV}\")\n",
    "    print(f\"Overall metrics -> {RESULT_CSV}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--csv\",\n",
    "        type=str,\n",
    "        default=DEFAULT_CSV,\n",
    "        help=\"Path to CSV with columns 'smiles' and 'antibiotic_activity'.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pick\",\n",
    "        type=str,\n",
    "        choices=[\"nearest\", \"farthest\"],\n",
    "        default=DEFAULT_PICK,\n",
    "        help=\"Negative selection mode relative to positive centroid (default: nearest).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--metric\",\n",
    "        type=str,\n",
    "        choices=[\"cosine\", \"euclidean\"],\n",
    "        default=DIST_METRIC,\n",
    "        help=\"Similarity metric in embedding space (default: cosine).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ratio\",\n",
    "        type=float,\n",
    "        default=NEG_POS_RATIO,\n",
    "        help=\"Negative:positive ratio for undersampling (default: 1.0).\",\n",
    "    )\n",
    "    # In notebook/IDE environments extra args like --f=... may be injected.\n",
    "    args, _ = parser.parse_known_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the GCN models features contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SMILES->Graph: 100%|██████████| 13524/13524 [00:11<00:00, 1151.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feat dim = 47 | Edge feat dim = 10 | N graphs = 13524\n",
      "[AE] Epoch 001 | recon MSE 0.02076 | lr 1.00e-03\n",
      "[AE] Epoch 002 | recon MSE 0.00463 | lr 1.00e-03\n",
      "[AE] Epoch 003 | recon MSE 0.00259 | lr 1.00e-03\n",
      "[AE] Epoch 004 | recon MSE 0.00179 | lr 1.00e-03\n",
      "[AE] Epoch 005 | recon MSE 0.00137 | lr 1.00e-03\n",
      "[AE] Epoch 006 | recon MSE 0.00114 | lr 1.00e-03\n",
      "[AE] Epoch 007 | recon MSE 0.00101 | lr 1.00e-03\n",
      "[AE] Epoch 008 | recon MSE 0.00093 | lr 1.00e-03\n",
      "[AE] Epoch 009 | recon MSE 0.00088 | lr 1.00e-03\n",
      "[AE] Epoch 010 | recon MSE 0.00094 | lr 1.00e-03\n",
      "[AE] Epoch 011 | recon MSE 0.00084 | lr 1.00e-03\n",
      "[AE] Epoch 012 | recon MSE 0.00081 | lr 1.00e-03\n",
      "[AE] Epoch 013 | recon MSE 0.00085 | lr 1.00e-03\n",
      "[AE] Epoch 014 | recon MSE 0.00078 | lr 1.00e-03\n",
      "[AE] Epoch 015 | recon MSE 0.00076 | lr 1.00e-03\n",
      "[AE] Epoch 016 | recon MSE 0.00076 | lr 1.00e-03\n",
      "[AE] Epoch 017 | recon MSE 0.00076 | lr 1.00e-03\n",
      "[AE] Epoch 018 | recon MSE 0.00075 | lr 1.00e-03\n",
      "[AE] Epoch 019 | recon MSE 0.00074 | lr 1.00e-03\n",
      "[AE] Epoch 020 | recon MSE 0.00074 | lr 1.00e-03\n",
      "[AE] Epoch 021 | recon MSE 0.00086 | lr 1.00e-03\n",
      "[AE] Epoch 022 | recon MSE 0.00074 | lr 1.00e-03\n",
      "[AE] Epoch 023 | recon MSE 0.00072 | lr 1.00e-03\n",
      "[AE] Epoch 024 | recon MSE 0.00072 | lr 1.00e-03\n",
      "[AE] Epoch 025 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 026 | recon MSE 0.00072 | lr 1.00e-03\n",
      "[AE] Epoch 027 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 028 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 029 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 030 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 031 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 032 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 033 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 034 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 035 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 036 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 037 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 038 | recon MSE 0.00073 | lr 1.00e-03\n",
      "[AE] Epoch 039 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 040 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 041 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 042 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 043 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 044 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 045 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 046 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 047 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 048 | recon MSE 0.00075 | lr 1.00e-03\n",
      "[AE] Epoch 049 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 050 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 051 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 052 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 053 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 054 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 055 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 056 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 057 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 058 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 059 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 060 | recon MSE 0.00067 | lr 1.00e-03\n",
      "After undersampling by 'nearest' (cosine) similarity: pos=470 neg=470 total=940\n",
      "[Fold 1] Ep 001 | train loss 0.6449 | val AP 0.9519 | lr 2.00e-03\n",
      "[Fold 1] Ep 002 | train loss 0.4425 | val AP 0.8957 | lr 2.00e-03\n",
      "[Fold 1] Ep 003 | train loss 0.3317 | val AP 0.9455 | lr 2.00e-03\n",
      "[Fold 1] Ep 004 | train loss 0.3259 | val AP 0.9562 | lr 2.00e-03\n",
      "[Fold 1] Ep 005 | train loss 0.2732 | val AP 0.9662 | lr 2.00e-03\n",
      "[Fold 1] Ep 006 | train loss 0.2855 | val AP 0.9499 | lr 2.00e-03\n",
      "[Fold 1] Ep 007 | train loss 0.2741 | val AP 0.8777 | lr 2.00e-03\n",
      "[Fold 1] Ep 008 | train loss 0.2363 | val AP 0.9364 | lr 2.00e-03\n",
      "[Fold 1] Ep 009 | train loss 0.2535 | val AP 0.9432 | lr 2.00e-03\n",
      "[Fold 1] Ep 010 | train loss 0.2505 | val AP 0.9241 | lr 2.00e-03\n",
      "[Fold 1] Ep 011 | train loss 0.2229 | val AP 0.9244 | lr 1.00e-03\n",
      "[Fold 1] Ep 012 | train loss 0.2201 | val AP 0.9361 | lr 1.00e-03\n",
      "[Fold 1] Ep 013 | train loss 0.1992 | val AP 0.9430 | lr 1.00e-03\n",
      "[Fold 1] Ep 014 | train loss 0.1986 | val AP 0.9497 | lr 1.00e-03\n",
      "[Fold 1] Ep 015 | train loss 0.1619 | val AP 0.9541 | lr 1.00e-03\n",
      "[Fold 1] Ep 016 | train loss 0.1580 | val AP 0.9542 | lr 1.00e-03\n",
      "[Fold 1] Ep 017 | train loss 0.1769 | val AP 0.9618 | lr 5.00e-04\n",
      "[Fold 1] Ep 018 | train loss 0.1558 | val AP 0.9534 | lr 5.00e-04\n",
      "[Fold 1] Ep 019 | train loss 0.1381 | val AP 0.9565 | lr 5.00e-04\n",
      "[Fold 1] Ep 020 | train loss 0.1336 | val AP 0.9567 | lr 5.00e-04\n",
      "[Fold 1] Ep 021 | train loss 0.1621 | val AP 0.9548 | lr 5.00e-04\n",
      "[Fold 1] Ep 022 | train loss 0.1483 | val AP 0.9522 | lr 5.00e-04\n",
      "[Fold 1] Ep 023 | train loss 0.1373 | val AP 0.9527 | lr 2.50e-04\n",
      "[Fold 1] Ep 024 | train loss 0.1340 | val AP 0.9570 | lr 2.50e-04\n",
      "[Fold 1] Ep 025 | train loss 0.1249 | val AP 0.9455 | lr 2.50e-04\n",
      "[Fold 1] Ep 026 | train loss 0.1189 | val AP 0.9559 | lr 2.50e-04\n",
      "[Fold 1] Ep 027 | train loss 0.1053 | val AP 0.9530 | lr 2.50e-04\n",
      "[Fold 1] Ep 028 | train loss 0.1253 | val AP 0.9569 | lr 2.50e-04\n",
      "[Fold 1] Ep 029 | train loss 0.1133 | val AP 0.9514 | lr 1.25e-04\n",
      "[Fold 1] Ep 030 | train loss 0.0999 | val AP 0.9554 | lr 1.25e-04\n",
      "[Fold 1] Early stop.\n",
      "[Fold 1] Test metrics: {'ROC-AUC': 0.9580126754187415, 'PRC-AUC': 0.9497681064353753, 'Accuracy': 0.9361702127659575, 'F1': 0.9361702127659575, 'MCC': 0.8723404255319149, 'Recall': 0.9361702127659575, 'Precision': 0.9361702127659575, 'False Positives': 6, 'False Positive Rate': np.float64(0.06382978723404255)}\n",
      "[Fold 2] Ep 001 | train loss 0.6479 | val AP 0.6986 | lr 2.00e-03\n",
      "[Fold 2] Ep 002 | train loss 0.4665 | val AP 0.8958 | lr 2.00e-03\n",
      "[Fold 2] Ep 003 | train loss 0.3352 | val AP 0.8958 | lr 2.00e-03\n",
      "[Fold 2] Ep 004 | train loss 0.2670 | val AP 0.9216 | lr 2.00e-03\n",
      "[Fold 2] Ep 005 | train loss 0.2964 | val AP 0.7691 | lr 2.00e-03\n",
      "[Fold 2] Ep 006 | train loss 0.3085 | val AP 0.9668 | lr 2.00e-03\n",
      "[Fold 2] Ep 007 | train loss 0.2368 | val AP 0.9019 | lr 2.00e-03\n",
      "[Fold 2] Ep 008 | train loss 0.2654 | val AP 0.9266 | lr 2.00e-03\n",
      "[Fold 2] Ep 009 | train loss 0.2212 | val AP 0.9488 | lr 2.00e-03\n",
      "[Fold 2] Ep 010 | train loss 0.2184 | val AP 0.8884 | lr 2.00e-03\n",
      "[Fold 2] Ep 011 | train loss 0.2155 | val AP 0.9496 | lr 2.00e-03\n",
      "[Fold 2] Ep 012 | train loss 0.2299 | val AP 0.9620 | lr 1.00e-03\n",
      "[Fold 2] Ep 013 | train loss 0.2006 | val AP 0.9610 | lr 1.00e-03\n",
      "[Fold 2] Ep 014 | train loss 0.2152 | val AP 0.9507 | lr 1.00e-03\n",
      "[Fold 2] Ep 015 | train loss 0.1580 | val AP 0.9728 | lr 1.00e-03\n",
      "[Fold 2] Ep 016 | train loss 0.1574 | val AP 0.9628 | lr 1.00e-03\n",
      "[Fold 2] Ep 017 | train loss 0.1728 | val AP 0.9623 | lr 1.00e-03\n",
      "[Fold 2] Ep 018 | train loss 0.1484 | val AP 0.9545 | lr 1.00e-03\n",
      "[Fold 2] Ep 019 | train loss 0.1634 | val AP 0.9594 | lr 1.00e-03\n",
      "[Fold 2] Ep 020 | train loss 0.1315 | val AP 0.9454 | lr 1.00e-03\n",
      "[Fold 2] Ep 021 | train loss 0.1371 | val AP 0.9691 | lr 5.00e-04\n",
      "[Fold 2] Ep 022 | train loss 0.1203 | val AP 0.9686 | lr 5.00e-04\n",
      "[Fold 2] Ep 023 | train loss 0.1161 | val AP 0.9587 | lr 5.00e-04\n",
      "[Fold 2] Ep 024 | train loss 0.1009 | val AP 0.9706 | lr 5.00e-04\n",
      "[Fold 2] Ep 025 | train loss 0.1031 | val AP 0.9647 | lr 5.00e-04\n",
      "[Fold 2] Ep 026 | train loss 0.1054 | val AP 0.9707 | lr 5.00e-04\n",
      "[Fold 2] Ep 027 | train loss 0.0870 | val AP 0.9718 | lr 2.50e-04\n",
      "[Fold 2] Ep 028 | train loss 0.0992 | val AP 0.9735 | lr 2.50e-04\n",
      "[Fold 2] Ep 029 | train loss 0.0931 | val AP 0.9731 | lr 2.50e-04\n",
      "[Fold 2] Ep 030 | train loss 0.0876 | val AP 0.9731 | lr 2.50e-04\n",
      "[Fold 2] Ep 031 | train loss 0.0803 | val AP 0.9739 | lr 2.50e-04\n",
      "[Fold 2] Ep 032 | train loss 0.0984 | val AP 0.9735 | lr 2.50e-04\n",
      "[Fold 2] Ep 033 | train loss 0.0874 | val AP 0.9708 | lr 2.50e-04\n",
      "[Fold 2] Ep 034 | train loss 0.0771 | val AP 0.9740 | lr 2.50e-04\n",
      "[Fold 2] Ep 035 | train loss 0.0799 | val AP 0.9754 | lr 2.50e-04\n",
      "[Fold 2] Ep 036 | train loss 0.0735 | val AP 0.9720 | lr 2.50e-04\n",
      "[Fold 2] Ep 037 | train loss 0.0951 | val AP 0.9715 | lr 2.50e-04\n",
      "[Fold 2] Ep 038 | train loss 0.0813 | val AP 0.9715 | lr 2.50e-04\n",
      "[Fold 2] Ep 039 | train loss 0.0810 | val AP 0.9706 | lr 2.50e-04\n",
      "[Fold 2] Ep 040 | train loss 0.0672 | val AP 0.9766 | lr 2.50e-04\n",
      "[Fold 2] Ep 041 | train loss 0.0739 | val AP 0.9719 | lr 2.50e-04\n",
      "[Fold 2] Ep 042 | train loss 0.0736 | val AP 0.9686 | lr 2.50e-04\n",
      "[Fold 2] Ep 043 | train loss 0.0969 | val AP 0.9736 | lr 2.50e-04\n",
      "[Fold 2] Ep 044 | train loss 0.0915 | val AP 0.9699 | lr 2.50e-04\n",
      "[Fold 2] Ep 045 | train loss 0.0668 | val AP 0.9707 | lr 2.50e-04\n",
      "[Fold 2] Ep 046 | train loss 0.0859 | val AP 0.9743 | lr 1.25e-04\n",
      "[Fold 2] Ep 047 | train loss 0.0716 | val AP 0.9723 | lr 1.25e-04\n",
      "[Fold 2] Ep 048 | train loss 0.0686 | val AP 0.9693 | lr 1.25e-04\n",
      "[Fold 2] Ep 049 | train loss 0.0688 | val AP 0.9707 | lr 1.25e-04\n",
      "[Fold 2] Ep 050 | train loss 0.0643 | val AP 0.9686 | lr 1.25e-04\n",
      "[Fold 2] Ep 051 | train loss 0.0749 | val AP 0.9709 | lr 1.25e-04\n",
      "[Fold 2] Ep 052 | train loss 0.0799 | val AP 0.9777 | lr 1.25e-04\n",
      "[Fold 2] Ep 053 | train loss 0.0682 | val AP 0.9776 | lr 1.25e-04\n",
      "[Fold 2] Ep 054 | train loss 0.0723 | val AP 0.9720 | lr 1.25e-04\n",
      "[Fold 2] Ep 055 | train loss 0.0771 | val AP 0.9699 | lr 1.25e-04\n",
      "[Fold 2] Ep 056 | train loss 0.0539 | val AP 0.9722 | lr 1.25e-04\n",
      "[Fold 2] Ep 057 | train loss 0.0827 | val AP 0.9748 | lr 1.25e-04\n",
      "[Fold 2] Ep 058 | train loss 0.0634 | val AP 0.9755 | lr 6.25e-05\n",
      "[Fold 2] Ep 059 | train loss 0.0796 | val AP 0.9743 | lr 6.25e-05\n",
      "[Fold 2] Ep 060 | train loss 0.0713 | val AP 0.9732 | lr 6.25e-05\n",
      "[Fold 2] Ep 061 | train loss 0.0738 | val AP 0.9731 | lr 6.25e-05\n",
      "[Fold 2] Ep 062 | train loss 0.0727 | val AP 0.9740 | lr 6.25e-05\n",
      "[Fold 2] Ep 063 | train loss 0.0596 | val AP 0.9749 | lr 6.25e-05\n",
      "[Fold 2] Ep 064 | train loss 0.0925 | val AP 0.9749 | lr 3.13e-05\n",
      "[Fold 2] Ep 065 | train loss 0.0839 | val AP 0.9739 | lr 3.13e-05\n",
      "[Fold 2] Ep 066 | train loss 0.0654 | val AP 0.9747 | lr 3.13e-05\n",
      "[Fold 2] Ep 067 | train loss 0.0687 | val AP 0.9747 | lr 3.13e-05\n",
      "[Fold 2] Early stop.\n",
      "[Fold 2] Test metrics: {'ROC-AUC': 0.9425079221367134, 'PRC-AUC': 0.954169996040974, 'Accuracy': 0.851063829787234, 'F1': 0.8372093023255814, 'MCC': 0.7125253031944253, 'Recall': 0.7659574468085106, 'Precision': 0.9230769230769231, 'False Positives': 6, 'False Positive Rate': np.float64(0.06382978723404255)}\n",
      "[Fold 3] Ep 001 | train loss 0.6285 | val AP 0.8954 | lr 2.00e-03\n",
      "[Fold 3] Ep 002 | train loss 0.4083 | val AP 0.9029 | lr 2.00e-03\n",
      "[Fold 3] Ep 003 | train loss 0.3126 | val AP 0.9378 | lr 2.00e-03\n",
      "[Fold 3] Ep 004 | train loss 0.3185 | val AP 0.8520 | lr 2.00e-03\n",
      "[Fold 3] Ep 005 | train loss 0.2617 | val AP 0.8503 | lr 2.00e-03\n",
      "[Fold 3] Ep 006 | train loss 0.3288 | val AP 0.8846 | lr 2.00e-03\n",
      "[Fold 3] Ep 007 | train loss 0.2449 | val AP 0.9154 | lr 2.00e-03\n",
      "[Fold 3] Ep 008 | train loss 0.2380 | val AP 0.9389 | lr 2.00e-03\n",
      "[Fold 3] Ep 009 | train loss 0.2591 | val AP 0.9077 | lr 2.00e-03\n",
      "[Fold 3] Ep 010 | train loss 0.2110 | val AP 0.9339 | lr 2.00e-03\n",
      "[Fold 3] Ep 011 | train loss 0.2153 | val AP 0.9544 | lr 2.00e-03\n",
      "[Fold 3] Ep 012 | train loss 0.2114 | val AP 0.9403 | lr 2.00e-03\n",
      "[Fold 3] Ep 013 | train loss 0.1849 | val AP 0.9577 | lr 2.00e-03\n",
      "[Fold 3] Ep 014 | train loss 0.1886 | val AP 0.9498 | lr 2.00e-03\n",
      "[Fold 3] Ep 015 | train loss 0.2290 | val AP 0.9305 | lr 2.00e-03\n",
      "[Fold 3] Ep 016 | train loss 0.1772 | val AP 0.9399 | lr 2.00e-03\n",
      "[Fold 3] Ep 017 | train loss 0.1899 | val AP 0.9475 | lr 2.00e-03\n",
      "[Fold 3] Ep 018 | train loss 0.1652 | val AP 0.9409 | lr 2.00e-03\n",
      "[Fold 3] Ep 019 | train loss 0.1560 | val AP 0.9158 | lr 1.00e-03\n",
      "[Fold 3] Ep 020 | train loss 0.1483 | val AP 0.9631 | lr 1.00e-03\n",
      "[Fold 3] Ep 021 | train loss 0.1763 | val AP 0.9424 | lr 1.00e-03\n",
      "[Fold 3] Ep 022 | train loss 0.1277 | val AP 0.9547 | lr 1.00e-03\n",
      "[Fold 3] Ep 023 | train loss 0.1404 | val AP 0.9562 | lr 1.00e-03\n",
      "[Fold 3] Ep 024 | train loss 0.1186 | val AP 0.9336 | lr 1.00e-03\n",
      "[Fold 3] Ep 025 | train loss 0.1177 | val AP 0.9502 | lr 1.00e-03\n",
      "[Fold 3] Ep 026 | train loss 0.1036 | val AP 0.9379 | lr 5.00e-04\n",
      "[Fold 3] Ep 027 | train loss 0.1100 | val AP 0.9503 | lr 5.00e-04\n",
      "[Fold 3] Ep 028 | train loss 0.1002 | val AP 0.9389 | lr 5.00e-04\n",
      "[Fold 3] Ep 029 | train loss 0.0851 | val AP 0.9344 | lr 5.00e-04\n",
      "[Fold 3] Ep 030 | train loss 0.1061 | val AP 0.9339 | lr 5.00e-04\n",
      "[Fold 3] Ep 031 | train loss 0.0906 | val AP 0.9401 | lr 5.00e-04\n",
      "[Fold 3] Ep 032 | train loss 0.0916 | val AP 0.9352 | lr 2.50e-04\n",
      "[Fold 3] Ep 033 | train loss 0.0765 | val AP 0.9404 | lr 2.50e-04\n",
      "[Fold 3] Ep 034 | train loss 0.0852 | val AP 0.9426 | lr 2.50e-04\n",
      "[Fold 3] Ep 035 | train loss 0.0980 | val AP 0.9401 | lr 2.50e-04\n",
      "[Fold 3] Early stop.\n",
      "[Fold 3] Test metrics: {'ROC-AUC': 0.9689904934359439, 'PRC-AUC': 0.9651921301716772, 'Accuracy': 0.9042553191489362, 'F1': 0.898876404494382, 'MCC': 0.8131249357707345, 'Recall': 0.851063829787234, 'Precision': 0.9523809523809523, 'False Positives': 4, 'False Positive Rate': np.float64(0.0425531914893617)}\n",
      "[Fold 4] Ep 001 | train loss 0.6299 | val AP 0.5181 | lr 2.00e-03\n",
      "[Fold 4] Ep 002 | train loss 0.4660 | val AP 0.9422 | lr 2.00e-03\n",
      "[Fold 4] Ep 003 | train loss 0.3805 | val AP 0.9369 | lr 2.00e-03\n",
      "[Fold 4] Ep 004 | train loss 0.3105 | val AP 0.8578 | lr 2.00e-03\n",
      "[Fold 4] Ep 005 | train loss 0.2739 | val AP 0.8848 | lr 2.00e-03\n",
      "[Fold 4] Ep 006 | train loss 0.2596 | val AP 0.9587 | lr 2.00e-03\n",
      "[Fold 4] Ep 007 | train loss 0.2675 | val AP 0.8680 | lr 2.00e-03\n",
      "[Fold 4] Ep 008 | train loss 0.2637 | val AP 0.9409 | lr 2.00e-03\n",
      "[Fold 4] Ep 009 | train loss 0.2488 | val AP 0.9552 | lr 2.00e-03\n",
      "[Fold 4] Ep 010 | train loss 0.2236 | val AP 0.9692 | lr 2.00e-03\n",
      "[Fold 4] Ep 011 | train loss 0.2158 | val AP 0.9710 | lr 2.00e-03\n",
      "[Fold 4] Ep 012 | train loss 0.2469 | val AP 0.9837 | lr 2.00e-03\n",
      "[Fold 4] Ep 013 | train loss 0.2113 | val AP 0.9335 | lr 2.00e-03\n",
      "[Fold 4] Ep 014 | train loss 0.2394 | val AP 0.9722 | lr 2.00e-03\n",
      "[Fold 4] Ep 015 | train loss 0.2164 | val AP 0.9496 | lr 2.00e-03\n",
      "[Fold 4] Ep 016 | train loss 0.1965 | val AP 0.9799 | lr 2.00e-03\n",
      "[Fold 4] Ep 017 | train loss 0.2052 | val AP 0.9064 | lr 2.00e-03\n",
      "[Fold 4] Ep 018 | train loss 0.2212 | val AP 0.9585 | lr 1.00e-03\n",
      "[Fold 4] Ep 019 | train loss 0.1870 | val AP 0.9884 | lr 1.00e-03\n",
      "[Fold 4] Ep 020 | train loss 0.1711 | val AP 0.9700 | lr 1.00e-03\n",
      "[Fold 4] Ep 021 | train loss 0.1850 | val AP 0.9933 | lr 1.00e-03\n",
      "[Fold 4] Ep 022 | train loss 0.1610 | val AP 0.9503 | lr 1.00e-03\n",
      "[Fold 4] Ep 023 | train loss 0.1529 | val AP 0.9849 | lr 1.00e-03\n",
      "[Fold 4] Ep 024 | train loss 0.1519 | val AP 0.9541 | lr 1.00e-03\n",
      "[Fold 4] Ep 025 | train loss 0.1495 | val AP 0.9944 | lr 1.00e-03\n",
      "[Fold 4] Ep 026 | train loss 0.1443 | val AP 0.9746 | lr 1.00e-03\n",
      "[Fold 4] Ep 027 | train loss 0.1511 | val AP 0.9702 | lr 1.00e-03\n",
      "[Fold 4] Ep 028 | train loss 0.1620 | val AP 0.9874 | lr 1.00e-03\n",
      "[Fold 4] Ep 029 | train loss 0.1457 | val AP 0.9859 | lr 1.00e-03\n",
      "[Fold 4] Ep 030 | train loss 0.1233 | val AP 0.9927 | lr 1.00e-03\n",
      "[Fold 4] Ep 031 | train loss 0.1362 | val AP 0.9877 | lr 5.00e-04\n",
      "[Fold 4] Ep 032 | train loss 0.1240 | val AP 0.9818 | lr 5.00e-04\n",
      "[Fold 4] Ep 033 | train loss 0.1074 | val AP 0.9803 | lr 5.00e-04\n",
      "[Fold 4] Ep 034 | train loss 0.1238 | val AP 0.9863 | lr 5.00e-04\n",
      "[Fold 4] Ep 035 | train loss 0.1027 | val AP 0.9877 | lr 5.00e-04\n",
      "[Fold 4] Ep 036 | train loss 0.1002 | val AP 0.9913 | lr 5.00e-04\n",
      "[Fold 4] Ep 037 | train loss 0.1048 | val AP 0.9903 | lr 2.50e-04\n",
      "[Fold 4] Ep 038 | train loss 0.0950 | val AP 0.9869 | lr 2.50e-04\n",
      "[Fold 4] Ep 039 | train loss 0.0827 | val AP 0.9861 | lr 2.50e-04\n",
      "[Fold 4] Ep 040 | train loss 0.0853 | val AP 0.9866 | lr 2.50e-04\n",
      "[Fold 4] Early stop.\n",
      "[Fold 4] Test metrics: {'ROC-AUC': 0.9701222272521502, 'PRC-AUC': 0.9781530793338535, 'Accuracy': 0.9521276595744681, 'F1': 0.9518716577540107, 'MCC': 0.9043064923087151, 'Recall': 0.9468085106382979, 'Precision': 0.956989247311828, 'False Positives': 4, 'False Positive Rate': np.float64(0.0425531914893617)}\n",
      "[Fold 5] Ep 001 | train loss 0.6603 | val AP 0.8641 | lr 2.00e-03\n",
      "[Fold 5] Ep 002 | train loss 0.4813 | val AP 0.9298 | lr 2.00e-03\n",
      "[Fold 5] Ep 003 | train loss 0.3483 | val AP 0.9404 | lr 2.00e-03\n",
      "[Fold 5] Ep 004 | train loss 0.3215 | val AP 0.8916 | lr 2.00e-03\n",
      "[Fold 5] Ep 005 | train loss 0.3043 | val AP 0.9043 | lr 2.00e-03\n",
      "[Fold 5] Ep 006 | train loss 0.2852 | val AP 0.9092 | lr 2.00e-03\n",
      "[Fold 5] Ep 007 | train loss 0.2710 | val AP 0.9453 | lr 2.00e-03\n",
      "[Fold 5] Ep 008 | train loss 0.2415 | val AP 0.9282 | lr 2.00e-03\n",
      "[Fold 5] Ep 009 | train loss 0.2744 | val AP 0.9708 | lr 2.00e-03\n",
      "[Fold 5] Ep 010 | train loss 0.2645 | val AP 0.7711 | lr 2.00e-03\n",
      "[Fold 5] Ep 011 | train loss 0.2150 | val AP 0.9639 | lr 2.00e-03\n",
      "[Fold 5] Ep 012 | train loss 0.2153 | val AP 0.9495 | lr 2.00e-03\n",
      "[Fold 5] Ep 013 | train loss 0.2098 | val AP 0.9575 | lr 2.00e-03\n",
      "[Fold 5] Ep 014 | train loss 0.2139 | val AP 0.9791 | lr 2.00e-03\n",
      "[Fold 5] Ep 015 | train loss 0.2011 | val AP 0.9495 | lr 2.00e-03\n",
      "[Fold 5] Ep 016 | train loss 0.1835 | val AP 0.9690 | lr 2.00e-03\n",
      "[Fold 5] Ep 017 | train loss 0.1751 | val AP 0.9476 | lr 2.00e-03\n",
      "[Fold 5] Ep 018 | train loss 0.1928 | val AP 0.9608 | lr 2.00e-03\n",
      "[Fold 5] Ep 019 | train loss 0.1756 | val AP 0.9670 | lr 2.00e-03\n",
      "[Fold 5] Ep 020 | train loss 0.1628 | val AP 0.9282 | lr 1.00e-03\n",
      "[Fold 5] Ep 021 | train loss 0.1531 | val AP 0.9388 | lr 1.00e-03\n",
      "[Fold 5] Ep 022 | train loss 0.1454 | val AP 0.9413 | lr 1.00e-03\n",
      "[Fold 5] Ep 023 | train loss 0.1364 | val AP 0.9387 | lr 1.00e-03\n",
      "[Fold 5] Ep 024 | train loss 0.1362 | val AP 0.9612 | lr 1.00e-03\n",
      "[Fold 5] Ep 025 | train loss 0.1378 | val AP 0.9533 | lr 1.00e-03\n",
      "[Fold 5] Ep 026 | train loss 0.1451 | val AP 0.9651 | lr 5.00e-04\n",
      "[Fold 5] Ep 027 | train loss 0.1177 | val AP 0.9539 | lr 5.00e-04\n",
      "[Fold 5] Ep 028 | train loss 0.1191 | val AP 0.9477 | lr 5.00e-04\n",
      "[Fold 5] Ep 029 | train loss 0.1196 | val AP 0.9636 | lr 5.00e-04\n",
      "[Fold 5] Ep 030 | train loss 0.1210 | val AP 0.9623 | lr 5.00e-04\n",
      "[Fold 5] Early stop.\n",
      "[Fold 5] Test metrics: {'ROC-AUC': 0.9660479855138072, 'PRC-AUC': 0.9723779608130719, 'Accuracy': 0.9148936170212766, 'F1': 0.9139784946236559, 'MCC': 0.8299751174897799, 'Recall': 0.9042553191489362, 'Precision': 0.9239130434782609, 'False Positives': 7, 'False Positive Rate': np.float64(0.07446808510638298)}\n",
      "\n",
      "========== Overall (5-fold aggregated) ==========\n",
      "ROC-AUC: 0.955220\n",
      "PRC-AUC: 0.960153\n",
      "Accuracy: 0.911702\n",
      "F1: 0.908891\n",
      "MCC: 0.824976\n",
      "Recall: 0.880851\n",
      "Precision: 0.938776\n",
      "False Positives: 27\n",
      "False Positive Rate: 0.057447\n",
      "Best fold model saved to: best_gine_model.pth\n",
      "\n",
      "Per-fold metrics -> cv_per_fold.csv\n",
      "Overall metrics -> cv_results.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "GINE + Graph Autoencoder (GAE) undersampling pipeline with explainability\n",
    "for molecular activity prediction.\n",
    "\n",
    "Pipeline\n",
    "--------\n",
    "1) RDKit: convert SMILES -> molecular graphs with rich node/edge features\n",
    "2) Train a Graph Autoencoder (reconstruct node features) to obtain graph embeddings\n",
    "3) Undersample negatives by similarity to the positive centroid in embedding space:\n",
    "   - 'nearest'  : pick the most similar negatives (smallest difference)  [DEFAULT]\n",
    "   - 'farthest' : pick the most dissimilar negatives (largest difference)\n",
    "   Supports cosine (default) or euclidean distance; negative:positive ~ 1:1 (configurable)\n",
    "4) Train a GINE-based graph classifier with Stratified 5-Fold cross-validation\n",
    "5) Report and save metrics per fold and overall:\n",
    "   ROC-AUC, PRC-AUC, Accuracy, F1, MCC, Recall, Precision, False Positives, False Positive Rate\n",
    "6) Save best-fold model weights and optional embeddings CSV\n",
    "7) Explainability (optional via CLI):\n",
    "   - Integrated Gradients for node features and edge features\n",
    "   - Global feature importance (semantic names)\n",
    "   - Highlight most important atoms/bonds on molecule images\n",
    "\n",
    "Requirements\n",
    "------------\n",
    "- rdkit\n",
    "- torch, torch_geometric (>= 2.2 recommended)\n",
    "- scikit-learn, pandas, numpy, tqdm\n",
    "- rdkit.Chem.Draw (for visualization)\n",
    "\n",
    "Input\n",
    "-----\n",
    "CSV with:\n",
    "- 'smiles' (string)\n",
    "- 'antibiotic_activity' (0/1)\n",
    "\n",
    "Usage\n",
    "-----\n",
    "Train + explain top 5 molecules (save PNGs to ./explain_out):\n",
    "    python train_gcn_gae_pipeline.py --csv ./data/raw_data.csv \\\n",
    "        --pick nearest --metric cosine --ratio 1.0 \\\n",
    "        --explain 5 --explain-steps 64 --explain-save-dir ./explain_out\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# PyG\n",
    "from torch_geometric.data import Data\n",
    "try:\n",
    "    from torch_geometric.loader import DataLoader\n",
    "except Exception:  # backward compatibility\n",
    "    from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool, GINEConv, BatchNorm\n",
    "\n",
    "# RDKit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    matthews_corrcoef,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "DEFAULT_CSV = \"./data/raw_data.csv\"\n",
    "RESULT_CSV = \"cv_results.csv\"\n",
    "FOLD_DETAIL_CSV = \"cv_per_fold.csv\"\n",
    "BEST_MODEL_PATH = \"best_gine_model.pth\"\n",
    "EMBED_CSV = \"graph_embeddings.csv\"\n",
    "\n",
    "BATCH_SIZE_AE = 64\n",
    "BATCH_SIZE_CLS = 64\n",
    "EPOCHS_AE = 60\n",
    "EPOCHS_CLS = 120\n",
    "PATIENCE = 15\n",
    "LR_AE = 1e-3\n",
    "LR_CLS = 2e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "DROPOUT = 0.2\n",
    "HIDDEN = 128\n",
    "NUM_GINE = 3\n",
    "VAL_SPLIT = 0.10\n",
    "DIST_METRIC = \"cosine\"        # 'cosine' or 'euclidean'\n",
    "NEG_POS_RATIO = 1.0\n",
    "DEFAULT_PICK = \"nearest\"      # 'nearest' or 'farthest'\n",
    "\n",
    "# Common atomic numbers; everything else goes to \"other\"\n",
    "COMMON_Z = [1, 5, 6, 7, 8, 9, 14, 15, 16, 17, 19, 11, 12, 20, 26, 29, 30, 35, 53]\n",
    "# H,B,C,N,O,F,Si,P,S,Cl,K,Na,Mg,Ca,Fe,Cu,Zn,Br,I\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def set_seed(seed: int = SEED) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def one_hot(val, choices):\n",
    "    vec = [0] * len(choices)\n",
    "    if val in choices:\n",
    "        vec[choices.index(val)] = 1\n",
    "    return vec\n",
    "\n",
    "\n",
    "def atom_features(atom: Chem.rdchem.Atom) -> List[float]:\n",
    "    \"\"\"\n",
    "    Node features:\n",
    "      - atomic number: one-hot (COMMON_Z + 'other')\n",
    "      - degree: one-hot [0..5]\n",
    "      - hybridization: one-hot {sp, sp2, sp3, sp3d, sp3d2, other}\n",
    "      - formal charge: one-hot [-2..2]\n",
    "      - total hydrogens: one-hot [0..4]\n",
    "      - aromatic (bool)\n",
    "      - in ring (bool)\n",
    "      - chirality tag: one-hot {unspecified, CW, CCW}\n",
    "    \"\"\"\n",
    "    z = atom.GetAtomicNum()\n",
    "    z_onehot = one_hot(z if z in COMMON_Z else -1, COMMON_Z + [-1])\n",
    "\n",
    "    degree = atom.GetTotalDegree()\n",
    "    degree_onehot = one_hot(min(degree, 5), list(range(6)))\n",
    "\n",
    "    hyb = atom.GetHybridization()\n",
    "    hyb_choices = [\n",
    "        Chem.rdchem.HybridizationType.SP,\n",
    "        Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3,\n",
    "        Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2,\n",
    "    ]\n",
    "    hyb_onehot = one_hot(hyb if hyb in hyb_choices else None, hyb_choices + [None])\n",
    "\n",
    "    charge = int(atom.GetFormalCharge())\n",
    "    charge = max(-2, min(2, charge))\n",
    "    charge_onehot = one_hot(charge, [-2, -1, 0, 1, 2])\n",
    "\n",
    "    num_h = min(atom.GetTotalNumHs(), 4)\n",
    "    num_h_onehot = one_hot(num_h, [0, 1, 2, 3, 4])\n",
    "\n",
    "    aromatic = [int(atom.GetIsAromatic())]\n",
    "    ring = [int(atom.IsInRing())]\n",
    "\n",
    "    chiral_tag = atom.GetChiralTag()\n",
    "    chiral_choices = [\n",
    "        Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "    ]\n",
    "    chiral_onehot = one_hot(\n",
    "        chiral_tag if chiral_tag in chiral_choices else Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "        chiral_choices,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        z_onehot\n",
    "        + degree_onehot\n",
    "        + hyb_onehot\n",
    "        + charge_onehot\n",
    "        + num_h_onehot\n",
    "        + aromatic\n",
    "        + ring\n",
    "        + chiral_onehot\n",
    "    )\n",
    "\n",
    "\n",
    "def bond_features(bond: Chem.rdchem.Bond) -> List[float]:\n",
    "    \"\"\"\n",
    "    Edge features:\n",
    "      - bond type: one-hot {single,double,triple,aromatic,other}\n",
    "      - conjugated (bool)\n",
    "      - in ring (bool)\n",
    "      - stereo: one-hot {none, Z, E}\n",
    "    \"\"\"\n",
    "    bt = bond.GetBondType()\n",
    "    bt_choices = [\n",
    "        Chem.BondType.SINGLE,\n",
    "        Chem.BondType.DOUBLE,\n",
    "        Chem.BondType.TRIPLE,\n",
    "        Chem.BondType.AROMATIC,\n",
    "    ]\n",
    "    bt_onehot = one_hot(bt if bt in bt_choices else None, bt_choices + [None])\n",
    "\n",
    "    conj = [int(bond.GetIsConjugated())]\n",
    "    ring = [int(bond.IsInRing())]\n",
    "\n",
    "    stereo = bond.GetStereo()\n",
    "    stereo_choices = [\n",
    "        Chem.rdchem.BondStereo.STEREONONE,\n",
    "        Chem.rdchem.BondStereo.STEREOZ,\n",
    "        Chem.rdchem.BondStereo.STEREOE,\n",
    "    ]\n",
    "    stereo_onehot = one_hot(\n",
    "        stereo if stereo in stereo_choices else Chem.rdchem.BondStereo.STEREONONE, stereo_choices\n",
    "    )\n",
    "\n",
    "    return bt_onehot + conj + ring + stereo_onehot\n",
    "\n",
    "\n",
    "def smiles_to_graph(smiles: str):\n",
    "    \"\"\"Build a PyG `Data` object from a SMILES string.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    # Keep aromatic flags; 2D coords are sufficient for this pipeline\n",
    "    Chem.Kekulize(mol, clearAromaticFlags=False)\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "\n",
    "    # Node features\n",
    "    x = [atom_features(a) for a in mol.GetAtoms()]\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "    # Edges + edge features (bidirectional)\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        f = bond_features(b)\n",
    "        edge_index.append([i, j]); edge_attr.append(f)\n",
    "        edge_index.append([j, i]); edge_attr.append(f)\n",
    "\n",
    "    if len(edge_index) == 0:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        dummy = Chem.MolFromSmiles(\"CC\").GetBonds()[0]\n",
    "        feat_w = len(bond_features(dummy))\n",
    "        edge_attr = torch.empty((0, feat_w), dtype=torch.float)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "    g = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    g.smiles = smiles  # keep for visualization/explainability\n",
    "    return g\n",
    "\n",
    "# -----------------------------\n",
    "# Models\n",
    "# -----------------------------\n",
    "def mlp(in_dim: int, out_dim: int) -> nn.Sequential:\n",
    "    return nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Linear(out_dim, out_dim))\n",
    "\n",
    "\n",
    "class GINEEncoder(nn.Module):\n",
    "    \"\"\"GINE encoder with BatchNorm, dropout and a light residual connection.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, edge_dim: int, hidden: int = HIDDEN, num_layers: int = NUM_GINE, dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # First layer\n",
    "        self.convs.append(GINEConv(mlp(in_dim, hidden), edge_dim=edge_dim))\n",
    "        self.bns.append(BatchNorm(hidden))\n",
    "\n",
    "        # Subsequent layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GINEConv(mlp(hidden, hidden), edge_dim=edge_dim))\n",
    "            self.bns.append(BatchNorm(hidden))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        h = x\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            h_res = h\n",
    "            h = conv(h, edge_index, edge_attr)\n",
    "            h = bn(h)\n",
    "            h = F.relu(h)\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            if h_res.shape == h.shape:\n",
    "                h = h + 0.1 * h_res  # tiny residual\n",
    "        return h  # node embeddings\n",
    "\n",
    "\n",
    "class GraphAE(nn.Module):\n",
    "    \"\"\"Graph Autoencoder: node encoder -> reconstruct node features; returns graph embedding.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, edge_dim: int, hidden: int = HIDDEN, num_layers: int = NUM_GINE, dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.encoder = GINEEncoder(in_dim, edge_dim, hidden, num_layers, dropout)\n",
    "        self.decoder = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, in_dim))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        h = self.encoder(x, edge_index, edge_attr)  # [N, hidden]\n",
    "        x_hat = self.decoder(h)                     # [N, in_dim]\n",
    "        g = global_mean_pool(h, batch)              # [B, hidden]\n",
    "        return x_hat, g\n",
    "\n",
    "    def encode_nodes(self, x, edge_index, edge_attr):\n",
    "        return self.encoder(x, edge_index, edge_attr)\n",
    "\n",
    "\n",
    "class GINEClassifier(nn.Module):\n",
    "    \"\"\"Graph-level classifier; encoder can be initialized from a trained AE.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, edge_dim: int, hidden: int = HIDDEN, num_layers: int = NUM_GINE, dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.encoder = GINEEncoder(in_dim, edge_dim, hidden, num_layers, dropout)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        h = self.encoder(x, edge_index, edge_attr)\n",
    "        g = global_mean_pool(h, batch)\n",
    "        logit = self.head(g).view(-1)\n",
    "        return logit\n",
    "\n",
    "    def load_from_ae(self, ae: GraphAE):\n",
    "        self.encoder.load_state_dict(ae.encoder.state_dict(), strict=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Training & evaluation helpers\n",
    "# -----------------------------\n",
    "def train_ae(ae: GraphAE, loader: DataLoader, device, epochs: int = EPOCHS_AE, lr: float = LR_AE,\n",
    "             wd: float = WEIGHT_DECAY, patience: int = PATIENCE) -> GraphAE:\n",
    "    \"\"\"Train GraphAE with node feature reconstruction loss (MSE).\"\"\"\n",
    "    ae = ae.to(device)\n",
    "    opt = torch.optim.Adam(ae.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    try:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=5, verbose=False)\n",
    "    except TypeError:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=5)\n",
    "\n",
    "    best_loss, bad = float(\"inf\"), 0\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        ae.train()\n",
    "        total = 0.0\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            x_hat, _ = ae(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            loss = F.mse_loss(x_hat, data.x)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(ae.parameters(), 2.0)\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "\n",
    "        mean_loss = total / max(len(loader), 1)\n",
    "        scheduler.step(mean_loss)\n",
    "        print(f\"[AE] Epoch {ep:03d} | recon MSE {mean_loss:.5f} | lr {opt.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        if mean_loss < best_loss - 1e-5:\n",
    "            best_loss, bad = mean_loss, 0\n",
    "            torch.save(ae.state_dict(), \"best_graph_ae.pth\")\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience and ep >= 20:\n",
    "                print(\"[AE] Early stop.\")\n",
    "                break\n",
    "\n",
    "    ae.load_state_dict(torch.load(\"best_graph_ae.pth\", map_location=device))\n",
    "    ae.eval()\n",
    "    return ae\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_graph_embeddings(ae: GraphAE, loader: DataLoader, device) -> np.ndarray:\n",
    "    \"\"\"Return graph embeddings [N_graphs, hidden] from a trained AE encoder.\"\"\"\n",
    "    ae.eval()\n",
    "    embs = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        h = ae.encode_nodes(data.x, data.edge_index, data.edge_attr)\n",
    "        g = global_mean_pool(h, data.batch)  # [B, hidden]\n",
    "        embs.append(g.cpu().numpy())\n",
    "    return np.concatenate(embs, axis=0)\n",
    "\n",
    "\n",
    "def select_negatives_by_similarity(\n",
    "    embs: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    metric: str = DIST_METRIC,\n",
    "    ratio: float = NEG_POS_RATIO,\n",
    "    pick: str = DEFAULT_PICK\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Select negatives relative to the positive centroid in the embedding space.\"\"\"\n",
    "    assert pick in (\"nearest\", \"farthest\"), \"pick must be 'nearest' or 'farthest'\"\n",
    "\n",
    "    pos_idx = np.where(labels == 1)[0]\n",
    "    neg_idx = np.where(labels == 0)[0]\n",
    "    if len(pos_idx) == 0 or len(neg_idx) == 0:\n",
    "        return np.ones_like(labels, dtype=bool)\n",
    "\n",
    "    pos_centroid = embs[pos_idx].mean(axis=0, keepdims=True)\n",
    "    neg_embs = embs[neg_idx]\n",
    "\n",
    "    if metric == \"cosine\":\n",
    "        a = neg_embs / (np.linalg.norm(neg_embs, axis=1, keepdims=True) + 1e-9)\n",
    "        b = pos_centroid / (np.linalg.norm(pos_centroid, axis=1, keepdims=True) + 1e-9)\n",
    "        sim = (a @ b.T).reshape(-1)  # larger = more similar\n",
    "        order = np.argsort(-sim) if pick == \"nearest\" else np.argsort(sim)\n",
    "    elif metric == \"euclidean\":\n",
    "        dist = np.linalg.norm(neg_embs - pos_centroid, axis=1)  # smaller = more similar\n",
    "        order = np.argsort(dist) if pick == \"nearest\" else np.argsort(-dist)\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'cosine' or 'euclidean'\")\n",
    "\n",
    "    k = int(round(len(pos_idx) * ratio))\n",
    "    chosen_neg = neg_idx[order[:k]]\n",
    "\n",
    "    mask = np.zeros_like(labels, dtype=bool)\n",
    "    mask[pos_idx] = True\n",
    "    mask[chosen_neg] = True\n",
    "    return mask\n",
    "\n",
    "\n",
    "def train_one_epoch_cls(model: nn.Module, loader: DataLoader, optimizer, criterion, device) -> float:\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        y = data.y.view(-1).to(device)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "    return total / max(len(loader), 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(model: nn.Module, loader: DataLoader, device):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        logits = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        prob = torch.sigmoid(logits)\n",
    "        ys.append(data.y.view(-1).cpu().numpy())\n",
    "        ps.append(prob.cpu().numpy())\n",
    "    y_true = np.concatenate(ys) if ys else np.array([])\n",
    "    y_pred = np.concatenate(ps) if ps else np.array([])\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def calc_metrics(y_true: np.ndarray, y_prob: np.ndarray, threshold: float = 0.5):\n",
    "    \"\"\"Return required metrics, including FP and FPR.\"\"\"\n",
    "    if y_true.size == 0:\n",
    "        keys = [\n",
    "            \"ROC-AUC\", \"PRC-AUC\", \"Accuracy\", \"F1\", \"MCC\",\n",
    "            \"Recall\", \"Precision\", \"False Positives\", \"False Positive Rate\",\n",
    "        ]\n",
    "        return {k: float(\"nan\") for k in keys}\n",
    "\n",
    "    y_hat = (y_prob >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat, labels=[0, 1]).ravel()\n",
    "    fpr = fp / max((fp + tn), 1)\n",
    "\n",
    "    return {\n",
    "        \"ROC-AUC\": roc_auc_score(y_true, y_prob),\n",
    "        \"PRC-AUC\": average_precision_score(y_true, y_prob),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_hat),\n",
    "        \"F1\": f1_score(y_true, y_hat, zero_division=0),\n",
    "        \"MCC\": matthews_corrcoef(y_true, y_hat),\n",
    "        \"Recall\": recall_score(y_true, y_hat, zero_division=0),\n",
    "        \"Precision\": precision_score(y_true, y_hat, zero_division=0),\n",
    "        \"False Positives\": int(fp),\n",
    "        \"False Positive Rate\": fpr,\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Explainability helpers\n",
    "# -----------------------------\n",
    "def build_node_feature_names() -> List[str]:\n",
    "    \"\"\"Return semantic names for each dimension in node features (must match atom_features order).\"\"\"\n",
    "    z_names = [f\"Z={Chem.GetPeriodicTable().GetElementSymbol(z)}\" for z in COMMON_Z] + [\"Z=other\"]\n",
    "    deg_names = [f\"deg={d}\" for d in range(6)]\n",
    "    hyb_names = [\"hyb=sp\", \"hyb=sp2\", \"hyb=sp3\", \"hyb=sp3d\", \"hyb=sp3d2\", \"hyb=other\"]\n",
    "    charge_names = [f\"charge={c}\" for c in [-2, -1, 0, 1, 2]]\n",
    "    numh_names = [f\"numH={h}\" for h in range(5)]\n",
    "    flags = [\"aromatic\", \"in_ring\"]\n",
    "    chiral = [\"chiral=unspecified\", \"chiral=CW\", \"chiral=CCW\"]\n",
    "    return z_names + deg_names + hyb_names + charge_names + numh_names + flags + chiral\n",
    "\n",
    "\n",
    "def build_edge_feature_names() -> List[str]:\n",
    "    \"\"\"Return semantic names for each dimension in edge features (must match bond_features order).\"\"\"\n",
    "    bond_names = [\"bond=single\", \"bond=double\", \"bond=triple\", \"bond=aromatic\", \"bond=other\"]\n",
    "    flags = [\"conjugated\", \"bond_in_ring\"]\n",
    "    stereo = [\"stereo=none\", \"stereo=Z\", \"stereo=E\"]\n",
    "    return bond_names + flags + stereo\n",
    "\n",
    "\n",
    "def integrated_gradients_graph(\n",
    "    model: nn.Module,\n",
    "    data: Data,\n",
    "    steps: int = 64,\n",
    "    device: str = \"cpu\",\n",
    "    baseline_x: torch.Tensor = None,\n",
    "    baseline_e: torch.Tensor = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Integrated Gradients on node features (x) and edge features (edge_attr) for a single-graph Data.\n",
    "    Returns:\n",
    "        attr_x: [N_nodes, x_dim]\n",
    "        attr_e: [N_edges, e_dim]  (directed edges as in Data)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x = data.x.clone().detach().to(device)\n",
    "    e = data.edge_attr.clone().detach().to(device) if data.edge_attr is not None else None\n",
    "    edge_index = data.edge_index.to(device)\n",
    "    batch = getattr(data, \"batch\", torch.zeros(x.size(0), dtype=torch.long, device=device))\n",
    "\n",
    "    x.requires_grad_(True)\n",
    "    if e is not None:\n",
    "        e.requires_grad_(True)\n",
    "\n",
    "    if baseline_x is None:\n",
    "        baseline_x = torch.zeros_like(x, device=device)\n",
    "    if e is not None and baseline_e is None:\n",
    "        baseline_e = torch.zeros_like(e, device=device)\n",
    "\n",
    "    total_grad_x = torch.zeros_like(x, device=device)\n",
    "    total_grad_e = torch.zeros_like(e, device=device) if e is not None else None\n",
    "\n",
    "    for i in range(1, steps + 1):\n",
    "        alpha = float(i) / steps\n",
    "        x_int = baseline_x + alpha * (x - baseline_x)\n",
    "        x_int.requires_grad_(True)\n",
    "        if e is not None:\n",
    "            e_int = baseline_e + alpha * (e - baseline_e)\n",
    "            e_int.requires_grad_(True)\n",
    "        else:\n",
    "            e_int = None\n",
    "\n",
    "        logit = model(x_int, edge_index, e_int, batch).view(-1)[0]\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        if x_int.grad is not None: x_int.grad.zero_()\n",
    "        if e_int is not None and e_int.grad is not None: e_int.grad.zero_()\n",
    "        logit.backward(retain_graph=True)\n",
    "\n",
    "        total_grad_x += x_int.grad\n",
    "        if e_int is not None:\n",
    "            total_grad_e += e_int.grad\n",
    "\n",
    "    avg_grad_x = total_grad_x / steps\n",
    "    attr_x = (x - baseline_x) * avg_grad_x\n",
    "    if e is not None:\n",
    "        avg_grad_e = total_grad_e / steps\n",
    "        attr_e = (e - baseline_e) * avg_grad_e\n",
    "    else:\n",
    "        attr_e = None\n",
    "    return attr_x.detach(), attr_e.detach() if attr_e is not None else None\n",
    "\n",
    "\n",
    "def aggregate_feature_importance_over_dataset(\n",
    "    model: nn.Module,\n",
    "    dataset: List[Data],\n",
    "    device: str,\n",
    "    steps: int = 64,\n",
    "    max_samples: int = 256\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compute global (dataset-level) feature importance by averaging |IG| over samples.\n",
    "    Returns:\n",
    "        node_feat_df: DataFrame with columns ['feature', 'abs_importance']\n",
    "        edge_feat_df: DataFrame with columns ['feature', 'abs_importance']  (empty if no edges)\n",
    "    \"\"\"\n",
    "    node_names = build_node_feature_names()\n",
    "    edge_names = build_edge_feature_names()\n",
    "\n",
    "    node_accum = np.zeros(len(node_names), dtype=float)\n",
    "    edge_accum = np.zeros(len(edge_names), dtype=float)\n",
    "\n",
    "    device = torch.device(device)\n",
    "    model.eval()\n",
    "\n",
    "    count_graphs = 0\n",
    "    for g in tqdm(dataset[:max_samples], desc=\"IG(dataset)\"):\n",
    "        gg = Data(x=g.x, edge_index=g.edge_index, edge_attr=g.edge_attr)\n",
    "        gg.batch = torch.zeros(gg.x.size(0), dtype=torch.long)\n",
    "        gg = gg.to(device)\n",
    "        attr_x, attr_e = integrated_gradients_graph(model, gg, steps=steps, device=device)\n",
    "\n",
    "        # node feature importance: mean over nodes of |attr|\n",
    "        node_imp = attr_x.abs().mean(dim=0).cpu().numpy()\n",
    "        node_accum += node_imp\n",
    "\n",
    "        # edge feature importance: mean over edges of |attr|\n",
    "        if attr_e is not None and attr_e.numel() > 0:\n",
    "            e_imp = attr_e.abs().mean(dim=0).cpu().numpy()\n",
    "            # pad if current model edge_dim < edge_names (should not happen if feature def consistent)\n",
    "            if len(e_imp) < len(edge_accum):\n",
    "                tmp = np.zeros_like(edge_accum); tmp[:len(e_imp)] = e_imp; e_imp = tmp\n",
    "            edge_accum += e_imp\n",
    "        count_graphs += 1\n",
    "\n",
    "    if count_graphs == 0:\n",
    "        count_graphs = 1\n",
    "\n",
    "    node_feat_df = pd.DataFrame({\n",
    "        \"feature\": node_names[:len(node_accum)],\n",
    "        \"abs_importance\": node_accum / count_graphs\n",
    "    }).sort_values(\"abs_importance\", ascending=False)\n",
    "\n",
    "    # if no edge features, return empty df\n",
    "    if edge_accum.sum() == 0:\n",
    "        edge_feat_df = pd.DataFrame(columns=[\"feature\", \"abs_importance\"])\n",
    "    else:\n",
    "        edge_feat_df = pd.DataFrame({\n",
    "            \"feature\": edge_names[:len(edge_accum)],\n",
    "            \"abs_importance\": edge_accum / count_graphs\n",
    "        }).sort_values(\"abs_importance\", ascending=False)\n",
    "\n",
    "    return node_feat_df, edge_feat_df\n",
    "\n",
    "\n",
    "def rank_nodes_and_edges_from_ig(attr_x: torch.Tensor, attr_e: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Collapse per-dimension attributions to per-node/per-edge scores (L1 norm).\n",
    "    Returns:\n",
    "        node_scores: [N_nodes] numpy\n",
    "        edge_scores: [N_edges] numpy (None if attr_e is None)\n",
    "    \"\"\"\n",
    "    node_scores = attr_x.abs().sum(dim=1).detach().cpu().numpy()\n",
    "    edge_scores = attr_e.abs().sum(dim=1).detach().cpu().numpy() if attr_e is not None else None\n",
    "    return node_scores, edge_scores\n",
    "\n",
    "\n",
    "def draw_highlight_smiles(smiles: str, node_scores: np.ndarray, edge_index: torch.Tensor,\n",
    "                          edge_scores: np.ndarray = None, topk_nodes: int = 10, topk_edges: int = 10,\n",
    "                          out_png: str = \"explain.png\"):\n",
    "    \"\"\"\n",
    "    Draw molecule with highlighted top-k atoms/bonds by importance scores.\n",
    "    \"\"\"\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return\n",
    "\n",
    "    # top-k atoms\n",
    "    node_order = np.argsort(-node_scores)\n",
    "    hl_atoms = set(node_order[:min(topk_nodes, len(node_scores))].tolist())\n",
    "\n",
    "    # prepare bond mapping (PyG edges are directed; RDKit bonds are undirected)\n",
    "    bonds = []\n",
    "    if edge_scores is not None and edge_index is not None and edge_index.size(1) > 0:\n",
    "        # Merge the two directions by taking max importance among (i->j) and (j->i)\n",
    "        m = {}\n",
    "        for k in range(edge_index.size(1)):\n",
    "            i, j = int(edge_index[0, k].item()), int(edge_index[1, k].item())\n",
    "            if i == j:\n",
    "                continue\n",
    "            key = tuple(sorted((i, j)))\n",
    "            score = edge_scores[k]\n",
    "            m[key] = max(m.get(key, 0.0), score)\n",
    "        # rank bonds\n",
    "        uniq_pairs = list(m.items())\n",
    "        uniq_pairs.sort(key=lambda x: -x[1])\n",
    "        bonds = [pair for pair, _ in uniq_pairs[:min(topk_edges, len(uniq_pairs))]]\n",
    "\n",
    "    # convert bonds to RDKit bond indices\n",
    "    hl_bonds_idx = set()\n",
    "    for i, j in bonds:\n",
    "        b = mol.GetBondBetweenAtoms(int(i), int(j))\n",
    "        if b is not None:\n",
    "            hl_bonds_idx.add(b.GetIdx())\n",
    "\n",
    "    # RDKit draw\n",
    "    atom_colors = {idx: (1.0, 0.2, 0.2) for idx in hl_atoms}  # red-ish for atoms\n",
    "    bond_colors = {idx: (0.2, 0.2, 1.0) for idx in hl_bonds_idx}  # blue-ish for bonds\n",
    "    drawer = Draw.MolDraw2DCairo(800, 600)\n",
    "    Draw.rdMolDraw2D.PrepareAndDrawMolecule(\n",
    "        drawer, mol,\n",
    "        highlightAtoms=list(hl_atoms),\n",
    "        highlightBonds=list(hl_bonds_idx),\n",
    "        highlightAtomColors=atom_colors,\n",
    "        highlightBondColors=bond_colors\n",
    "    )\n",
    "    drawer.FinishDrawing()\n",
    "    os.makedirs(os.path.dirname(out_png) or \".\", exist_ok=True)\n",
    "    drawer.WriteDrawingText(out_png)\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main(args) -> None:\n",
    "    set_seed(SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 1) Load data\n",
    "    df = pd.read_csv(args.csv)\n",
    "    assert \"smiles\" in df.columns and \"antibiotic_activity\" in df.columns, \\\n",
    "        \"CSV must contain columns 'smiles' and 'antibiotic_activity'.\"\n",
    "    smiles = df[\"smiles\"].astype(str).tolist()\n",
    "    labels = df[\"antibiotic_activity\"].astype(int).to_numpy()\n",
    "\n",
    "    # 2) Build graphs\n",
    "    data_list: List[Data] = []\n",
    "    drop_idx = []\n",
    "    for i, smi in enumerate(tqdm(smiles, desc=\"SMILES->Graph\")):\n",
    "        g = smiles_to_graph(smi)\n",
    "        if g is None or g.x.numel() == 0:\n",
    "            drop_idx.append(i); continue\n",
    "        g.y = torch.tensor([labels[i]], dtype=torch.float32)\n",
    "        data_list.append(g)\n",
    "\n",
    "    if len(data_list) == 0:\n",
    "        raise RuntimeError(\"No valid molecules after SMILES->graph conversion.\")\n",
    "\n",
    "    if drop_idx:\n",
    "        print(f\"Warning: {len(drop_idx)} SMILES failed to convert and were skipped.\")\n",
    "        labels = np.delete(labels, drop_idx, axis=0)\n",
    "\n",
    "    in_dim = data_list[0].x.size(1)\n",
    "    edge_dim = data_list[0].edge_attr.size(1) if data_list[0].edge_attr is not None else 0\n",
    "    print(f\"Node feat dim = {in_dim} | Edge feat dim = {edge_dim} | N graphs = {len(data_list)}\")\n",
    "\n",
    "    # 3) Train Graph AE\n",
    "    ae_loader = DataLoader(data_list, batch_size=BATCH_SIZE_AE, shuffle=True, num_workers=0)\n",
    "    ae = GraphAE(in_dim, edge_dim, hidden=HIDDEN, num_layers=NUM_GINE, dropout=DROPOUT)\n",
    "    ae = train_ae(ae, ae_loader, device)\n",
    "\n",
    "    # 4) Embeddings & undersampling\n",
    "    eval_loader = DataLoader(data_list, batch_size=BATCH_SIZE_AE, shuffle=False, num_workers=0)\n",
    "    embs = get_graph_embeddings(ae, eval_loader, device)  # [N, hidden]\n",
    "    pd.DataFrame(embs).to_csv(EMBED_CSV, index=False)\n",
    "\n",
    "    pick_mode = getattr(args, \"pick\", DEFAULT_PICK)\n",
    "    dist_metric = getattr(args, \"metric\", DIST_METRIC)\n",
    "    ratio = float(getattr(args, \"ratio\", NEG_POS_RATIO))\n",
    "\n",
    "    mask = select_negatives_by_similarity(\n",
    "        embs, labels, metric=dist_metric, ratio=ratio, pick=pick_mode\n",
    "    )\n",
    "    data_balanced = [d for d, m in zip(data_list, mask) if m]\n",
    "    labels_balanced = labels[mask]\n",
    "    print(\n",
    "        f\"After undersampling by '{pick_mode}' ({dist_metric}) similarity: \"\n",
    "        f\"pos={labels_balanced.sum()} neg={(1 - labels_balanced).sum()} total={len(data_balanced)}\"\n",
    "    )\n",
    "\n",
    "    # 5) 5-fold CV training\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    fold_metrics = []\n",
    "    all_true, all_prob = [], []\n",
    "    best_score = -1.0\n",
    "\n",
    "    for fold, (tr_idx, te_idx) in enumerate(skf.split(np.arange(len(data_balanced)), labels_balanced), 1):\n",
    "        train_subset = [data_balanced[i] for i in tr_idx]\n",
    "        test_subset  = [data_balanced[i] for i in te_idx]\n",
    "        y_train = labels_balanced[tr_idx]\n",
    "        y_test  = labels_balanced[te_idx]\n",
    "\n",
    "        tr_part, val_part, _, _ = train_test_split(\n",
    "            train_subset, y_train, test_size=VAL_SPLIT, stratify=y_train, random_state=SEED\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(tr_part, batch_size=BATCH_SIZE_CLS, shuffle=True, num_workers=0)\n",
    "        val_loader   = DataLoader(val_part, batch_size=BATCH_SIZE_CLS, shuffle=False, num_workers=0)\n",
    "        test_loader  = DataLoader(test_subset, batch_size=BATCH_SIZE_CLS, shuffle=False, num_workers=0)\n",
    "\n",
    "        model = GINEClassifier(in_dim, edge_dim, hidden=HIDDEN, num_layers=NUM_GINE, dropout=DROPOUT).to(device)\n",
    "        model.load_from_ae(ae)\n",
    "\n",
    "        pos_count = float((y_train == 1).sum()); neg_count = float((y_train == 0).sum())\n",
    "        pos_weight = torch.tensor([(neg_count / max(pos_count, 1.0))], device=device, dtype=torch.float32)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR_CLS, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=5)\n",
    "\n",
    "        best_val, bad = -1.0, 0\n",
    "        best_state = None\n",
    "\n",
    "        for ep in range(1, EPOCHS_CLS + 1):\n",
    "            loss = train_one_epoch_cls(model, train_loader, optimizer, criterion, device)\n",
    "            yv, pv = infer(model, val_loader, device)\n",
    "            val_ap = average_precision_score(yv, pv) if yv.size > 0 else 0.0\n",
    "            scheduler.step(val_ap)\n",
    "            print(\n",
    "                f\"[Fold {fold}] Ep {ep:03d} | train loss {loss:.4f} | val AP {val_ap:.4f} \"\n",
    "                f\"| lr {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "            )\n",
    "\n",
    "            if val_ap > best_val + 1e-5:\n",
    "                best_val, bad = val_ap, 0\n",
    "                best_state = model.state_dict()\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= PATIENCE and ep >= 30:\n",
    "                    print(f\"[Fold {fold}] Early stop.\")\n",
    "                    break\n",
    "\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        yt, pt = infer(model, test_loader, device)\n",
    "        all_true.append(yt); all_prob.append(pt)\n",
    "        m = calc_metrics(yt, pt, threshold=0.5)\n",
    "        fold_metrics.append({\"Fold\": fold, **m})\n",
    "        print(f\"[Fold {fold}] Test metrics: {m}\")\n",
    "\n",
    "        if m[\"ROC-AUC\"] > best_score:\n",
    "            best_score = m[\"ROC-AUC\"]\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "\n",
    "    pd.DataFrame(fold_metrics).to_csv(FOLD_DETAIL_CSV, index=False)\n",
    "\n",
    "    all_true = np.concatenate(all_true); all_prob = np.concatenate(all_prob)\n",
    "    overall_metrics = calc_metrics(all_true, all_prob, threshold=0.5)\n",
    "\n",
    "    print(\"\\n========== Overall (5-fold aggregated) ==========\")\n",
    "    for k, v in overall_metrics.items():\n",
    "        print(f\"{k}: {v:.6f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "    print(f\"Best fold model saved to: {BEST_MODEL_PATH}\")\n",
    "\n",
    "    out_row = {\n",
    "        \"Model\": f\"GINE(AE init) + {pick_mode} undersampling ({dist_metric}), ratio={ratio}\",\n",
    "        **overall_metrics,\n",
    "    }\n",
    "    pd.DataFrame([out_row]).to_csv(RESULT_CSV, index=False)\n",
    "    print(f\"\\nPer-fold metrics -> {FOLD_DETAIL_CSV}\")\n",
    "    print(f\"Overall metrics -> {RESULT_CSV}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6) Optional explainability\n",
    "    # -----------------------------\n",
    "    if getattr(args, \"explain\", 0) and getattr(args, \"explain\", 0) > 0:\n",
    "        explain_n = int(getattr(args, \"explain\", 0))\n",
    "        explain_steps = int(getattr(args, \"explain_steps\", 64))\n",
    "        save_dir = getattr(args, \"explain_save_dir\", \"./explain_out\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Load the best model picked above\n",
    "        expl_model = GINEClassifier(in_dim, edge_dim, hidden=HIDDEN, num_layers=NUM_GINE, dropout=DROPOUT).to(device)\n",
    "        expl_model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "        expl_model.eval()\n",
    "\n",
    "        # Pick top-N confident positives and negatives from the balanced set\n",
    "        loader_all = DataLoader(data_balanced, batch_size=BATCH_SIZE_CLS, shuffle=False, num_workers=0)\n",
    "        _, prob_all = infer(expl_model, loader_all, device)\n",
    "        prob_all = prob_all.reshape(-1)\n",
    "        idx_sorted_pos = np.argsort(-prob_all)  # most positive\n",
    "        idx_sorted_neg = np.argsort(prob_all)   # most negative\n",
    "\n",
    "        picks = []\n",
    "        for i in idx_sorted_pos[:explain_n]:\n",
    "            picks.append((\"pos\", i))\n",
    "        for i in idx_sorted_neg[:explain_n]:\n",
    "            picks.append((\"neg\", i))\n",
    "\n",
    "        # Global feature importance (on a sample up to 256 graphs)\n",
    "        node_df, edge_df = aggregate_feature_importance_over_dataset(\n",
    "            expl_model, data_balanced, device=device, steps=explain_steps, max_samples=256\n",
    "        )\n",
    "        node_df.to_csv(os.path.join(save_dir, \"global_node_feature_importance.csv\"), index=False)\n",
    "        edge_df.to_csv(os.path.join(save_dir, \"global_edge_feature_importance.csv\"), index=False)\n",
    "\n",
    "        print(f\"[Explain] Saved global feature importance CSVs to: {save_dir}\")\n",
    "\n",
    "        # Per-molecule IG + visualization\n",
    "        for tag, idx in picks:\n",
    "            g = data_balanced[int(idx)]\n",
    "            gg = Data(x=g.x, edge_index=g.edge_index, edge_attr=g.edge_attr)\n",
    "            gg.batch = torch.zeros(gg.x.size(0), dtype=torch.long)\n",
    "            gg = gg.to(device)\n",
    "\n",
    "            attr_x, attr_e = integrated_gradients_graph(expl_model, gg, steps=explain_steps, device=device)\n",
    "            node_scores, edge_scores = rank_nodes_and_edges_from_ig(attr_x, attr_e)\n",
    "\n",
    "            # Save raw per-feature attributions (mean over nodes/edges) for this molecule\n",
    "            node_names = build_node_feature_names()\n",
    "            per_node_feat = attr_x.abs().mean(dim=0).detach().cpu().numpy()\n",
    "            pd.DataFrame({\"feature\": node_names[:len(per_node_feat)],\n",
    "                          \"abs_IG\": per_node_feat}).sort_values(\"abs_IG\", ascending=False)\\\n",
    "                .to_csv(os.path.join(save_dir, f\"{tag}_mol{idx}_node_feat_IG.csv\"), index=False)\n",
    "\n",
    "            if attr_e is not None and attr_e.numel() > 0:\n",
    "                edge_names = build_edge_feature_names()\n",
    "                per_edge_feat = attr_e.abs().mean(dim=0).detach().cpu().numpy()\n",
    "                pd.DataFrame({\"feature\": edge_names[:len(per_edge_feat)],\n",
    "                              \"abs_IG\": per_edge_feat}).sort_values(\"abs_IG\", ascending=False)\\\n",
    "                    .to_csv(os.path.join(save_dir, f\"{tag}_mol{idx}_edge_feat_IG.csv\"), index=False)\n",
    "\n",
    "            # Draw highlight PNG\n",
    "            out_png = os.path.join(save_dir, f\"{tag}_mol{idx}.png\")\n",
    "            draw_highlight_smiles(\n",
    "                g.smiles, node_scores, g.edge_index.cpu(),\n",
    "                edge_scores=edge_scores, topk_nodes=10, topk_edges=10, out_png=out_png\n",
    "            )\n",
    "            print(f\"[Explain] Saved {out_png}\")\n",
    "\n",
    "# -----------------------------\n",
    "# CLI\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--csv\", type=str, default=DEFAULT_CSV,\n",
    "                        help=\"Path to CSV with columns 'smiles' and 'antibiotic_activity'.\")\n",
    "    parser.add_argument(\"--pick\", type=str, choices=[\"nearest\", \"farthest\"], default=DEFAULT_PICK,\n",
    "                        help=\"Negative selection mode relative to positive centroid (default: nearest).\")\n",
    "    parser.add_argument(\"--metric\", type=str, choices=[\"cosine\", \"euclidean\"], default=DIST_METRIC,\n",
    "                        help=\"Similarity metric in embedding space (default: cosine).\")\n",
    "    parser.add_argument(\"--ratio\", type=float, default=NEG_POS_RATIO,\n",
    "                        help=\"Negative:positive ratio for undersampling (default: 1.0).\")\n",
    "\n",
    "    # Explainability options\n",
    "    parser.add_argument(\"--explain\", type=int, default=0,\n",
    "                        help=\"If >0, generate IG explanations for top-N positives and negatives.\")\n",
    "    parser.add_argument(\"--explain-steps\", type=int, default=64,\n",
    "                        help=\"Number of IG Riemann steps (default: 64).\")\n",
    "    parser.add_argument(\"--explain-save-dir\", type=str, default=\"./explain_out\",\n",
    "                        help=\"Directory to save explanation outputs (PNGs and CSVs).\")\n",
    "\n",
    "    # Parse-known to be notebook-friendly\n",
    "    args, _ = parser.parse_known_args()\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
