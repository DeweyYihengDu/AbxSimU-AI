{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SMILES->Graph: 100%|██████████| 13524/13524 [00:11<00:00, 1203.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feat dim = 47 | Edge feat dim = 10 | N graphs = 13524\n",
      "[AE] Epoch 001 | recon MSE 0.02076 | lr 1.00e-03\n",
      "[AE] Epoch 002 | recon MSE 0.00463 | lr 1.00e-03\n",
      "[AE] Epoch 003 | recon MSE 0.00259 | lr 1.00e-03\n",
      "[AE] Epoch 004 | recon MSE 0.00179 | lr 1.00e-03\n",
      "[AE] Epoch 005 | recon MSE 0.00137 | lr 1.00e-03\n",
      "[AE] Epoch 006 | recon MSE 0.00114 | lr 1.00e-03\n",
      "[AE] Epoch 007 | recon MSE 0.00101 | lr 1.00e-03\n",
      "[AE] Epoch 008 | recon MSE 0.00093 | lr 1.00e-03\n",
      "[AE] Epoch 009 | recon MSE 0.00088 | lr 1.00e-03\n",
      "[AE] Epoch 010 | recon MSE 0.00094 | lr 1.00e-03\n",
      "[AE] Epoch 011 | recon MSE 0.00084 | lr 1.00e-03\n",
      "[AE] Epoch 012 | recon MSE 0.00081 | lr 1.00e-03\n",
      "[AE] Epoch 013 | recon MSE 0.00085 | lr 1.00e-03\n",
      "[AE] Epoch 014 | recon MSE 0.00078 | lr 1.00e-03\n",
      "[AE] Epoch 015 | recon MSE 0.00076 | lr 1.00e-03\n",
      "[AE] Epoch 016 | recon MSE 0.00076 | lr 1.00e-03\n",
      "[AE] Epoch 017 | recon MSE 0.00076 | lr 1.00e-03\n",
      "[AE] Epoch 018 | recon MSE 0.00075 | lr 1.00e-03\n",
      "[AE] Epoch 019 | recon MSE 0.00074 | lr 1.00e-03\n",
      "[AE] Epoch 020 | recon MSE 0.00074 | lr 1.00e-03\n",
      "[AE] Epoch 021 | recon MSE 0.00086 | lr 1.00e-03\n",
      "[AE] Epoch 022 | recon MSE 0.00074 | lr 1.00e-03\n",
      "[AE] Epoch 023 | recon MSE 0.00072 | lr 1.00e-03\n",
      "[AE] Epoch 024 | recon MSE 0.00072 | lr 1.00e-03\n",
      "[AE] Epoch 025 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 026 | recon MSE 0.00072 | lr 1.00e-03\n",
      "[AE] Epoch 027 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 028 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 029 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 030 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 031 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 032 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 033 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 034 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 035 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 036 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 037 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 038 | recon MSE 0.00073 | lr 1.00e-03\n",
      "[AE] Epoch 039 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 040 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 041 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 042 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 043 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 044 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 045 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 046 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 047 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 048 | recon MSE 0.00075 | lr 1.00e-03\n",
      "[AE] Epoch 049 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 050 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 051 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 052 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 053 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 054 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 055 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 056 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 057 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 058 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 059 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 060 | recon MSE 0.00067 | lr 1.00e-03\n",
      "After undersampling by 'nearest' (cosine) similarity: pos=470 neg=470 total=940\n",
      "[Fold 1] Ep 001 | train loss 0.6449 | val AP 0.9519 | lr 2.00e-03\n",
      "[Fold 1] Ep 002 | train loss 0.4425 | val AP 0.8957 | lr 2.00e-03\n",
      "[Fold 1] Ep 003 | train loss 0.3317 | val AP 0.9455 | lr 2.00e-03\n",
      "[Fold 1] Ep 004 | train loss 0.3259 | val AP 0.9562 | lr 2.00e-03\n",
      "[Fold 1] Ep 005 | train loss 0.2732 | val AP 0.9662 | lr 2.00e-03\n",
      "[Fold 1] Ep 006 | train loss 0.2855 | val AP 0.9499 | lr 2.00e-03\n",
      "[Fold 1] Ep 007 | train loss 0.2741 | val AP 0.8777 | lr 2.00e-03\n",
      "[Fold 1] Ep 008 | train loss 0.2363 | val AP 0.9364 | lr 2.00e-03\n",
      "[Fold 1] Ep 009 | train loss 0.2535 | val AP 0.9432 | lr 2.00e-03\n",
      "[Fold 1] Ep 010 | train loss 0.2505 | val AP 0.9241 | lr 2.00e-03\n",
      "[Fold 1] Ep 011 | train loss 0.2229 | val AP 0.9244 | lr 1.00e-03\n",
      "[Fold 1] Ep 012 | train loss 0.2201 | val AP 0.9361 | lr 1.00e-03\n",
      "[Fold 1] Ep 013 | train loss 0.1992 | val AP 0.9430 | lr 1.00e-03\n",
      "[Fold 1] Ep 014 | train loss 0.1986 | val AP 0.9497 | lr 1.00e-03\n",
      "[Fold 1] Ep 015 | train loss 0.1619 | val AP 0.9541 | lr 1.00e-03\n",
      "[Fold 1] Ep 016 | train loss 0.1580 | val AP 0.9542 | lr 1.00e-03\n",
      "[Fold 1] Ep 017 | train loss 0.1769 | val AP 0.9618 | lr 5.00e-04\n",
      "[Fold 1] Ep 018 | train loss 0.1558 | val AP 0.9534 | lr 5.00e-04\n",
      "[Fold 1] Ep 019 | train loss 0.1381 | val AP 0.9565 | lr 5.00e-04\n",
      "[Fold 1] Ep 020 | train loss 0.1336 | val AP 0.9567 | lr 5.00e-04\n",
      "[Fold 1] Ep 021 | train loss 0.1621 | val AP 0.9548 | lr 5.00e-04\n",
      "[Fold 1] Ep 022 | train loss 0.1483 | val AP 0.9522 | lr 5.00e-04\n",
      "[Fold 1] Ep 023 | train loss 0.1373 | val AP 0.9527 | lr 2.50e-04\n",
      "[Fold 1] Ep 024 | train loss 0.1340 | val AP 0.9570 | lr 2.50e-04\n",
      "[Fold 1] Ep 025 | train loss 0.1249 | val AP 0.9455 | lr 2.50e-04\n",
      "[Fold 1] Ep 026 | train loss 0.1189 | val AP 0.9559 | lr 2.50e-04\n",
      "[Fold 1] Ep 027 | train loss 0.1053 | val AP 0.9530 | lr 2.50e-04\n",
      "[Fold 1] Ep 028 | train loss 0.1253 | val AP 0.9569 | lr 2.50e-04\n",
      "[Fold 1] Ep 029 | train loss 0.1133 | val AP 0.9514 | lr 1.25e-04\n",
      "[Fold 1] Ep 030 | train loss 0.0999 | val AP 0.9554 | lr 1.25e-04\n",
      "[Fold 1] Early stop.\n",
      "[Fold 1] Test metrics: {'ROC-AUC': 0.9580126754187415, 'PRC-AUC': 0.9497681064353753, 'Accuracy': 0.9361702127659575, 'F1': 0.9361702127659575, 'MCC': 0.8723404255319149, 'Recall': 0.9361702127659575, 'Precision': 0.9361702127659575, 'False Positives': 6, 'False Positive Rate': np.float64(0.06382978723404255)}\n",
      "[Fold 2] Ep 001 | train loss 0.6479 | val AP 0.6986 | lr 2.00e-03\n",
      "[Fold 2] Ep 002 | train loss 0.4665 | val AP 0.8958 | lr 2.00e-03\n",
      "[Fold 2] Ep 003 | train loss 0.3352 | val AP 0.8958 | lr 2.00e-03\n",
      "[Fold 2] Ep 004 | train loss 0.2670 | val AP 0.9216 | lr 2.00e-03\n",
      "[Fold 2] Ep 005 | train loss 0.2964 | val AP 0.7691 | lr 2.00e-03\n",
      "[Fold 2] Ep 006 | train loss 0.3085 | val AP 0.9668 | lr 2.00e-03\n",
      "[Fold 2] Ep 007 | train loss 0.2368 | val AP 0.9019 | lr 2.00e-03\n",
      "[Fold 2] Ep 008 | train loss 0.2654 | val AP 0.9266 | lr 2.00e-03\n",
      "[Fold 2] Ep 009 | train loss 0.2212 | val AP 0.9488 | lr 2.00e-03\n",
      "[Fold 2] Ep 010 | train loss 0.2184 | val AP 0.8884 | lr 2.00e-03\n",
      "[Fold 2] Ep 011 | train loss 0.2155 | val AP 0.9496 | lr 2.00e-03\n",
      "[Fold 2] Ep 012 | train loss 0.2299 | val AP 0.9620 | lr 1.00e-03\n",
      "[Fold 2] Ep 013 | train loss 0.2006 | val AP 0.9610 | lr 1.00e-03\n",
      "[Fold 2] Ep 014 | train loss 0.2152 | val AP 0.9507 | lr 1.00e-03\n",
      "[Fold 2] Ep 015 | train loss 0.1580 | val AP 0.9728 | lr 1.00e-03\n",
      "[Fold 2] Ep 016 | train loss 0.1574 | val AP 0.9628 | lr 1.00e-03\n",
      "[Fold 2] Ep 017 | train loss 0.1728 | val AP 0.9623 | lr 1.00e-03\n",
      "[Fold 2] Ep 018 | train loss 0.1484 | val AP 0.9545 | lr 1.00e-03\n",
      "[Fold 2] Ep 019 | train loss 0.1634 | val AP 0.9594 | lr 1.00e-03\n",
      "[Fold 2] Ep 020 | train loss 0.1315 | val AP 0.9454 | lr 1.00e-03\n",
      "[Fold 2] Ep 021 | train loss 0.1371 | val AP 0.9691 | lr 5.00e-04\n",
      "[Fold 2] Ep 022 | train loss 0.1203 | val AP 0.9686 | lr 5.00e-04\n",
      "[Fold 2] Ep 023 | train loss 0.1161 | val AP 0.9587 | lr 5.00e-04\n",
      "[Fold 2] Ep 024 | train loss 0.1009 | val AP 0.9706 | lr 5.00e-04\n",
      "[Fold 2] Ep 025 | train loss 0.1031 | val AP 0.9647 | lr 5.00e-04\n",
      "[Fold 2] Ep 026 | train loss 0.1054 | val AP 0.9707 | lr 5.00e-04\n",
      "[Fold 2] Ep 027 | train loss 0.0870 | val AP 0.9718 | lr 2.50e-04\n",
      "[Fold 2] Ep 028 | train loss 0.0992 | val AP 0.9735 | lr 2.50e-04\n",
      "[Fold 2] Ep 029 | train loss 0.0931 | val AP 0.9731 | lr 2.50e-04\n",
      "[Fold 2] Ep 030 | train loss 0.0876 | val AP 0.9731 | lr 2.50e-04\n",
      "[Fold 2] Ep 031 | train loss 0.0803 | val AP 0.9739 | lr 2.50e-04\n",
      "[Fold 2] Ep 032 | train loss 0.0984 | val AP 0.9735 | lr 2.50e-04\n",
      "[Fold 2] Ep 033 | train loss 0.0874 | val AP 0.9708 | lr 2.50e-04\n",
      "[Fold 2] Ep 034 | train loss 0.0771 | val AP 0.9740 | lr 2.50e-04\n",
      "[Fold 2] Ep 035 | train loss 0.0799 | val AP 0.9754 | lr 2.50e-04\n",
      "[Fold 2] Ep 036 | train loss 0.0735 | val AP 0.9720 | lr 2.50e-04\n",
      "[Fold 2] Ep 037 | train loss 0.0951 | val AP 0.9715 | lr 2.50e-04\n",
      "[Fold 2] Ep 038 | train loss 0.0813 | val AP 0.9715 | lr 2.50e-04\n",
      "[Fold 2] Ep 039 | train loss 0.0810 | val AP 0.9706 | lr 2.50e-04\n",
      "[Fold 2] Ep 040 | train loss 0.0672 | val AP 0.9766 | lr 2.50e-04\n",
      "[Fold 2] Ep 041 | train loss 0.0739 | val AP 0.9719 | lr 2.50e-04\n",
      "[Fold 2] Ep 042 | train loss 0.0736 | val AP 0.9686 | lr 2.50e-04\n",
      "[Fold 2] Ep 043 | train loss 0.0969 | val AP 0.9736 | lr 2.50e-04\n",
      "[Fold 2] Ep 044 | train loss 0.0915 | val AP 0.9699 | lr 2.50e-04\n",
      "[Fold 2] Ep 045 | train loss 0.0668 | val AP 0.9707 | lr 2.50e-04\n",
      "[Fold 2] Ep 046 | train loss 0.0859 | val AP 0.9743 | lr 1.25e-04\n",
      "[Fold 2] Ep 047 | train loss 0.0716 | val AP 0.9723 | lr 1.25e-04\n",
      "[Fold 2] Ep 048 | train loss 0.0686 | val AP 0.9693 | lr 1.25e-04\n",
      "[Fold 2] Ep 049 | train loss 0.0688 | val AP 0.9707 | lr 1.25e-04\n",
      "[Fold 2] Ep 050 | train loss 0.0643 | val AP 0.9686 | lr 1.25e-04\n",
      "[Fold 2] Ep 051 | train loss 0.0749 | val AP 0.9709 | lr 1.25e-04\n",
      "[Fold 2] Ep 052 | train loss 0.0799 | val AP 0.9777 | lr 1.25e-04\n",
      "[Fold 2] Ep 053 | train loss 0.0682 | val AP 0.9776 | lr 1.25e-04\n",
      "[Fold 2] Ep 054 | train loss 0.0723 | val AP 0.9720 | lr 1.25e-04\n",
      "[Fold 2] Ep 055 | train loss 0.0771 | val AP 0.9699 | lr 1.25e-04\n",
      "[Fold 2] Ep 056 | train loss 0.0539 | val AP 0.9722 | lr 1.25e-04\n",
      "[Fold 2] Ep 057 | train loss 0.0827 | val AP 0.9748 | lr 1.25e-04\n",
      "[Fold 2] Ep 058 | train loss 0.0634 | val AP 0.9755 | lr 6.25e-05\n",
      "[Fold 2] Ep 059 | train loss 0.0796 | val AP 0.9743 | lr 6.25e-05\n",
      "[Fold 2] Ep 060 | train loss 0.0713 | val AP 0.9732 | lr 6.25e-05\n",
      "[Fold 2] Ep 061 | train loss 0.0738 | val AP 0.9731 | lr 6.25e-05\n",
      "[Fold 2] Ep 062 | train loss 0.0727 | val AP 0.9740 | lr 6.25e-05\n",
      "[Fold 2] Ep 063 | train loss 0.0596 | val AP 0.9749 | lr 6.25e-05\n",
      "[Fold 2] Ep 064 | train loss 0.0925 | val AP 0.9749 | lr 3.13e-05\n",
      "[Fold 2] Ep 065 | train loss 0.0839 | val AP 0.9739 | lr 3.13e-05\n",
      "[Fold 2] Ep 066 | train loss 0.0654 | val AP 0.9747 | lr 3.13e-05\n",
      "[Fold 2] Ep 067 | train loss 0.0687 | val AP 0.9747 | lr 3.13e-05\n",
      "[Fold 2] Early stop.\n",
      "[Fold 2] Test metrics: {'ROC-AUC': 0.9425079221367134, 'PRC-AUC': 0.954169996040974, 'Accuracy': 0.851063829787234, 'F1': 0.8372093023255814, 'MCC': 0.7125253031944253, 'Recall': 0.7659574468085106, 'Precision': 0.9230769230769231, 'False Positives': 6, 'False Positive Rate': np.float64(0.06382978723404255)}\n",
      "[Fold 3] Ep 001 | train loss 0.6285 | val AP 0.8954 | lr 2.00e-03\n",
      "[Fold 3] Ep 002 | train loss 0.4083 | val AP 0.9029 | lr 2.00e-03\n",
      "[Fold 3] Ep 003 | train loss 0.3126 | val AP 0.9378 | lr 2.00e-03\n",
      "[Fold 3] Ep 004 | train loss 0.3185 | val AP 0.8520 | lr 2.00e-03\n",
      "[Fold 3] Ep 005 | train loss 0.2617 | val AP 0.8503 | lr 2.00e-03\n",
      "[Fold 3] Ep 006 | train loss 0.3288 | val AP 0.8846 | lr 2.00e-03\n",
      "[Fold 3] Ep 007 | train loss 0.2449 | val AP 0.9154 | lr 2.00e-03\n",
      "[Fold 3] Ep 008 | train loss 0.2380 | val AP 0.9389 | lr 2.00e-03\n",
      "[Fold 3] Ep 009 | train loss 0.2591 | val AP 0.9077 | lr 2.00e-03\n",
      "[Fold 3] Ep 010 | train loss 0.2110 | val AP 0.9339 | lr 2.00e-03\n",
      "[Fold 3] Ep 011 | train loss 0.2153 | val AP 0.9544 | lr 2.00e-03\n",
      "[Fold 3] Ep 012 | train loss 0.2114 | val AP 0.9403 | lr 2.00e-03\n",
      "[Fold 3] Ep 013 | train loss 0.1849 | val AP 0.9577 | lr 2.00e-03\n",
      "[Fold 3] Ep 014 | train loss 0.1886 | val AP 0.9498 | lr 2.00e-03\n",
      "[Fold 3] Ep 015 | train loss 0.2290 | val AP 0.9305 | lr 2.00e-03\n",
      "[Fold 3] Ep 016 | train loss 0.1772 | val AP 0.9399 | lr 2.00e-03\n",
      "[Fold 3] Ep 017 | train loss 0.1899 | val AP 0.9475 | lr 2.00e-03\n",
      "[Fold 3] Ep 018 | train loss 0.1652 | val AP 0.9409 | lr 2.00e-03\n",
      "[Fold 3] Ep 019 | train loss 0.1560 | val AP 0.9158 | lr 1.00e-03\n",
      "[Fold 3] Ep 020 | train loss 0.1483 | val AP 0.9631 | lr 1.00e-03\n",
      "[Fold 3] Ep 021 | train loss 0.1763 | val AP 0.9424 | lr 1.00e-03\n",
      "[Fold 3] Ep 022 | train loss 0.1277 | val AP 0.9547 | lr 1.00e-03\n",
      "[Fold 3] Ep 023 | train loss 0.1404 | val AP 0.9562 | lr 1.00e-03\n",
      "[Fold 3] Ep 024 | train loss 0.1186 | val AP 0.9336 | lr 1.00e-03\n",
      "[Fold 3] Ep 025 | train loss 0.1177 | val AP 0.9502 | lr 1.00e-03\n",
      "[Fold 3] Ep 026 | train loss 0.1036 | val AP 0.9379 | lr 5.00e-04\n",
      "[Fold 3] Ep 027 | train loss 0.1100 | val AP 0.9503 | lr 5.00e-04\n",
      "[Fold 3] Ep 028 | train loss 0.1002 | val AP 0.9389 | lr 5.00e-04\n",
      "[Fold 3] Ep 029 | train loss 0.0851 | val AP 0.9344 | lr 5.00e-04\n",
      "[Fold 3] Ep 030 | train loss 0.1061 | val AP 0.9339 | lr 5.00e-04\n",
      "[Fold 3] Ep 031 | train loss 0.0906 | val AP 0.9401 | lr 5.00e-04\n",
      "[Fold 3] Ep 032 | train loss 0.0916 | val AP 0.9352 | lr 2.50e-04\n",
      "[Fold 3] Ep 033 | train loss 0.0765 | val AP 0.9404 | lr 2.50e-04\n",
      "[Fold 3] Ep 034 | train loss 0.0852 | val AP 0.9426 | lr 2.50e-04\n",
      "[Fold 3] Ep 035 | train loss 0.0980 | val AP 0.9401 | lr 2.50e-04\n",
      "[Fold 3] Early stop.\n",
      "[Fold 3] Test metrics: {'ROC-AUC': 0.9689904934359439, 'PRC-AUC': 0.9651921301716772, 'Accuracy': 0.9042553191489362, 'F1': 0.898876404494382, 'MCC': 0.8131249357707345, 'Recall': 0.851063829787234, 'Precision': 0.9523809523809523, 'False Positives': 4, 'False Positive Rate': np.float64(0.0425531914893617)}\n",
      "[Fold 4] Ep 001 | train loss 0.6299 | val AP 0.5181 | lr 2.00e-03\n",
      "[Fold 4] Ep 002 | train loss 0.4660 | val AP 0.9422 | lr 2.00e-03\n",
      "[Fold 4] Ep 003 | train loss 0.3805 | val AP 0.9369 | lr 2.00e-03\n",
      "[Fold 4] Ep 004 | train loss 0.3105 | val AP 0.8578 | lr 2.00e-03\n",
      "[Fold 4] Ep 005 | train loss 0.2739 | val AP 0.8848 | lr 2.00e-03\n",
      "[Fold 4] Ep 006 | train loss 0.2596 | val AP 0.9587 | lr 2.00e-03\n",
      "[Fold 4] Ep 007 | train loss 0.2675 | val AP 0.8680 | lr 2.00e-03\n",
      "[Fold 4] Ep 008 | train loss 0.2637 | val AP 0.9409 | lr 2.00e-03\n",
      "[Fold 4] Ep 009 | train loss 0.2488 | val AP 0.9552 | lr 2.00e-03\n",
      "[Fold 4] Ep 010 | train loss 0.2236 | val AP 0.9692 | lr 2.00e-03\n",
      "[Fold 4] Ep 011 | train loss 0.2158 | val AP 0.9710 | lr 2.00e-03\n",
      "[Fold 4] Ep 012 | train loss 0.2469 | val AP 0.9837 | lr 2.00e-03\n",
      "[Fold 4] Ep 013 | train loss 0.2113 | val AP 0.9335 | lr 2.00e-03\n",
      "[Fold 4] Ep 014 | train loss 0.2394 | val AP 0.9722 | lr 2.00e-03\n",
      "[Fold 4] Ep 015 | train loss 0.2164 | val AP 0.9496 | lr 2.00e-03\n",
      "[Fold 4] Ep 016 | train loss 0.1965 | val AP 0.9799 | lr 2.00e-03\n",
      "[Fold 4] Ep 017 | train loss 0.2052 | val AP 0.9064 | lr 2.00e-03\n",
      "[Fold 4] Ep 018 | train loss 0.2212 | val AP 0.9585 | lr 1.00e-03\n",
      "[Fold 4] Ep 019 | train loss 0.1870 | val AP 0.9884 | lr 1.00e-03\n",
      "[Fold 4] Ep 020 | train loss 0.1711 | val AP 0.9700 | lr 1.00e-03\n",
      "[Fold 4] Ep 021 | train loss 0.1850 | val AP 0.9933 | lr 1.00e-03\n",
      "[Fold 4] Ep 022 | train loss 0.1610 | val AP 0.9503 | lr 1.00e-03\n",
      "[Fold 4] Ep 023 | train loss 0.1529 | val AP 0.9849 | lr 1.00e-03\n",
      "[Fold 4] Ep 024 | train loss 0.1519 | val AP 0.9541 | lr 1.00e-03\n",
      "[Fold 4] Ep 025 | train loss 0.1495 | val AP 0.9944 | lr 1.00e-03\n",
      "[Fold 4] Ep 026 | train loss 0.1443 | val AP 0.9746 | lr 1.00e-03\n",
      "[Fold 4] Ep 027 | train loss 0.1511 | val AP 0.9702 | lr 1.00e-03\n",
      "[Fold 4] Ep 028 | train loss 0.1620 | val AP 0.9874 | lr 1.00e-03\n",
      "[Fold 4] Ep 029 | train loss 0.1457 | val AP 0.9859 | lr 1.00e-03\n",
      "[Fold 4] Ep 030 | train loss 0.1233 | val AP 0.9927 | lr 1.00e-03\n",
      "[Fold 4] Ep 031 | train loss 0.1362 | val AP 0.9877 | lr 5.00e-04\n",
      "[Fold 4] Ep 032 | train loss 0.1240 | val AP 0.9818 | lr 5.00e-04\n",
      "[Fold 4] Ep 033 | train loss 0.1074 | val AP 0.9803 | lr 5.00e-04\n",
      "[Fold 4] Ep 034 | train loss 0.1238 | val AP 0.9863 | lr 5.00e-04\n",
      "[Fold 4] Ep 035 | train loss 0.1027 | val AP 0.9877 | lr 5.00e-04\n",
      "[Fold 4] Ep 036 | train loss 0.1002 | val AP 0.9913 | lr 5.00e-04\n",
      "[Fold 4] Ep 037 | train loss 0.1048 | val AP 0.9903 | lr 2.50e-04\n",
      "[Fold 4] Ep 038 | train loss 0.0950 | val AP 0.9869 | lr 2.50e-04\n",
      "[Fold 4] Ep 039 | train loss 0.0827 | val AP 0.9861 | lr 2.50e-04\n",
      "[Fold 4] Ep 040 | train loss 0.0853 | val AP 0.9866 | lr 2.50e-04\n",
      "[Fold 4] Early stop.\n",
      "[Fold 4] Test metrics: {'ROC-AUC': 0.9701222272521502, 'PRC-AUC': 0.9781530793338535, 'Accuracy': 0.9521276595744681, 'F1': 0.9518716577540107, 'MCC': 0.9043064923087151, 'Recall': 0.9468085106382979, 'Precision': 0.956989247311828, 'False Positives': 4, 'False Positive Rate': np.float64(0.0425531914893617)}\n",
      "[Fold 5] Ep 001 | train loss 0.6603 | val AP 0.8641 | lr 2.00e-03\n",
      "[Fold 5] Ep 002 | train loss 0.4813 | val AP 0.9298 | lr 2.00e-03\n",
      "[Fold 5] Ep 003 | train loss 0.3483 | val AP 0.9404 | lr 2.00e-03\n",
      "[Fold 5] Ep 004 | train loss 0.3215 | val AP 0.8916 | lr 2.00e-03\n",
      "[Fold 5] Ep 005 | train loss 0.3043 | val AP 0.9043 | lr 2.00e-03\n",
      "[Fold 5] Ep 006 | train loss 0.2852 | val AP 0.9092 | lr 2.00e-03\n",
      "[Fold 5] Ep 007 | train loss 0.2710 | val AP 0.9453 | lr 2.00e-03\n",
      "[Fold 5] Ep 008 | train loss 0.2415 | val AP 0.9282 | lr 2.00e-03\n",
      "[Fold 5] Ep 009 | train loss 0.2744 | val AP 0.9708 | lr 2.00e-03\n",
      "[Fold 5] Ep 010 | train loss 0.2645 | val AP 0.7711 | lr 2.00e-03\n",
      "[Fold 5] Ep 011 | train loss 0.2150 | val AP 0.9639 | lr 2.00e-03\n",
      "[Fold 5] Ep 012 | train loss 0.2153 | val AP 0.9495 | lr 2.00e-03\n",
      "[Fold 5] Ep 013 | train loss 0.2098 | val AP 0.9575 | lr 2.00e-03\n",
      "[Fold 5] Ep 014 | train loss 0.2139 | val AP 0.9791 | lr 2.00e-03\n",
      "[Fold 5] Ep 015 | train loss 0.2011 | val AP 0.9495 | lr 2.00e-03\n",
      "[Fold 5] Ep 016 | train loss 0.1835 | val AP 0.9690 | lr 2.00e-03\n",
      "[Fold 5] Ep 017 | train loss 0.1751 | val AP 0.9476 | lr 2.00e-03\n",
      "[Fold 5] Ep 018 | train loss 0.1928 | val AP 0.9608 | lr 2.00e-03\n",
      "[Fold 5] Ep 019 | train loss 0.1756 | val AP 0.9670 | lr 2.00e-03\n",
      "[Fold 5] Ep 020 | train loss 0.1628 | val AP 0.9282 | lr 1.00e-03\n",
      "[Fold 5] Ep 021 | train loss 0.1531 | val AP 0.9388 | lr 1.00e-03\n",
      "[Fold 5] Ep 022 | train loss 0.1454 | val AP 0.9413 | lr 1.00e-03\n",
      "[Fold 5] Ep 023 | train loss 0.1364 | val AP 0.9387 | lr 1.00e-03\n",
      "[Fold 5] Ep 024 | train loss 0.1362 | val AP 0.9612 | lr 1.00e-03\n",
      "[Fold 5] Ep 025 | train loss 0.1378 | val AP 0.9533 | lr 1.00e-03\n",
      "[Fold 5] Ep 026 | train loss 0.1451 | val AP 0.9651 | lr 5.00e-04\n",
      "[Fold 5] Ep 027 | train loss 0.1177 | val AP 0.9539 | lr 5.00e-04\n",
      "[Fold 5] Ep 028 | train loss 0.1191 | val AP 0.9477 | lr 5.00e-04\n",
      "[Fold 5] Ep 029 | train loss 0.1196 | val AP 0.9636 | lr 5.00e-04\n",
      "[Fold 5] Ep 030 | train loss 0.1210 | val AP 0.9623 | lr 5.00e-04\n",
      "[Fold 5] Early stop.\n",
      "[Fold 5] Test metrics: {'ROC-AUC': 0.9660479855138072, 'PRC-AUC': 0.9723779608130719, 'Accuracy': 0.9148936170212766, 'F1': 0.9139784946236559, 'MCC': 0.8299751174897799, 'Recall': 0.9042553191489362, 'Precision': 0.9239130434782609, 'False Positives': 7, 'False Positive Rate': np.float64(0.07446808510638298)}\n",
      "\n",
      "========== Overall (5-fold aggregated) ==========\n",
      "ROC-AUC: 0.955220\n",
      "PRC-AUC: 0.960153\n",
      "Accuracy: 0.911702\n",
      "F1: 0.908891\n",
      "MCC: 0.824976\n",
      "Recall: 0.880851\n",
      "Precision: 0.938776\n",
      "False Positives: 27\n",
      "False Positive Rate: 0.057447\n",
      "Best fold model saved to: best_gine_model.pth\n",
      "\n",
      "Per-fold metrics -> cv_per_fold.csv\n",
      "Overall metrics -> cv_results.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "GINE + Graph Autoencoder (GAE) undersampling pipeline for molecular activity prediction.\n",
    "\n",
    "Pipeline\n",
    "--------\n",
    "1) RDKit: convert SMILES -> molecular graphs with rich node/edge features\n",
    "2) Train a Graph Autoencoder (reconstruct node features) to obtain graph embeddings\n",
    "3) Undersample negatives by similarity to the positive centroid in embedding space:\n",
    "   - 'nearest'  : pick the most similar negatives (smallest difference)  [DEFAULT]\n",
    "   - 'farthest' : pick the most dissimilar negatives (largest difference)\n",
    "   Supports cosine (default) or euclidean distance; negative:positive ~ 1:1 (configurable)\n",
    "4) Train a GINE-based graph classifier with Stratified 5-Fold cross-validation\n",
    "5) Report and save metrics per fold and overall:\n",
    "   ROC-AUC, PRC-AUC, Accuracy, F1, MCC, Recall, Precision, False Positives, False Positive Rate\n",
    "6) Save best-fold model weights and optional embeddings CSV\n",
    "\n",
    "Requirements\n",
    "------------\n",
    "- rdkit\n",
    "- torch, torch_geometric (>= 2.2 recommended)\n",
    "- scikit-learn, pandas, numpy, tqdm\n",
    "\n",
    "Input\n",
    "-----\n",
    "A CSV with at least two columns:\n",
    "- 'smiles' : SMILES string\n",
    "- 'antibiotic_activity' : binary label {0,1}\n",
    "\n",
    "Usage\n",
    "-----\n",
    "Command line:\n",
    "    python train_gcn_gae_pipeline.py --csv /path/to/data.csv \\\n",
    "        --pick nearest --metric cosine --ratio 1.0\n",
    "\n",
    "Jupyter / IDE:\n",
    "    This script uses `parse_known_args()` to ignore extra kernel args.\n",
    "    Alternatively, call:\n",
    "        from types import SimpleNamespace\n",
    "        main(SimpleNamespace(csv='/path/to/data.csv', pick='nearest', metric='cosine', ratio=1.0))\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# PyG\n",
    "from torch_geometric.data import Data\n",
    "try:\n",
    "    from torch_geometric.loader import DataLoader\n",
    "except Exception:  # backward compatibility with very old PyG versions\n",
    "    from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool, GINEConv, BatchNorm\n",
    "\n",
    "# RDKit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    matthews_corrcoef,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration (edit as needed)\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "DEFAULT_CSV = \"./data/raw_data.csv\"\n",
    "RESULT_CSV = \"cv_results.csv\"\n",
    "FOLD_DETAIL_CSV = \"cv_per_fold.csv\"\n",
    "BEST_MODEL_PATH = \"best_gine_model.pth\"\n",
    "EMBED_CSV = \"graph_embeddings.csv\"\n",
    "\n",
    "MAX_WORKERS = min(os.cpu_count() or 0, 30)  # keep <= 30 CPUs if you parallelize elsewhere\n",
    "BATCH_SIZE_AE = 64\n",
    "BATCH_SIZE_CLS = 64\n",
    "EPOCHS_AE = 60\n",
    "EPOCHS_CLS = 120\n",
    "PATIENCE = 15\n",
    "LR_AE = 1e-3\n",
    "LR_CLS = 2e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "DROPOUT = 0.2\n",
    "HIDDEN = 128\n",
    "NUM_GINE = 3                  # number of encoder layers\n",
    "VAL_SPLIT = 0.10              # split from the training fold for early stopping\n",
    "DIST_METRIC = \"cosine\"        # 'cosine' or 'euclidean' (default used if not overriden by CLI)\n",
    "NEG_POS_RATIO = 1.0           # negative:positive = 1:1 by default\n",
    "DEFAULT_PICK = \"nearest\"      # 'nearest' (most similar negatives) or 'farthest' (most dissimilar)\n",
    "\n",
    "# Common atomic numbers; everything else goes to \"other\"\n",
    "COMMON_Z = [1, 5, 6, 7, 8, 9, 14, 15, 16, 17, 19, 11, 12, 20, 26, 29, 30, 35, 53]\n",
    "# H,B,C,N,O,F,Si,P,S,Cl,K,Na,Mg,Ca,Fe,Cu,Zn,Br,I\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def set_seed(seed: int = SEED) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def one_hot(val, choices):\n",
    "    vec = [0] * len(choices)\n",
    "    if val in choices:\n",
    "        vec[choices.index(val)] = 1\n",
    "    return vec\n",
    "\n",
    "\n",
    "def atom_features(atom: Chem.rdchem.Atom) -> List[float]:\n",
    "    \"\"\"\n",
    "    Node features:\n",
    "      - atomic number: one-hot (COMMON_Z + 'other')\n",
    "      - degree: one-hot [0..5]\n",
    "      - hybridization: one-hot {sp, sp2, sp3, sp3d, sp3d2, other}\n",
    "      - formal charge: one-hot [-2..2]\n",
    "      - total hydrogens: one-hot [0..4]\n",
    "      - aromatic (bool)\n",
    "      - in ring (bool)\n",
    "      - chirality tag: one-hot {unspecified, CW, CCW}\n",
    "    \"\"\"\n",
    "    z = atom.GetAtomicNum()\n",
    "    z_onehot = one_hot(z if z in COMMON_Z else -1, COMMON_Z + [-1])\n",
    "\n",
    "    degree = atom.GetTotalDegree()\n",
    "    degree_onehot = one_hot(min(degree, 5), list(range(6)))\n",
    "\n",
    "    hyb = atom.GetHybridization()\n",
    "    hyb_choices = [\n",
    "        Chem.rdchem.HybridizationType.SP,\n",
    "        Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3,\n",
    "        Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2,\n",
    "    ]\n",
    "    hyb_onehot = one_hot(hyb if hyb in hyb_choices else None, hyb_choices + [None])\n",
    "\n",
    "    charge = int(atom.GetFormalCharge())\n",
    "    charge = max(-2, min(2, charge))\n",
    "    charge_onehot = one_hot(charge, [-2, -1, 0, 1, 2])\n",
    "\n",
    "    num_h = min(atom.GetTotalNumHs(), 4)\n",
    "    num_h_onehot = one_hot(num_h, [0, 1, 2, 3, 4])\n",
    "\n",
    "    aromatic = [int(atom.GetIsAromatic())]\n",
    "    ring = [int(atom.IsInRing())]\n",
    "\n",
    "    chiral_tag = atom.GetChiralTag()\n",
    "    chiral_choices = [\n",
    "        Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "    ]\n",
    "    chiral_onehot = one_hot(\n",
    "        chiral_tag if chiral_tag in chiral_choices else Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "        chiral_choices,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        z_onehot\n",
    "        + degree_onehot\n",
    "        + hyb_onehot\n",
    "        + charge_onehot\n",
    "        + num_h_onehot\n",
    "        + aromatic\n",
    "        + ring\n",
    "        + chiral_onehot\n",
    "    )\n",
    "\n",
    "\n",
    "def bond_features(bond: Chem.rdchem.Bond) -> List[float]:\n",
    "    \"\"\"\n",
    "    Edge features:\n",
    "      - bond type: one-hot {single,double,triple,aromatic,other}\n",
    "      - conjugated (bool)\n",
    "      - in ring (bool)\n",
    "      - stereo: one-hot {none, Z, E}\n",
    "    \"\"\"\n",
    "    bt = bond.GetBondType()\n",
    "    bt_choices = [\n",
    "        Chem.BondType.SINGLE,\n",
    "        Chem.BondType.DOUBLE,\n",
    "        Chem.BondType.TRIPLE,\n",
    "        Chem.BondType.AROMATIC,\n",
    "    ]\n",
    "    bt_onehot = one_hot(bt if bt in bt_choices else None, bt_choices + [None])\n",
    "\n",
    "    conj = [int(bond.GetIsConjugated())]\n",
    "    ring = [int(bond.IsInRing())]\n",
    "\n",
    "    stereo = bond.GetStereo()\n",
    "    stereo_choices = [\n",
    "        Chem.rdchem.BondStereo.STEREONONE,\n",
    "        Chem.rdchem.BondStereo.STEREOZ,\n",
    "        Chem.rdchem.BondStereo.STEREOE,\n",
    "    ]\n",
    "    stereo_onehot = one_hot(\n",
    "        stereo if stereo in stereo_choices else Chem.rdchem.BondStereo.STEREONONE, stereo_choices\n",
    "    )\n",
    "\n",
    "    return bt_onehot + conj + ring + stereo_onehot\n",
    "\n",
    "\n",
    "def smiles_to_graph(smiles: str):\n",
    "    \"\"\"Build a PyG `Data` object from a SMILES string.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    # Keep aromatic flags; 2D coords are sufficient for this pipeline\n",
    "    Chem.Kekulize(mol, clearAromaticFlags=False)\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "\n",
    "    # Node features\n",
    "    x = [atom_features(a) for a in mol.GetAtoms()]\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "    # Edges + edge features (bidirectional)\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        f = bond_features(b)\n",
    "        edge_index.append([i, j]); edge_attr.append(f)\n",
    "        edge_index.append([j, i]); edge_attr.append(f)\n",
    "\n",
    "    if len(edge_index) == 0:\n",
    "        # Rare edge-less molecule\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        # Build an empty edge_attr with correct feature width by probing a dummy bond\n",
    "        dummy = Chem.MolFromSmiles(\"CC\").GetBonds()[0]\n",
    "        feat_w = len(bond_features(dummy))\n",
    "        edge_attr = torch.empty((0, feat_w), dtype=torch.float)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "# -----------------------------\n",
    "# Models\n",
    "# -----------------------------\n",
    "def mlp(in_dim: int, out_dim: int) -> nn.Sequential:\n",
    "    return nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Linear(out_dim, out_dim))\n",
    "\n",
    "\n",
    "class GINEEncoder(nn.Module):\n",
    "    \"\"\"GINE encoder with BatchNorm, dropout and a light residual connection.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, edge_dim: int, hidden: int = HIDDEN, num_layers: int = NUM_GINE, dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # First layer\n",
    "        self.convs.append(GINEConv(mlp(in_dim, hidden), edge_dim=edge_dim))\n",
    "        self.bns.append(BatchNorm(hidden))\n",
    "\n",
    "        # Subsequent layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GINEConv(mlp(hidden, hidden), edge_dim=edge_dim))\n",
    "            self.bns.append(BatchNorm(hidden))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        h = x\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            h_res = h\n",
    "            h = conv(h, edge_index, edge_attr)\n",
    "            h = bn(h)\n",
    "            h = F.relu(h)\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            # Tiny residual when shape matches\n",
    "            if h_res.shape == h.shape:\n",
    "                h = h + 0.1 * h_res\n",
    "        return h  # node embeddings\n",
    "\n",
    "\n",
    "class GraphAE(nn.Module):\n",
    "    \"\"\"Graph Autoencoder: node encoder -> reconstruct node features; returns graph embedding.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, edge_dim: int, hidden: int = HIDDEN, num_layers: int = NUM_GINE, dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.encoder = GINEEncoder(in_dim, edge_dim, hidden, num_layers, dropout)\n",
    "        self.decoder = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, in_dim))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        h = self.encoder(x, edge_index, edge_attr)  # [N, hidden]\n",
    "        x_hat = self.decoder(h)                     # [N, in_dim]\n",
    "        g = global_mean_pool(h, batch)              # [B, hidden] graph embedding\n",
    "        return x_hat, g\n",
    "\n",
    "    def encode_nodes(self, x, edge_index, edge_attr):\n",
    "        return self.encoder(x, edge_index, edge_attr)\n",
    "\n",
    "\n",
    "class GINEClassifier(nn.Module):\n",
    "    \"\"\"Graph-level classifier; encoder can be initialized from a trained AE.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, edge_dim: int, hidden: int = HIDDEN, num_layers: int = NUM_GINE, dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.encoder = GINEEncoder(in_dim, edge_dim, hidden, num_layers, dropout)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        h = self.encoder(x, edge_index, edge_attr)\n",
    "        g = global_mean_pool(h, batch)\n",
    "        logit = self.head(g).view(-1)\n",
    "        return logit\n",
    "\n",
    "    def load_from_ae(self, ae: GraphAE):\n",
    "        self.encoder.load_state_dict(ae.encoder.state_dict(), strict=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Training & evaluation helpers\n",
    "# -----------------------------\n",
    "def train_ae(ae: GraphAE, loader: DataLoader, device, epochs: int = EPOCHS_AE, lr: float = LR_AE,\n",
    "             wd: float = WEIGHT_DECAY, patience: int = PATIENCE) -> GraphAE:\n",
    "    \"\"\"Train GraphAE with node feature reconstruction loss (MSE).\"\"\"\n",
    "    ae = ae.to(device)\n",
    "    opt = torch.optim.Adam(ae.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    # ReduceLROnPlateau: some torch versions don't support 'verbose'\n",
    "    try:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=5, verbose=False)\n",
    "    except TypeError:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=5)\n",
    "\n",
    "    best_loss, bad = float(\"inf\"), 0\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        ae.train()\n",
    "        total = 0.0\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            x_hat, _ = ae(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            loss = F.mse_loss(x_hat, data.x)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(ae.parameters(), 2.0)\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "\n",
    "        mean_loss = total / max(len(loader), 1)\n",
    "        scheduler.step(mean_loss)\n",
    "        lr_now = opt.param_groups[0][\"lr\"]\n",
    "        print(f\"[AE] Epoch {ep:03d} | recon MSE {mean_loss:.5f} | lr {lr_now:.2e}\")\n",
    "\n",
    "        if mean_loss < best_loss - 1e-5:\n",
    "            best_loss, bad = mean_loss, 0\n",
    "            torch.save(ae.state_dict(), \"best_graph_ae.pth\")\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience and ep >= 20:\n",
    "                print(\"[AE] Early stop.\")\n",
    "                break\n",
    "\n",
    "    ae.load_state_dict(torch.load(\"best_graph_ae.pth\", map_location=device))\n",
    "    ae.eval()\n",
    "    return ae\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_graph_embeddings(ae: GraphAE, loader: DataLoader, device) -> np.ndarray:\n",
    "    \"\"\"Return graph embeddings [N_graphs, hidden] from a trained AE encoder.\"\"\"\n",
    "    ae.eval()\n",
    "    embs = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        h = ae.encode_nodes(data.x, data.edge_index, data.edge_attr)\n",
    "        g = global_mean_pool(h, data.batch)  # [B, hidden]\n",
    "        embs.append(g.cpu().numpy())\n",
    "    return np.concatenate(embs, axis=0)\n",
    "\n",
    "\n",
    "def select_negatives_by_similarity(\n",
    "    embs: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    metric: str = DIST_METRIC,     # 'cosine' or 'euclidean'\n",
    "    ratio: float = NEG_POS_RATIO,  # negatives : positives\n",
    "    pick: str = DEFAULT_PICK       # 'nearest' (most similar) or 'farthest' (most dissimilar)\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Select negatives relative to the positive centroid in the embedding space.\n",
    "\n",
    "    Behavior\n",
    "    --------\n",
    "    - cosine:    higher similarity -> more similar; lower -> more dissimilar\n",
    "    - euclidean: smaller distance  -> more similar; larger -> more dissimilar\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mask : (N,) bool array indicating retained samples (all positives + selected negatives)\n",
    "    \"\"\"\n",
    "    assert pick in (\"nearest\", \"farthest\"), \"pick must be 'nearest' or 'farthest'\"\n",
    "\n",
    "    pos_idx = np.where(labels == 1)[0]\n",
    "    neg_idx = np.where(labels == 0)[0]\n",
    "    if len(pos_idx) == 0 or len(neg_idx) == 0:\n",
    "        return np.ones_like(labels, dtype=bool)\n",
    "\n",
    "    pos_centroid = embs[pos_idx].mean(axis=0, keepdims=True)  # [1, d]\n",
    "    neg_embs = embs[neg_idx]                                  # [K, d]\n",
    "\n",
    "    if metric == \"cosine\":\n",
    "        a = neg_embs / (np.linalg.norm(neg_embs, axis=1, keepdims=True) + 1e-9)\n",
    "        b = pos_centroid / (np.linalg.norm(pos_centroid, axis=1, keepdims=True) + 1e-9)\n",
    "        sim = (a @ b.T).reshape(-1)  # larger = more similar\n",
    "        if pick == \"nearest\":\n",
    "            order = np.argsort(-sim)   # descending  -> most similar first\n",
    "        else:\n",
    "            order = np.argsort(sim)    # ascending   -> most dissimilar first\n",
    "    elif metric == \"euclidean\":\n",
    "        dist = np.linalg.norm(neg_embs - pos_centroid, axis=1)  # smaller = more similar\n",
    "        if pick == \"nearest\":\n",
    "            order = np.argsort(dist)   # ascending  -> most similar first\n",
    "        else:\n",
    "            order = np.argsort(-dist)  # descending -> most dissimilar first\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'cosine' or 'euclidean'\")\n",
    "\n",
    "    k = int(round(len(pos_idx) * ratio))\n",
    "    chosen_neg = neg_idx[order[:k]]\n",
    "\n",
    "    mask = np.zeros_like(labels, dtype=bool)\n",
    "    mask[pos_idx] = True\n",
    "    mask[chosen_neg] = True\n",
    "    return mask\n",
    "\n",
    "\n",
    "def train_one_epoch_cls(model: nn.Module, loader: DataLoader, optimizer, criterion, device) -> float:\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        y = data.y.view(-1).to(device)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "    return total / max(len(loader), 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(model: nn.Module, loader: DataLoader, device):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        logits = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        prob = torch.sigmoid(logits)\n",
    "        ys.append(data.y.view(-1).cpu().numpy())\n",
    "        ps.append(prob.cpu().numpy())\n",
    "    y_true = np.concatenate(ys) if ys else np.array([])\n",
    "    y_pred = np.concatenate(ps) if ps else np.array([])\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def calc_metrics(y_true: np.ndarray, y_prob: np.ndarray, threshold: float = 0.5):\n",
    "    \"\"\"Return all required metrics, including FP and FPR.\"\"\"\n",
    "    if y_true.size == 0:\n",
    "        keys = [\n",
    "            \"ROC-AUC\", \"PRC-AUC\", \"Accuracy\", \"F1\", \"MCC\",\n",
    "            \"Recall\", \"Precision\", \"False Positives\", \"False Positive Rate\",\n",
    "        ]\n",
    "        return {k: float(\"nan\") for k in keys}\n",
    "\n",
    "    y_hat = (y_prob >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat, labels=[0, 1]).ravel()\n",
    "    fpr = fp / max((fp + tn), 1)\n",
    "\n",
    "    return {\n",
    "        \"ROC-AUC\": roc_auc_score(y_true, y_prob),\n",
    "        \"PRC-AUC\": average_precision_score(y_true, y_prob),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_hat),\n",
    "        \"F1\": f1_score(y_true, y_hat, zero_division=0),\n",
    "        \"MCC\": matthews_corrcoef(y_true, y_hat),\n",
    "        \"Recall\": recall_score(y_true, y_hat, zero_division=0),\n",
    "        \"Precision\": precision_score(y_true, y_hat, zero_division=0),\n",
    "        \"False Positives\": int(fp),\n",
    "        \"False Positive Rate\": fpr,\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main(args) -> None:\n",
    "    set_seed(SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 1) Load data\n",
    "    df = pd.read_csv(args.csv)\n",
    "    assert \"smiles\" in df.columns and \"antibiotic_activity\" in df.columns, \\\n",
    "        \"CSV must contain columns 'smiles' and 'antibiotic_activity'.\"\n",
    "    smiles = df[\"smiles\"].astype(str).tolist()\n",
    "    labels = df[\"antibiotic_activity\"].astype(int).to_numpy()\n",
    "\n",
    "    # 2) Build graphs\n",
    "    data_list: List[Data] = []\n",
    "    drop_idx = []\n",
    "    for i, smi in enumerate(tqdm(smiles, desc=\"SMILES->Graph\")):\n",
    "        g = smiles_to_graph(smi)\n",
    "        if g is None or g.x.numel() == 0:\n",
    "            drop_idx.append(i)\n",
    "            continue\n",
    "        g.y = torch.tensor([labels[i]], dtype=torch.float32)\n",
    "        data_list.append(g)\n",
    "\n",
    "    if len(data_list) == 0:\n",
    "        raise RuntimeError(\"No valid molecules after SMILES->graph conversion.\")\n",
    "\n",
    "    if drop_idx:\n",
    "        print(f\"Warning: {len(drop_idx)} SMILES failed to convert and were skipped.\")\n",
    "        labels = np.delete(labels, drop_idx, axis=0)\n",
    "\n",
    "    in_dim = data_list[0].x.size(1)\n",
    "    edge_dim = data_list[0].edge_attr.size(1) if data_list[0].edge_attr is not None else 0\n",
    "    print(f\"Node feat dim = {in_dim} | Edge feat dim = {edge_dim} | N graphs = {len(data_list)}\")\n",
    "\n",
    "    # 3) Train Graph AE on ALL samples\n",
    "    ae_loader = DataLoader(data_list, batch_size=BATCH_SIZE_AE, shuffle=True, num_workers=0)\n",
    "    ae = GraphAE(in_dim, edge_dim, hidden=HIDDEN, num_layers=NUM_GINE, dropout=DROPOUT)\n",
    "    ae = train_ae(ae, ae_loader, device)\n",
    "\n",
    "    # 4) Get embeddings and perform similarity-based undersampling\n",
    "    eval_loader = DataLoader(data_list, batch_size=BATCH_SIZE_AE, shuffle=False, num_workers=0)\n",
    "    embs = get_graph_embeddings(ae, eval_loader, device)  # [N, hidden]\n",
    "    pd.DataFrame(embs).to_csv(EMBED_CSV, index=False)\n",
    "\n",
    "    pick_mode = getattr(args, \"pick\", DEFAULT_PICK)\n",
    "    dist_metric = getattr(args, \"metric\", DIST_METRIC)\n",
    "    ratio = float(getattr(args, \"ratio\", NEG_POS_RATIO))\n",
    "\n",
    "    mask = select_negatives_by_similarity(\n",
    "        embs, labels, metric=dist_metric, ratio=ratio, pick=pick_mode\n",
    "    )\n",
    "    data_balanced = [d for d, m in zip(data_list, mask) if m]\n",
    "    labels_balanced = labels[mask]\n",
    "    print(\n",
    "        f\"After undersampling by '{pick_mode}' ({dist_metric}) similarity: \"\n",
    "        f\"pos={labels_balanced.sum()} neg={(1 - labels_balanced).sum()} total={len(data_balanced)}\"\n",
    "    )\n",
    "\n",
    "    # 5) 5-fold CV training\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    fold_metrics = []\n",
    "    all_true, all_prob = [], []\n",
    "    best_score = -1.0\n",
    "\n",
    "    for fold, (tr_idx, te_idx) in enumerate(skf.split(np.arange(len(data_balanced)), labels_balanced), 1):\n",
    "        train_subset = [data_balanced[i] for i in tr_idx]\n",
    "        test_subset = [data_balanced[i] for i in te_idx]\n",
    "        y_train = labels_balanced[tr_idx]\n",
    "        y_test = labels_balanced[te_idx]\n",
    "\n",
    "        # validation split from training fold for early stopping\n",
    "        tr_part, val_part, _, _ = train_test_split(\n",
    "            train_subset, y_train, test_size=VAL_SPLIT, stratify=y_train, random_state=SEED\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(tr_part, batch_size=BATCH_SIZE_CLS, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_part, batch_size=BATCH_SIZE_CLS, shuffle=False, num_workers=0)\n",
    "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE_CLS, shuffle=False, num_workers=0)\n",
    "\n",
    "        model = GINEClassifier(in_dim, edge_dim, hidden=HIDDEN, num_layers=NUM_GINE, dropout=DROPOUT).to(device)\n",
    "        model.load_from_ae(ae)\n",
    "\n",
    "        # class imbalance guard (should be close to 1:1 after undersampling)\n",
    "        pos_count = float((y_train == 1).sum())\n",
    "        neg_count = float((y_train == 0).sum())\n",
    "        pos_weight = torch.tensor([(neg_count / max(pos_count, 1.0))], device=device, dtype=torch.float32)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR_CLS, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=5)\n",
    "\n",
    "        best_val, bad = -1.0, 0\n",
    "        best_state = None\n",
    "\n",
    "        for ep in range(1, EPOCHS_CLS + 1):\n",
    "            loss = train_one_epoch_cls(model, train_loader, optimizer, criterion, device)\n",
    "            yv, pv = infer(model, val_loader, device)\n",
    "            val_ap = average_precision_score(yv, pv) if yv.size > 0 else 0.0  # PRC-AUC for early stopping\n",
    "            scheduler.step(val_ap)\n",
    "            print(\n",
    "                f\"[Fold {fold}] Ep {ep:03d} | train loss {loss:.4f} | val AP {val_ap:.4f} \"\n",
    "                f\"| lr {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "            )\n",
    "\n",
    "            if val_ap > best_val + 1e-5:\n",
    "                best_val, bad = val_ap, 0\n",
    "                best_state = model.state_dict()\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= PATIENCE and ep >= 30:\n",
    "                    print(f\"[Fold {fold}] Early stop.\")\n",
    "                    break\n",
    "\n",
    "        # test with best validation state\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        yt, pt = infer(model, test_loader, device)\n",
    "        all_true.append(yt)\n",
    "        all_prob.append(pt)\n",
    "        m = calc_metrics(yt, pt, threshold=0.5)\n",
    "        fold_metrics.append({\"Fold\": fold, **m})\n",
    "        print(f\"[Fold {fold}] Test metrics: {m}\")\n",
    "\n",
    "        # keep best model by ROC-AUC\n",
    "        if m[\"ROC-AUC\"] > best_score:\n",
    "            best_score = m[\"ROC-AUC\"]\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "\n",
    "    # 6) Save per-fold and overall results\n",
    "    pd.DataFrame(fold_metrics).to_csv(FOLD_DETAIL_CSV, index=False)\n",
    "\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_prob = np.concatenate(all_prob)\n",
    "    overall_metrics = calc_metrics(all_true, all_prob, threshold=0.5)\n",
    "\n",
    "    print(\"\\n========== Overall (5-fold aggregated) ==========\")\n",
    "    for k, v in overall_metrics.items():\n",
    "        print(f\"{k}: {v:.6f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "    print(f\"Best fold model saved to: {BEST_MODEL_PATH}\")\n",
    "\n",
    "    # Single-row summary for easy comparison across models\n",
    "    out_row = {\n",
    "        \"Model\": f\"GINE(AE init) + {pick_mode} undersampling ({dist_metric}), ratio={ratio}\",\n",
    "        **overall_metrics,\n",
    "    }\n",
    "    pd.DataFrame([out_row]).to_csv(RESULT_CSV, index=False)\n",
    "    print(f\"\\nPer-fold metrics -> {FOLD_DETAIL_CSV}\")\n",
    "    print(f\"Overall metrics -> {RESULT_CSV}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--csv\",\n",
    "        type=str,\n",
    "        default=DEFAULT_CSV,\n",
    "        help=\"Path to CSV with columns 'smiles' and 'antibiotic_activity'.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pick\",\n",
    "        type=str,\n",
    "        choices=[\"nearest\", \"farthest\"],\n",
    "        default=DEFAULT_PICK,\n",
    "        help=\"Negative selection mode relative to positive centroid (default: nearest).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--metric\",\n",
    "        type=str,\n",
    "        choices=[\"cosine\", \"euclidean\"],\n",
    "        default=DIST_METRIC,\n",
    "        help=\"Similarity metric in embedding space (default: cosine).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ratio\",\n",
    "        type=float,\n",
    "        default=NEG_POS_RATIO,\n",
    "        help=\"Negative:positive ratio for undersampling (default: 1.0).\",\n",
    "    )\n",
    "    # In notebook/IDE environments extra args like --f=... may be injected.\n",
    "    args, _ = parser.parse_known_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\tau = \\alpha\\times\\frac{a}{b}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
