{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier  # 决策树\n",
    "from sklearn.neighbors import KNeighborsClassifier  # KNN\n",
    "from xgboost import XGBClassifier  # XGBoost\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Training model: Logistic Regression\n",
      "Best Params: {'C': 0.01}\n",
      "ROC-AUC: 0.5864\n",
      "PRC-AUC: 0.4440\n",
      "Accuracy: 0.6667\n",
      "F1 Score: 0.2533\n",
      "MCC: 0.1263\n",
      "Recall: 0.1696\n",
      "Precision: 0.5000\n",
      "False Positives: 19\n",
      "False Positive Rate: 0.0848\n",
      "\n",
      ">>> Training model: SVM\n",
      "Best Params: {'C': 1, 'kernel': 'rbf'}\n",
      "ROC-AUC: 0.5453\n",
      "PRC-AUC: 0.4645\n",
      "Accuracy: 0.6875\n",
      "F1 Score: 0.1176\n",
      "MCC: 0.2063\n",
      "Recall: 0.0625\n",
      "Precision: 1.0000\n",
      "False Positives: 0\n",
      "False Positive Rate: 0.0000\n",
      "\n",
      ">>> Training model: Random Forest\n",
      "Best Params: {'max_depth': None, 'n_estimators': 100}\n",
      "ROC-AUC: 0.5521\n",
      "PRC-AUC: 0.4401\n",
      "Accuracy: 0.6905\n",
      "F1 Score: 0.1746\n",
      "MCC: 0.2001\n",
      "Recall: 0.0982\n",
      "Precision: 0.7857\n",
      "False Positives: 3\n",
      "False Positive Rate: 0.0134\n",
      "\n",
      ">>> Training model: Decision Tree\n",
      "Best Params: {'max_depth': None, 'min_samples_split': 10}\n",
      "ROC-AUC: 0.4468\n",
      "PRC-AUC: 0.4032\n",
      "Accuracy: 0.4524\n",
      "F1 Score: 0.3235\n",
      "MCC: -0.1180\n",
      "Recall: 0.3929\n",
      "Precision: 0.2750\n",
      "False Positives: 116\n",
      "False Positive Rate: 0.5179\n",
      "\n",
      ">>> Training model: KNN\n",
      "Best Params: {'n_neighbors': 5, 'weights': 'distance'}\n",
      "ROC-AUC: 0.5418\n",
      "PRC-AUC: 0.4917\n",
      "Accuracy: 0.6875\n",
      "F1 Score: 0.1322\n",
      "MCC: 0.1955\n",
      "Recall: 0.0714\n",
      "Precision: 0.8889\n",
      "False Positives: 1\n",
      "False Positive Rate: 0.0045\n",
      "\n",
      ">>> Training model: XGBoost\n",
      "Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 50}\n",
      "ROC-AUC: 0.6801\n",
      "PRC-AUC: 0.4941\n",
      "Accuracy: 0.6696\n",
      "F1 Score: 0.1120\n",
      "MCC: 0.0873\n",
      "Recall: 0.0625\n",
      "Precision: 0.5385\n",
      "False Positives: 6\n",
      "False Positive Rate: 0.0268\n",
      "\n",
      "所有模型的评估结果：\n",
      "                 Model                                        Best Params  \\\n",
      "0  Logistic Regression                                        {'C': 0.01}   \n",
      "1                  SVM                          {'C': 1, 'kernel': 'rbf'}   \n",
      "2        Random Forest           {'max_depth': None, 'n_estimators': 100}   \n",
      "3        Decision Tree       {'max_depth': None, 'min_samples_split': 10}   \n",
      "4                  KNN          {'n_neighbors': 5, 'weights': 'distance'}   \n",
      "5              XGBoost  {'learning_rate': 0.1, 'max_depth': 6, 'n_esti...   \n",
      "\n",
      "    ROC-AUC   PRC-AUC  Accuracy  F1 Score       MCC    Recall  Precision  \\\n",
      "0  0.586356  0.444028  0.666667  0.253333  0.126252  0.169643   0.500000   \n",
      "1  0.545261  0.464487  0.687500  0.117647  0.206284  0.062500   1.000000   \n",
      "2  0.552077  0.440077  0.690476  0.174603  0.200100  0.098214   0.785714   \n",
      "3  0.446767  0.403227  0.452381  0.323529 -0.117985  0.392857   0.275000   \n",
      "4  0.541793  0.491664  0.687500  0.132231  0.195515  0.071429   0.888889   \n",
      "5  0.680106  0.494089  0.669643  0.112000  0.087298  0.062500   0.538462   \n",
      "\n",
      "   False Positives  False Positive Rate  \n",
      "0               19             0.084821  \n",
      "1                0             0.000000  \n",
      "2                3             0.013393  \n",
      "3              116             0.517857  \n",
      "4                1             0.004464  \n",
      "5                6             0.026786  \n",
      "\n",
      "评估结果已保存到 model_evaluation_results.csv。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n1) 将 library_1_binarized.csv 和 library_2_binarized.csv 拼接为训练集，library_3_binarized.csv 为测试集；\\n2) 根据 'antibiotic_activity' 分为正负样本，并对负样本进行KMeans聚类，从而选取代表性的负样本；\\n3) 计算Morgan指纹(若binarized.csv中无需再计算指纹则可跳过这一步)；\\n4) 训练集和测试集分别拼合正负样本后，进行标准化处理；\\n5) 用多种模型(GridSearchCV)进行超参数搜索；\\n6) 计算 ROC-AUC、PRC-AUC、Accuracy、F1、MCC、Recall、Precision、混淆矩阵等指标，并输出结果到CSV。\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier  # 决策树\n",
    "from sklearn.neighbors import KNeighborsClassifier  # KNN\n",
    "from xgboost import XGBClassifier  # XGBoost\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============== 1. 读取数据 ==============\n",
    "# 请根据实际文件路径进行修改\n",
    "train_data_1 = pd.read_csv(r'C:\\Users\\DuYih\\Desktop\\github\\DL Microbiology Antibiotics\\SyntheMol-main\\data\\Data\\1_training_data\\library_1_binarized.csv')\n",
    "train_data_2 = pd.read_csv(r'C:\\Users\\DuYih\\Desktop\\github\\DL Microbiology Antibiotics\\SyntheMol-main\\data\\Data\\1_training_data\\library_2_binarized.csv')\n",
    "test_data = pd.read_csv(r'C:\\Users\\DuYih\\Desktop\\github\\DL Microbiology Antibiotics\\SyntheMol-main\\data\\Data\\1_training_data\\library_3_binarized.csv')\n",
    "\n",
    "# 合并作为训练集\n",
    "train_data = pd.concat([train_data_1, train_data_2], ignore_index=True)\n",
    "\n",
    "# ============== 2. 如果需要，从SMILES计算Morgan指纹 ==============\n",
    "# 若 bianrized.csv 中实际上并无 'smiles' 列（或已是数值特征），可注释此函数与相关调用。\n",
    "def get_fingerprint(smiles, n_bits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits)\n",
    "        return np.array(fp)\n",
    "    else:\n",
    "        return np.zeros(n_bits)\n",
    "\n",
    "# 如果 CSV 中仍包含“smiles”列，需要计算分子指纹；否则请将此段注释/删除\n",
    "if 'smiles' in train_data.columns:\n",
    "    train_data['fingerprint'] = train_data['smiles'].apply(get_fingerprint)\n",
    "if 'smiles' in test_data.columns:\n",
    "    test_data['fingerprint'] = test_data['smiles'].apply(get_fingerprint)\n",
    "\n",
    "# ============== 3. 分离正负样本 ==============\n",
    "# 假设各文件都带有 'antibiotic_activity' 列 (0或1)\n",
    "train_positive = train_data[train_data['antibiotic_activity'] == 1]\n",
    "train_negative = train_data[train_data['antibiotic_activity'] == 0]\n",
    "\n",
    "test_positive = test_data[test_data['antibiotic_activity'] == 1]\n",
    "test_negative = test_data[test_data['antibiotic_activity'] == 0]\n",
    "\n",
    "# ============== 4. 负样本聚类筛选 ==============\n",
    "# ---- 训练集 ----\n",
    "num_clusters_train = 2 * len(train_positive)\n",
    "num_clusters_train = min(num_clusters_train, len(train_negative))  # 防止聚类数>负样本总数\n",
    "num_clusters_train = max(num_clusters_train, 1)  # 至少保证1个簇\n",
    "\n",
    "train_negative_fps = np.vstack(train_negative['fingerprint'].values)  # 将fingerprint列拼接成矩阵\n",
    "kmeans_train = KMeans(n_clusters=num_clusters_train, random_state=42)\n",
    "kmeans_train.fit(train_negative_fps)\n",
    "closest_train, _ = pairwise_distances_argmin_min(kmeans_train.cluster_centers_, train_negative_fps)\n",
    "selected_negative_train = train_negative.iloc[closest_train]\n",
    "\n",
    "# ---- 测试集 ----\n",
    "num_clusters_test = 2 * len(test_positive)\n",
    "num_clusters_test = min(num_clusters_test, len(test_negative))\n",
    "num_clusters_test = max(num_clusters_test, 1)\n",
    "\n",
    "test_negative_fps = np.vstack(test_negative['fingerprint'].values)\n",
    "kmeans_test = KMeans(n_clusters=num_clusters_test, random_state=42)\n",
    "kmeans_test.fit(test_negative_fps)\n",
    "closest_test, _ = pairwise_distances_argmin_min(kmeans_test.cluster_centers_, test_negative_fps)\n",
    "selected_negative_test = test_negative.iloc[closest_test]\n",
    "\n",
    "# ============== 5. 最终训练集与测试集 ==============\n",
    "final_train_data = pd.concat([train_positive, selected_negative_train], ignore_index=True)\n",
    "final_test_data = pd.concat([test_positive, selected_negative_test], ignore_index=True)\n",
    "\n",
    "# 提取特征和标签\n",
    "X_train = np.vstack(final_train_data['fingerprint'].values)\n",
    "y_train = final_train_data['antibiotic_activity'].values\n",
    "\n",
    "X_test = np.vstack(final_test_data['fingerprint'].values)\n",
    "y_test = final_test_data['antibiotic_activity'].values\n",
    "\n",
    "# ============== 6. 特征缩放 ==============\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ============== 7. 定义模型与超参数 ==============\n",
    "models_params = {\n",
    "    'Logistic Regression': (\n",
    "        LogisticRegression(max_iter=1000),\n",
    "        {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "    ),\n",
    "    'SVM': (\n",
    "        SVC(probability=True),\n",
    "        {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "    ),\n",
    "    'Random Forest': (\n",
    "        RandomForestClassifier(),\n",
    "        {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20]}\n",
    "    ),\n",
    "    'Decision Tree': (\n",
    "        DecisionTreeClassifier(),\n",
    "        {'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10]}\n",
    "    ),\n",
    "    'KNN': (\n",
    "        KNeighborsClassifier(),\n",
    "        {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}\n",
    "    ),\n",
    "    'XGBoost': (\n",
    "        XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        {'n_estimators': [50, 100], 'max_depth': [3, 6], 'learning_rate': [0.01, 0.1]}\n",
    "    )\n",
    "}\n",
    "\n",
    "# 用于保存各模型训练结果\n",
    "results = []\n",
    "\n",
    "# ============== 8. 模型训练与评估 ==============\n",
    "for model_name, (model, params) in models_params.items():\n",
    "    print(f\"\\n>>> Training model: {model_name}\")\n",
    "    clf = GridSearchCV(model, params, scoring='roc_auc', cv=5)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    best_model = clf.best_estimator_\n",
    "    # 预测\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    # 若支持 predict_proba，则拿到正类(=1)的预测概率；否则使用 decision_function 做归一化\n",
    "    if hasattr(best_model, \"predict_proba\"):\n",
    "        y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        y_decision = best_model.decision_function(X_test_scaled)\n",
    "        # 将决策函数输出归一化到 [0,1]\n",
    "        y_pred_proba = (\n",
    "            (y_decision - y_decision.min()) / (y_decision.max() - y_decision.min())\n",
    "            if y_decision.max() != y_decision.min()\n",
    "            else np.zeros_like(y_decision)\n",
    "        )\n",
    "    \n",
    "    # 计算性能指标\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    prc_auc = auc(recall, precision)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    recall_val = recall_score(y_test, y_pred)\n",
    "    precision_val = precision_score(y_test, y_pred)\n",
    "\n",
    "    # 混淆矩阵\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    fp_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "    print(f\"Best Params: {clf.best_params_}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PRC-AUC: {prc_auc:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"MCC: {mcc:.4f}\")\n",
    "    print(f\"Recall: {recall_val:.4f}\")\n",
    "    print(f\"Precision: {precision_val:.4f}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Positive Rate: {fp_rate:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Best Params': clf.best_params_,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'PRC-AUC': prc_auc,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'MCC': mcc,\n",
    "        'Recall': recall_val,\n",
    "        'Precision': precision_val,\n",
    "        'False Positives': fp,\n",
    "        'False Positive Rate': fp_rate\n",
    "    })\n",
    "\n",
    "# ============== 9. 结果保存并展示 ==============\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('model_evaluation_results12312.csv', index=False)\n",
    "print(\"\\n所有模型的评估结果：\")\n",
    "print(results_df)\n",
    "print(\"\\n评估结果已保存到 model_evaluation_results.csv。\")\n",
    "\n",
    "# ============== 代码总结 ==============\n",
    "\"\"\"\n",
    "1) 将 library_1_binarized.csv 和 library_2_binarized.csv 拼接为训练集，library_3_binarized.csv 为测试集；\n",
    "2) 根据 'antibiotic_activity' 分为正负样本，并对负样本进行KMeans聚类，从而选取代表性的负样本；\n",
    "3) 计算Morgan指纹(若binarized.csv中无需再计算指纹则可跳过这一步)；\n",
    "4) 训练集和测试集分别拼合正负样本后，进行标准化处理；\n",
    "5) 用多种模型(GridSearchCV)进行超参数搜索；\n",
    "6) 计算 ROC-AUC、PRC-AUC、Accuracy、F1、MCC、Recall、Precision、混淆矩阵等指标，并输出结果到CSV。\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library_3: 在训练集聚类范围内的化合物数 = 3848\n",
      "Library_3: 不在范围内的化合物数 = 1528\n",
      "最终用于验证的测试样本数 = 249\n",
      "\n",
      ">>> Training model: Logistic Regression\n",
      "Best Params: {'C': 0.01}\n",
      "ROC-AUC: 0.5826\n",
      "PRC-AUC: 0.4469\n",
      "Accuracy: 0.6747\n",
      "F1 Score: 0.2832\n",
      "MCC: 0.1570\n",
      "Recall: 0.1928\n",
      "Precision: 0.5333\n",
      "False Positives: 14\n",
      "False Positive Rate: 0.0843\n",
      "\n",
      ">>> Training model: SVM\n",
      "Best Params: {'C': 1, 'kernel': 'rbf'}\n",
      "ROC-AUC: 0.5813\n",
      "PRC-AUC: 0.4982\n",
      "Accuracy: 0.6908\n",
      "F1 Score: 0.1348\n",
      "MCC: 0.2222\n",
      "Recall: 0.0723\n",
      "Precision: 1.0000\n",
      "False Positives: 0\n",
      "False Positive Rate: 0.0000\n",
      "\n",
      ">>> Training model: Random Forest\n",
      "Best Params: {'max_depth': None, 'n_estimators': 100}\n",
      "ROC-AUC: 0.5689\n",
      "PRC-AUC: 0.4538\n",
      "Accuracy: 0.6908\n",
      "F1 Score: 0.1720\n",
      "MCC: 0.2025\n",
      "Recall: 0.0964\n",
      "Precision: 0.8000\n",
      "False Positives: 2\n",
      "False Positive Rate: 0.0120\n",
      "\n",
      ">>> Training model: Decision Tree\n",
      "Best Params: {'max_depth': None, 'min_samples_split': 5}\n",
      "ROC-AUC: 0.5534\n",
      "PRC-AUC: 0.4963\n",
      "Accuracy: 0.5863\n",
      "F1 Score: 0.3905\n",
      "MCC: 0.0776\n",
      "Recall: 0.3976\n",
      "Precision: 0.3837\n",
      "False Positives: 53\n",
      "False Positive Rate: 0.3193\n",
      "\n",
      ">>> Training model: KNN\n",
      "Best Params: {'n_neighbors': 5, 'weights': 'distance'}\n",
      "ROC-AUC: 0.5673\n",
      "PRC-AUC: 0.5691\n",
      "Accuracy: 0.6908\n",
      "F1 Score: 0.1538\n",
      "MCC: 0.2094\n",
      "Recall: 0.0843\n",
      "Precision: 0.8750\n",
      "False Positives: 1\n",
      "False Positive Rate: 0.0060\n",
      "\n",
      ">>> Training model: XGBoost\n",
      "Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 50}\n",
      "ROC-AUC: 0.6825\n",
      "PRC-AUC: 0.4872\n",
      "Accuracy: 0.6627\n",
      "F1 Score: 0.0870\n",
      "MCC: 0.0456\n",
      "Recall: 0.0482\n",
      "Precision: 0.4444\n",
      "False Positives: 5\n",
      "False Positive Rate: 0.0301\n",
      "\n",
      "所有模型的评估结果：\n",
      "                 Model                                        Best Params  \\\n",
      "0  Logistic Regression                                        {'C': 0.01}   \n",
      "1                  SVM                          {'C': 1, 'kernel': 'rbf'}   \n",
      "2        Random Forest           {'max_depth': None, 'n_estimators': 100}   \n",
      "3        Decision Tree        {'max_depth': None, 'min_samples_split': 5}   \n",
      "4                  KNN          {'n_neighbors': 5, 'weights': 'distance'}   \n",
      "5              XGBoost  {'learning_rate': 0.1, 'max_depth': 6, 'n_esti...   \n",
      "\n",
      "    ROC-AUC   PRC-AUC  Accuracy  F1 Score       MCC    Recall  Precision  \\\n",
      "0  0.582559  0.446879  0.674699  0.283186  0.157027  0.192771   0.533333   \n",
      "1  0.581325  0.498206  0.690763  0.134831  0.222222  0.072289   1.000000   \n",
      "2  0.568914  0.453753  0.690763  0.172043  0.202495  0.096386   0.800000   \n",
      "3  0.553382  0.496307  0.586345  0.390533  0.077640  0.397590   0.383721   \n",
      "4  0.567281  0.569068  0.690763  0.153846  0.209351  0.084337   0.875000   \n",
      "5  0.682465  0.487176  0.662651  0.086957  0.045644  0.048193   0.444444   \n",
      "\n",
      "   False Positives  False Positive Rate  \n",
      "0               14             0.084337  \n",
      "1                0             0.000000  \n",
      "2                2             0.012048  \n",
      "3               53             0.319277  \n",
      "4                1             0.006024  \n",
      "5                5             0.030120  \n",
      "\n",
      "评估结果已保存到 model_evaluation_results.csv。\n",
      "不在范围内的化合物已输出到 library_3_out_range.csv，共 1528 条。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n1) 训练集 (library_1 + library_2)：\\n   - 分出正负样本，对负样本做 KMeans 并选取簇中心附近样本，形成最终训练集；\\n   - 同时记录训练集负样本各簇在 KMeans 中的“半径”。\\n2) 测试集 (library_3)：\\n   - 利用训练负样本聚类的中心和半径来判断 library_3 的每个化合物是否在“训练集聚类范围”内；\\n     - 若距离其所分配的簇中心 <= 该簇在训练集中的最大距离则“在范围内”，否则“超出范围”；\\n   - 仅对“在范围内”的化合物做后续测试：其中的负样本再做 KMeans 筛选，正样本直接保留；\\n   - 合并得到最终测试集后，与训练好的模型进行预测和评估；\\n3) 计算多种指标并输出到 model_evaluation_results.csv；\\n4) 同时将“在范围内”与“不在范围内”的化合物分别输出为 CSV，方便对比。\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier  # 决策树\n",
    "from sklearn.neighbors import KNeighborsClassifier  # KNN\n",
    "from xgboost import XGBClassifier  # XGBoost\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============== 1. 读取数据 ==============\n",
    "# 请根据实际路径修改\n",
    "train_data_1 = pd.read_csv(r'C:\\Users\\DuYih\\Desktop\\github\\DL Microbiology Antibiotics\\SyntheMol-main\\data\\Data\\1_training_data\\library_1_binarized.csv')\n",
    "train_data_2 = pd.read_csv(r'C:\\Users\\DuYih\\Desktop\\github\\DL Microbiology Antibiotics\\SyntheMol-main\\data\\Data\\1_training_data\\library_2_binarized.csv')\n",
    "test_data = pd.read_csv(r'C:\\Users\\DuYih\\Desktop\\github\\DL Microbiology Antibiotics\\SyntheMol-main\\data\\Data\\1_training_data\\library_3_binarized.csv')\n",
    "\n",
    "# 合并library_1与library_2作为训练集\n",
    "train_data = pd.concat([train_data_1, train_data_2], ignore_index=True)\n",
    "\n",
    "# ============== 2. 从SMILES计算Morgan指纹（如已是数值特征可去掉此步） ==============\n",
    "def get_fingerprint(smiles, n_bits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits)\n",
    "        return np.array(fp)\n",
    "    else:\n",
    "        return np.zeros(n_bits)\n",
    "\n",
    "# 如果CSV含有'smiles'列，则计算指纹\n",
    "if 'smiles' in train_data.columns:\n",
    "    train_data['fingerprint'] = train_data['smiles'].apply(get_fingerprint)\n",
    "if 'smiles' in test_data.columns:\n",
    "    test_data['fingerprint'] = test_data['smiles'].apply(get_fingerprint)\n",
    "\n",
    "# ============== 3. 分离正负样本（训练集） ==============\n",
    "train_positive = train_data[train_data['antibiotic_activity'] == 1]\n",
    "train_negative = train_data[train_data['antibiotic_activity'] == 0]\n",
    "\n",
    "# ============== 4. 对训练负样本做KMeans聚类并选取代表 ==============\n",
    "num_clusters_train = 2 * len(train_positive)\n",
    "num_clusters_train = min(num_clusters_train, len(train_negative))  # 防止聚类数超过负样本量\n",
    "num_clusters_train = max(num_clusters_train, 1)\n",
    "\n",
    "train_negative_fps = np.vstack(train_negative['fingerprint'].values)\n",
    "kmeans_train = KMeans(n_clusters=num_clusters_train, random_state=42)\n",
    "kmeans_train.fit(train_negative_fps)\n",
    "\n",
    "# 找到各簇中心点附近的负样本（代表样本）\n",
    "closest_train, _ = pairwise_distances_argmin_min(kmeans_train.cluster_centers_, train_negative_fps)\n",
    "selected_negative_train = train_negative.iloc[closest_train]\n",
    "\n",
    "# ============== 4.1 计算各簇“半径”（训练集的负样本到其簇中心的最大距离） ==============\n",
    "# 用于后续判定 “测试集化合物是否在训练集聚类范围内”\n",
    "train_cluster_labels = kmeans_train.predict(train_negative_fps)\n",
    "radii = []\n",
    "for c_id in range(num_clusters_train):\n",
    "    center = kmeans_train.cluster_centers_[c_id]\n",
    "    # 该簇下所有训练负样本\n",
    "    cluster_points = train_negative_fps[train_cluster_labels == c_id]\n",
    "    dists = np.linalg.norm(cluster_points - center, axis=1)\n",
    "    max_dist = dists.max()\n",
    "    radii.append(max_dist)\n",
    "radii = np.array(radii)\n",
    "\n",
    "# ============== 5. 构建最终训练集：正样本 + 代表负样本 ==============\n",
    "final_train_data = pd.concat([train_positive, selected_negative_train], ignore_index=True)\n",
    "\n",
    "# ============== 6. 处理测试集 ==============\n",
    "# 先判断library_3中的化合物是否在“训练集聚类范围”内\n",
    "# 如果在范围内 -> 用于后续验证； 否则 -> 视为不做验证的那部分\n",
    "\n",
    "# 先取出 test_data 中全部指纹\n",
    "test_data_fps = np.vstack(test_data['fingerprint'].values)\n",
    "\n",
    "# 定义函数：判断某个指纹是否在训练负样本的聚类范围内\n",
    "def is_in_training_cluster_range(fp):\n",
    "    # 先用 kmeans_train 预测属于哪一个簇\n",
    "    assigned_cluster = kmeans_train.predict(fp.reshape(1, -1))[0]\n",
    "    # 计算该簇中心 与 该指纹 的欧几里得距离\n",
    "    center = kmeans_train.cluster_centers_[assigned_cluster]\n",
    "    dist = np.linalg.norm(fp - center)\n",
    "    # 如果该距离 <= 该簇在训练集中对应的“最大距离”，则认为在范围内\n",
    "    if dist <= radii[assigned_cluster]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "in_range_mask = []\n",
    "for i in range(len(test_data_fps)):\n",
    "    fp = test_data_fps[i]\n",
    "    in_range_mask.append(is_in_training_cluster_range(fp))\n",
    "\n",
    "test_data['in_training_cluster_range'] = in_range_mask\n",
    "\n",
    "# 拆分：在范围内 vs 不在范围内\n",
    "test_data_in_range = test_data[test_data['in_training_cluster_range'] == True]\n",
    "test_data_out_range = test_data[test_data['in_training_cluster_range'] == False]\n",
    "\n",
    "print(f\"Library_3: 在训练集聚类范围内的化合物数 = {len(test_data_in_range)}\")\n",
    "print(f\"Library_3: 不在范围内的化合物数 = {len(test_data_out_range)}\")\n",
    "\n",
    "# ============== 7. 对“范围内”的测试集再做负样本KMeans选取 ==============\n",
    "test_positive_in_range = test_data_in_range[test_data_in_range['antibiotic_activity'] == 1]\n",
    "test_negative_in_range = test_data_in_range[test_data_in_range['antibiotic_activity'] == 0]\n",
    "\n",
    "num_clusters_test_in_range = 2 * len(test_positive_in_range)\n",
    "num_clusters_test_in_range = min(num_clusters_test_in_range, len(test_negative_in_range))\n",
    "num_clusters_test_in_range = max(num_clusters_test_in_range, 1)\n",
    "\n",
    "test_negative_in_range_fps = np.vstack(test_negative_in_range['fingerprint'].values)\n",
    "kmeans_test_in_range = KMeans(n_clusters=num_clusters_test_in_range, random_state=42)\n",
    "kmeans_test_in_range.fit(test_negative_in_range_fps)\n",
    "\n",
    "closest_test_in_range, _ = pairwise_distances_argmin_min(kmeans_test_in_range.cluster_centers_, test_negative_in_range_fps)\n",
    "selected_negative_test_in_range = test_negative_in_range.iloc[closest_test_in_range]\n",
    "\n",
    "# 最终用于模型验证的 测试集 = 所有“范围内”正样本 + 聚类筛选的负样本\n",
    "final_test_data = pd.concat([test_positive_in_range, selected_negative_test_in_range], ignore_index=True)\n",
    "print(f\"最终用于验证的测试样本数 = {len(final_test_data)}\")\n",
    "\n",
    "# ============== 8. 准备训练/测试特征与标签 ==============\n",
    "X_train = np.vstack(final_train_data['fingerprint'].values)\n",
    "y_train = final_train_data['antibiotic_activity'].values\n",
    "\n",
    "X_test = np.vstack(final_test_data['fingerprint'].values)\n",
    "y_test = final_test_data['antibiotic_activity'].values\n",
    "\n",
    "# ============== 9. 特征缩放 ==============\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ============== 10. 定义模型与超参数网格 ==============\n",
    "models_params = {\n",
    "    'Logistic Regression': (\n",
    "        LogisticRegression(max_iter=1000),\n",
    "        {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "    ),\n",
    "    'SVM': (\n",
    "        SVC(probability=True),\n",
    "        {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "    ),\n",
    "    'Random Forest': (\n",
    "        RandomForestClassifier(),\n",
    "        {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20]}\n",
    "    ),\n",
    "    'Decision Tree': (\n",
    "        DecisionTreeClassifier(),\n",
    "        {'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10]}\n",
    "    ),\n",
    "    'KNN': (\n",
    "        KNeighborsClassifier(),\n",
    "        {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}\n",
    "    ),\n",
    "    'XGBoost': (\n",
    "        XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        {'n_estimators': [50, 100], 'max_depth': [3, 6], 'learning_rate': [0.01, 0.1]}\n",
    "    )\n",
    "}\n",
    "\n",
    "# 用于保存各模型结果\n",
    "results = []\n",
    "\n",
    "# ============== 11. 训练与评估 ==============\n",
    "for model_name, (model, params) in models_params.items():\n",
    "    print(f\"\\n>>> Training model: {model_name}\")\n",
    "    clf = GridSearchCV(model, params, scoring='roc_auc', cv=5)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    best_model = clf.best_estimator_\n",
    "\n",
    "    # 预测\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    # 预测概率\n",
    "    if hasattr(best_model, \"predict_proba\"):\n",
    "        y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        # 对不支持 predict_proba 的模型，用 decision_function 做归一化\n",
    "        y_decision = best_model.decision_function(X_test_scaled)\n",
    "        y_pred_proba = (\n",
    "            (y_decision - y_decision.min()) / (y_decision.max() - y_decision.min())\n",
    "            if y_decision.max() != y_decision.min() else np.zeros_like(y_decision)\n",
    "        )\n",
    "    \n",
    "    # 计算各项指标\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    precision_arr, recall_arr, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    prc_auc = auc(recall_arr, precision_arr)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    recall_val = recall_score(y_test, y_pred)\n",
    "    precision_val = precision_score(y_test, y_pred)\n",
    "\n",
    "    # 混淆矩阵\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    fp_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "    print(f\"Best Params: {clf.best_params_}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PRC-AUC: {prc_auc:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"MCC: {mcc:.4f}\")\n",
    "    print(f\"Recall: {recall_val:.4f}\")\n",
    "    print(f\"Precision: {precision_val:.4f}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Positive Rate: {fp_rate:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Best Params': clf.best_params_,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'PRC-AUC': prc_auc,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'MCC': mcc,\n",
    "        'Recall': recall_val,\n",
    "        'Precision': precision_val,\n",
    "        'False Positives': fp,\n",
    "        'False Positive Rate': fp_rate\n",
    "    })\n",
    "\n",
    "# ============== 12. 输出结果 ==============\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('model_evaluation_results_part_library3.csv', index=False)\n",
    "\n",
    "# 把“在范围内”和“不在范围内”的化合物也输出，便于查看\n",
    "test_data_in_range.to_csv('library_3_in_range.csv', index=False)\n",
    "test_data_out_range.to_csv('library_3_out_range.csv', index=False)\n",
    "\n",
    "print(\"\\n所有模型的评估结果：\")\n",
    "print(results_df)\n",
    "print(\"\\n评估结果已保存到 model_evaluation_results.csv。\")\n",
    "print(f\"不在范围内的化合物已输出到 library_3_out_range.csv，共 {len(test_data_out_range)} 条。\")\n",
    "\n",
    "# ============== 代码总结 ==============\n",
    "\"\"\"\n",
    "1) 训练集 (library_1 + library_2)：\n",
    "   - 分出正负样本，对负样本做 KMeans 并选取簇中心附近样本，形成最终训练集；\n",
    "   - 同时记录训练集负样本各簇在 KMeans 中的“半径”。\n",
    "2) 测试集 (library_3)：\n",
    "   - 利用训练负样本聚类的中心和半径来判断 library_3 的每个化合物是否在“训练集聚类范围”内；\n",
    "     - 若距离其所分配的簇中心 <= 该簇在训练集中的最大距离则“在范围内”，否则“超出范围”；\n",
    "   - 仅对“在范围内”的化合物做后续测试：其中的负样本再做 KMeans 筛选，正样本直接保留；\n",
    "   - 合并得到最终测试集后，与训练好的模型进行预测和评估；\n",
    "3) 计算多种指标并输出到 model_evaluation_results.csv；\n",
    "4) 同时将“在范围内”与“不在范围内”的化合物分别输出为 CSV，方便对比。\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier  # 添加决策树\n",
    "from sklearn.neighbors import KNeighborsClassifier  # 添加KNN\n",
    "from xgboost import XGBClassifier  # 添加XGBoost\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    confusion_matrix  # 添加混淆矩阵\n",
    ")\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('C:/Users/DuYih/Desktop/github/DL Microbiology Antibiotics/SyntheMol-main/data/Data/1_training_data/raw_data.csv')\n",
    "\n",
    "# 计算分子指纹\n",
    "def get_fingerprint(smiles, n_bits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits)\n",
    "        return np.array(fp)\n",
    "    else:\n",
    "        return np.zeros(n_bits)\n",
    "\n",
    "# 应用分子指纹计算\n",
    "data['fingerprint'] = data['smiles'].apply(lambda x: get_fingerprint(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "                               ...                        \n",
       "13519    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "13520    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "13521    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "13522    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "13523    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "Name: fingerprint, Length: 13524, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['fingerprint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "Processing fold 2\n",
      "Processing fold 3\n",
      "Processing fold 4\n",
      "Processing fold 5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier  # 添加决策树\n",
    "from sklearn.neighbors import KNeighborsClassifier  # 添加KNN\n",
    "from xgboost import XGBClassifier  # 添加XGBoost\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    confusion_matrix  # 添加混淆矩阵\n",
    ")\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('C:/Users/DuYih/Desktop/github/DL Microbiology Antibiotics/SyntheMol-main/data/Data/1_training_data/raw_data.csv')\n",
    "\n",
    "# 计算分子指纹\n",
    "def get_fingerprint(smiles, n_bits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits)\n",
    "        return np.array(fp)\n",
    "    else:\n",
    "        return np.zeros(n_bits)\n",
    "\n",
    "# 应用分子指纹计算\n",
    "data['fingerprint'] = data['smiles'].apply(lambda x: get_fingerprint(x))\n",
    "\n",
    "# 分离正负样本\n",
    "positive_samples = data[data['antibiotic_activity'] == 1]\n",
    "negative_samples = data[data['antibiotic_activity'] == 0]\n",
    "\n",
    "# 设置5折交叉验证\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 模型和超参数设置，添加决策树、KNN和XGBoost\n",
    "models_params = {\n",
    "    'Logistic Regression': (LogisticRegression(max_iter=1000), {'C': [0.01, 0.1, 1, 10, 100]}),\n",
    "    'SVM': (SVC(probability=True), {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}), \n",
    "    'Random Forest': (RandomForestClassifier(), {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20]}),\n",
    "    'Decision Tree': (DecisionTreeClassifier(), {'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10]}),\n",
    "    'KNN': (KNeighborsClassifier(), {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}),\n",
    "    'XGBoost': (XGBClassifier(use_label_encoder=False, eval_metric='logloss'), {'n_estimators': [50, 100], 'max_depth': [3, 6], 'learning_rate': [0.01, 0.1]})\n",
    "}\n",
    "\n",
    "# 存储每折的结果，包括新增的指标\n",
    "results = {model: {'ROC-AUC': [], 'PRC-AUC': [], 'Accuracy': [], 'F1 Score': [], 'MCC': [], 'Recall': [], 'Precision': [], \n",
    "                   'False Positives': [], 'False Positive Rate': []} for model in models_params}\n",
    "\n",
    "# 存储所有折的详细结果\n",
    "all_fold_results = []\n",
    "\n",
    "# 进行5折交叉验证\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(positive_samples)):\n",
    "    print(f\"Processing fold {fold + 1}\")\n",
    "    positive_train = positive_samples.iloc[train_index]\n",
    "    positive_test = positive_samples.iloc[test_index]\n",
    "    \n",
    "    # 分离负样本\n",
    "    negative_train, negative_test = train_test_split(negative_samples, test_size=0.2, random_state=fold)\n",
    "    \n",
    "    # 对训练集负样本进行聚类选择\n",
    "    num_clusters_train = 2 * len(positive_train)\n",
    "    num_clusters_train = min(num_clusters_train, len(negative_train))  # 防止聚类数超过样本数\n",
    "    num_clusters_train = max(num_clusters_train, 1)  # 至少一个簇\n",
    "    kmeans_train = KMeans(n_clusters=num_clusters_train, random_state=42)\n",
    "    negative_train_fingerprints = np.vstack(negative_train['fingerprint'].values)\n",
    "    kmeans_train.fit(negative_train_fingerprints)\n",
    "    closest_train, _ = pairwise_distances_argmin_min(kmeans_train.cluster_centers_, negative_train_fingerprints)\n",
    "    selected_negative_train = negative_train.iloc[closest_train[:num_clusters_train]]\n",
    "    \n",
    "    # 对测试集负样本进行聚类选择\n",
    "    num_clusters_test = 2 * len(positive_test)\n",
    "    num_clusters_test = min(num_clusters_test, len(negative_test))  # 防止聚类数超过样本数\n",
    "    num_clusters_test = max(num_clusters_test, 1)  # 至少一个簇\n",
    "    kmeans_test = KMeans(n_clusters=num_clusters_test, random_state=42)\n",
    "    negative_test_fingerprints = np.vstack(negative_test['fingerprint'].values)\n",
    "    kmeans_test.fit(negative_test_fingerprints)\n",
    "    closest_test, _ = pairwise_distances_argmin_min(kmeans_test.cluster_centers_, negative_test_fingerprints)\n",
    "    selected_negative_test = negative_test.iloc[closest_test[:num_clusters_test]]\n",
    "    \n",
    "    # 合并训练和测试数据集\n",
    "    train_data = pd.concat([positive_train, selected_negative_train])\n",
    "    test_data = pd.concat([positive_test, selected_negative_test])\n",
    "    \n",
    "    # 准备训练和测试数据\n",
    "    X_train = np.vstack(train_data['fingerprint'].values)\n",
    "    y_train = train_data['antibiotic_activity'].values\n",
    "    X_test = np.vstack(test_data['fingerprint'].values)\n",
    "    y_test = test_data['antibiotic_activity'].values\n",
    "    \n",
    "    # 特征缩放\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 对每个模型进行训练和评估\n",
    "    for model_name, (model, params) in models_params.items():\n",
    "        clf = GridSearchCV(model, params, scoring='roc_auc', cv=5)\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "        best_model = clf.best_estimator_\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        \n",
    "        # 处理预测概率\n",
    "        if hasattr(best_model, \"predict_proba\"):\n",
    "            y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "        else:\n",
    "            # 对于不支持 predict_proba 的模型，如某些 SVM\n",
    "            y_pred_proba = best_model.decision_function(X_test_scaled)\n",
    "            # 将决策函数输出归一化到 [0,1]\n",
    "            y_pred_proba = (y_pred_proba - y_pred_proba.min()) / (y_pred_proba.max() - y_pred_proba.min()) if y_pred_proba.max() != y_pred_proba.min() else np.zeros_like(y_pred_proba)\n",
    "        \n",
    "        # 计算性能指标\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        prc_auc = auc(recall, precision)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        recall_value = recall_score(y_test, y_pred)\n",
    "        precision_score_value = precision_score(y_test, y_pred)\n",
    "        # 计算混淆矩阵\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        fp_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "        # 存储结果\n",
    "        results[model_name]['ROC-AUC'].append(roc_auc)\n",
    "        results[model_name]['PRC-AUC'].append(prc_auc)\n",
    "        results[model_name]['Accuracy'].append(accuracy)\n",
    "        results[model_name]['F1 Score'].append(f1)\n",
    "        results[model_name]['MCC'].append(mcc)\n",
    "        results[model_name]['Precision'].append(precision_score_value)\n",
    "        results[model_name]['Recall'].append(recall_value)\n",
    "        results[model_name]['False Positives'].append(fp)\n",
    "        results[model_name]['False Positive Rate'].append(fp_rate)\n",
    "        \n",
    "        # 存储每折每个模型的详细结果\n",
    "        fold_result = {\n",
    "            'Fold': fold + 1,\n",
    "            'Model': model_name,\n",
    "            'ROC-AUC': roc_auc,\n",
    "            'PRC-AUC': prc_auc,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1 Score': f1,\n",
    "            'MCC': mcc,\n",
    "            'Recall': recall_value,\n",
    "            'Precision': precision_score_value,\n",
    "            'False Positives': fp,\n",
    "            'False Positive Rate': fp_rate\n",
    "        }\n",
    "        all_fold_results.append(fold_result)\n",
    "\n",
    "# 将所有折的结果保存为DataFrame\n",
    "fold_results_df = pd.DataFrame(all_fold_results)\n",
    "# 保存到CSV文件\n",
    "fold_results_df.to_csv('model_evaluation_per_fold_tem.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SMILES->Graph: 100%|██████████| 13524/13524 [00:10<00:00, 1249.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feat dim = 47 | Edge feat dim = 10 | N graphs = 13524\n",
      "[AE] Epoch 001 | recon MSE 0.02076 | lr 1.00e-03\n",
      "[AE] Epoch 002 | recon MSE 0.00463 | lr 1.00e-03\n",
      "[AE] Epoch 003 | recon MSE 0.00259 | lr 1.00e-03\n",
      "[AE] Epoch 004 | recon MSE 0.00179 | lr 1.00e-03\n",
      "[AE] Epoch 005 | recon MSE 0.00137 | lr 1.00e-03\n",
      "[AE] Epoch 006 | recon MSE 0.00114 | lr 1.00e-03\n",
      "[AE] Epoch 007 | recon MSE 0.00101 | lr 1.00e-03\n",
      "[AE] Epoch 008 | recon MSE 0.00093 | lr 1.00e-03\n",
      "[AE] Epoch 009 | recon MSE 0.00088 | lr 1.00e-03\n",
      "[AE] Epoch 010 | recon MSE 0.00094 | lr 1.00e-03\n",
      "[AE] Epoch 011 | recon MSE 0.00084 | lr 1.00e-03\n",
      "[AE] Epoch 012 | recon MSE 0.00081 | lr 1.00e-03\n",
      "[AE] Epoch 013 | recon MSE 0.00085 | lr 1.00e-03\n",
      "[AE] Epoch 014 | recon MSE 0.00078 | lr 1.00e-03\n",
      "[AE] Epoch 015 | recon MSE 0.00076 | lr 1.00e-03\n",
      "[AE] Epoch 016 | recon MSE 0.00076 | lr 1.00e-03\n",
      "[AE] Epoch 017 | recon MSE 0.00076 | lr 1.00e-03\n",
      "[AE] Epoch 018 | recon MSE 0.00075 | lr 1.00e-03\n",
      "[AE] Epoch 019 | recon MSE 0.00074 | lr 1.00e-03\n",
      "[AE] Epoch 020 | recon MSE 0.00074 | lr 1.00e-03\n",
      "[AE] Epoch 021 | recon MSE 0.00086 | lr 1.00e-03\n",
      "[AE] Epoch 022 | recon MSE 0.00074 | lr 1.00e-03\n",
      "[AE] Epoch 023 | recon MSE 0.00072 | lr 1.00e-03\n",
      "[AE] Epoch 024 | recon MSE 0.00072 | lr 1.00e-03\n",
      "[AE] Epoch 025 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 026 | recon MSE 0.00072 | lr 1.00e-03\n",
      "[AE] Epoch 027 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 028 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 029 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 030 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 031 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 032 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 033 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 034 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 035 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 036 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 037 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 038 | recon MSE 0.00073 | lr 1.00e-03\n",
      "[AE] Epoch 039 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 040 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 041 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 042 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 043 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 044 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 045 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 046 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 047 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 048 | recon MSE 0.00075 | lr 1.00e-03\n",
      "[AE] Epoch 049 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 050 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 051 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 052 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 053 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 054 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 055 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 056 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 057 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 058 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 059 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 060 | recon MSE 0.00067 | lr 1.00e-03\n",
      "After undersampling by dissimilarity: pos=470 neg=470 total=940\n",
      "[Fold 1] Ep 001 | train loss 0.6205 | val AP 0.5963 | lr 2.00e-03\n",
      "[Fold 1] Ep 002 | train loss 0.5200 | val AP 0.7713 | lr 2.00e-03\n",
      "[Fold 1] Ep 003 | train loss 0.4211 | val AP 0.7574 | lr 2.00e-03\n",
      "[Fold 1] Ep 004 | train loss 0.4217 | val AP 0.7933 | lr 2.00e-03\n",
      "[Fold 1] Ep 005 | train loss 0.3982 | val AP 0.6689 | lr 2.00e-03\n",
      "[Fold 1] Ep 006 | train loss 0.3207 | val AP 0.8975 | lr 2.00e-03\n",
      "[Fold 1] Ep 007 | train loss 0.2907 | val AP 0.8969 | lr 2.00e-03\n",
      "[Fold 1] Ep 008 | train loss 0.2472 | val AP 0.9106 | lr 2.00e-03\n",
      "[Fold 1] Ep 009 | train loss 0.2489 | val AP 0.9354 | lr 2.00e-03\n",
      "[Fold 1] Ep 010 | train loss 0.2339 | val AP 0.8895 | lr 2.00e-03\n",
      "[Fold 1] Ep 011 | train loss 0.2332 | val AP 0.8967 | lr 2.00e-03\n",
      "[Fold 1] Ep 012 | train loss 0.2252 | val AP 0.7558 | lr 2.00e-03\n",
      "[Fold 1] Ep 013 | train loss 0.2453 | val AP 0.9123 | lr 2.00e-03\n",
      "[Fold 1] Ep 014 | train loss 0.2401 | val AP 0.9654 | lr 2.00e-03\n",
      "[Fold 1] Ep 015 | train loss 0.1974 | val AP 0.8376 | lr 2.00e-03\n",
      "[Fold 1] Ep 016 | train loss 0.1850 | val AP 0.9307 | lr 2.00e-03\n",
      "[Fold 1] Ep 017 | train loss 0.1669 | val AP 0.9393 | lr 2.00e-03\n",
      "[Fold 1] Ep 018 | train loss 0.1693 | val AP 0.9491 | lr 2.00e-03\n",
      "[Fold 1] Ep 019 | train loss 0.1959 | val AP 0.9203 | lr 2.00e-03\n",
      "[Fold 1] Ep 020 | train loss 0.1731 | val AP 0.9290 | lr 1.00e-03\n",
      "[Fold 1] Ep 021 | train loss 0.1574 | val AP 0.9248 | lr 1.00e-03\n",
      "[Fold 1] Ep 022 | train loss 0.1380 | val AP 0.8954 | lr 1.00e-03\n",
      "[Fold 1] Ep 023 | train loss 0.1453 | val AP 0.9551 | lr 1.00e-03\n",
      "[Fold 1] Ep 024 | train loss 0.1138 | val AP 0.9586 | lr 1.00e-03\n",
      "[Fold 1] Ep 025 | train loss 0.1174 | val AP 0.9299 | lr 1.00e-03\n",
      "[Fold 1] Ep 026 | train loss 0.1051 | val AP 0.9462 | lr 5.00e-04\n",
      "[Fold 1] Ep 027 | train loss 0.1203 | val AP 0.9100 | lr 5.00e-04\n",
      "[Fold 1] Ep 028 | train loss 0.0894 | val AP 0.9630 | lr 5.00e-04\n",
      "[Fold 1] Ep 029 | train loss 0.0965 | val AP 0.9300 | lr 5.00e-04\n",
      "[Fold 1] Ep 030 | train loss 0.0944 | val AP 0.9238 | lr 5.00e-04\n",
      "[Fold 1] Early stop.\n",
      "[Fold 1] Test metrics: {'ROC-AUC': 0.9770258035310097, 'PRC-AUC': 0.9728953900674392, 'Accuracy': 0.973404255319149, 'F1': 0.9735449735449735, 'MCC': 0.9468620919467723, 'Recall': 0.9787234042553191, 'Precision': 0.968421052631579, 'False Positives': 3, 'False Positive Rate': np.float64(0.031914893617021274)}\n",
      "[Fold 2] Ep 001 | train loss 0.6179 | val AP 0.7839 | lr 2.00e-03\n",
      "[Fold 2] Ep 002 | train loss 0.4557 | val AP 0.6785 | lr 2.00e-03\n",
      "[Fold 2] Ep 003 | train loss 0.3698 | val AP 0.6986 | lr 2.00e-03\n",
      "[Fold 2] Ep 004 | train loss 0.3380 | val AP 0.7483 | lr 2.00e-03\n",
      "[Fold 2] Ep 005 | train loss 0.3044 | val AP 0.8321 | lr 2.00e-03\n",
      "[Fold 2] Ep 006 | train loss 0.2898 | val AP 0.5678 | lr 2.00e-03\n",
      "[Fold 2] Ep 007 | train loss 0.3532 | val AP 0.8186 | lr 2.00e-03\n",
      "[Fold 2] Ep 008 | train loss 0.2703 | val AP 0.8914 | lr 2.00e-03\n",
      "[Fold 2] Ep 009 | train loss 0.2464 | val AP 0.8041 | lr 2.00e-03\n",
      "[Fold 2] Ep 010 | train loss 0.2491 | val AP 0.8669 | lr 2.00e-03\n",
      "[Fold 2] Ep 011 | train loss 0.2129 | val AP 0.8535 | lr 2.00e-03\n",
      "[Fold 2] Ep 012 | train loss 0.2023 | val AP 0.8593 | lr 2.00e-03\n",
      "[Fold 2] Ep 013 | train loss 0.1865 | val AP 0.9025 | lr 2.00e-03\n",
      "[Fold 2] Ep 014 | train loss 0.2026 | val AP 0.8351 | lr 2.00e-03\n",
      "[Fold 2] Ep 015 | train loss 0.2075 | val AP 0.8738 | lr 2.00e-03\n",
      "[Fold 2] Ep 016 | train loss 0.1872 | val AP 0.8519 | lr 2.00e-03\n",
      "[Fold 2] Ep 017 | train loss 0.1965 | val AP 0.8916 | lr 2.00e-03\n",
      "[Fold 2] Ep 018 | train loss 0.1684 | val AP 0.9107 | lr 2.00e-03\n",
      "[Fold 2] Ep 019 | train loss 0.1539 | val AP 0.9063 | lr 2.00e-03\n",
      "[Fold 2] Ep 020 | train loss 0.1409 | val AP 0.9073 | lr 2.00e-03\n",
      "[Fold 2] Ep 021 | train loss 0.1416 | val AP 0.9037 | lr 2.00e-03\n",
      "[Fold 2] Ep 022 | train loss 0.1399 | val AP 0.8890 | lr 2.00e-03\n",
      "[Fold 2] Ep 023 | train loss 0.1259 | val AP 0.8713 | lr 2.00e-03\n",
      "[Fold 2] Ep 024 | train loss 0.1374 | val AP 0.8796 | lr 1.00e-03\n",
      "[Fold 2] Ep 025 | train loss 0.1184 | val AP 0.9154 | lr 1.00e-03\n",
      "[Fold 2] Ep 026 | train loss 0.1167 | val AP 0.8812 | lr 1.00e-03\n",
      "[Fold 2] Ep 027 | train loss 0.0906 | val AP 0.8944 | lr 1.00e-03\n",
      "[Fold 2] Ep 028 | train loss 0.0929 | val AP 0.9089 | lr 1.00e-03\n",
      "[Fold 2] Ep 029 | train loss 0.1000 | val AP 0.8956 | lr 1.00e-03\n",
      "[Fold 2] Ep 030 | train loss 0.0953 | val AP 0.9111 | lr 1.00e-03\n",
      "[Fold 2] Ep 031 | train loss 0.0998 | val AP 0.9254 | lr 1.00e-03\n",
      "[Fold 2] Ep 032 | train loss 0.1040 | val AP 0.9207 | lr 1.00e-03\n",
      "[Fold 2] Ep 033 | train loss 0.0925 | val AP 0.9315 | lr 1.00e-03\n",
      "[Fold 2] Ep 034 | train loss 0.0846 | val AP 0.9443 | lr 1.00e-03\n",
      "[Fold 2] Ep 035 | train loss 0.0788 | val AP 0.9091 | lr 1.00e-03\n",
      "[Fold 2] Ep 036 | train loss 0.0847 | val AP 0.9173 | lr 1.00e-03\n",
      "[Fold 2] Ep 037 | train loss 0.0692 | val AP 0.9165 | lr 1.00e-03\n",
      "[Fold 2] Ep 038 | train loss 0.0721 | val AP 0.9213 | lr 1.00e-03\n",
      "[Fold 2] Ep 039 | train loss 0.0772 | val AP 0.9037 | lr 1.00e-03\n",
      "[Fold 2] Ep 040 | train loss 0.0842 | val AP 0.9141 | lr 5.00e-04\n",
      "[Fold 2] Ep 041 | train loss 0.0722 | val AP 0.9307 | lr 5.00e-04\n",
      "[Fold 2] Ep 042 | train loss 0.0650 | val AP 0.9203 | lr 5.00e-04\n",
      "[Fold 2] Ep 043 | train loss 0.0737 | val AP 0.9041 | lr 5.00e-04\n",
      "[Fold 2] Ep 044 | train loss 0.0738 | val AP 0.9188 | lr 5.00e-04\n",
      "[Fold 2] Ep 045 | train loss 0.0647 | val AP 0.9265 | lr 5.00e-04\n",
      "[Fold 2] Ep 046 | train loss 0.0687 | val AP 0.9252 | lr 2.50e-04\n",
      "[Fold 2] Ep 047 | train loss 0.0600 | val AP 0.9227 | lr 2.50e-04\n",
      "[Fold 2] Ep 048 | train loss 0.0584 | val AP 0.9217 | lr 2.50e-04\n",
      "[Fold 2] Ep 049 | train loss 0.0505 | val AP 0.9240 | lr 2.50e-04\n",
      "[Fold 2] Early stop.\n",
      "[Fold 2] Test metrics: {'ROC-AUC': 0.9474875509280217, 'PRC-AUC': 0.9172489591048052, 'Accuracy': 0.9042553191489362, 'F1': 0.9032258064516129, 'MCC': 0.8086937042208112, 'Recall': 0.8936170212765957, 'Precision': 0.9130434782608695, 'False Positives': 8, 'False Positive Rate': np.float64(0.0851063829787234)}\n",
      "[Fold 3] Ep 001 | train loss 0.6469 | val AP 0.7994 | lr 2.00e-03\n",
      "[Fold 3] Ep 002 | train loss 0.5173 | val AP 0.9272 | lr 2.00e-03\n",
      "[Fold 3] Ep 003 | train loss 0.4365 | val AP 0.9622 | lr 2.00e-03\n",
      "[Fold 3] Ep 004 | train loss 0.3789 | val AP 0.8479 | lr 2.00e-03\n",
      "[Fold 3] Ep 005 | train loss 0.3284 | val AP 0.8497 | lr 2.00e-03\n",
      "[Fold 3] Ep 006 | train loss 0.3107 | val AP 0.7480 | lr 2.00e-03\n",
      "[Fold 3] Ep 007 | train loss 0.2866 | val AP 0.9704 | lr 2.00e-03\n",
      "[Fold 3] Ep 008 | train loss 0.2803 | val AP 0.9720 | lr 2.00e-03\n",
      "[Fold 3] Ep 009 | train loss 0.2413 | val AP 0.9797 | lr 2.00e-03\n",
      "[Fold 3] Ep 010 | train loss 0.2080 | val AP 0.9172 | lr 2.00e-03\n",
      "[Fold 3] Ep 011 | train loss 0.1837 | val AP 0.9790 | lr 2.00e-03\n",
      "[Fold 3] Ep 012 | train loss 0.2291 | val AP 0.9805 | lr 2.00e-03\n",
      "[Fold 3] Ep 013 | train loss 0.2136 | val AP 0.8595 | lr 2.00e-03\n",
      "[Fold 3] Ep 014 | train loss 0.1730 | val AP 0.9220 | lr 2.00e-03\n",
      "[Fold 3] Ep 015 | train loss 0.1895 | val AP 0.9684 | lr 2.00e-03\n",
      "[Fold 3] Ep 016 | train loss 0.1711 | val AP 0.9806 | lr 2.00e-03\n",
      "[Fold 3] Ep 017 | train loss 0.1762 | val AP 0.9838 | lr 2.00e-03\n",
      "[Fold 3] Ep 018 | train loss 0.1683 | val AP 0.9452 | lr 2.00e-03\n",
      "[Fold 3] Ep 019 | train loss 0.2082 | val AP 0.9379 | lr 2.00e-03\n",
      "[Fold 3] Ep 020 | train loss 0.2057 | val AP 0.9661 | lr 2.00e-03\n",
      "[Fold 3] Ep 021 | train loss 0.1301 | val AP 0.9946 | lr 2.00e-03\n",
      "[Fold 3] Ep 022 | train loss 0.1344 | val AP 0.9755 | lr 2.00e-03\n",
      "[Fold 3] Ep 023 | train loss 0.1497 | val AP 0.9660 | lr 2.00e-03\n",
      "[Fold 3] Ep 024 | train loss 0.1313 | val AP 0.9910 | lr 2.00e-03\n",
      "[Fold 3] Ep 025 | train loss 0.1636 | val AP 0.9904 | lr 2.00e-03\n",
      "[Fold 3] Ep 026 | train loss 0.1389 | val AP 0.9679 | lr 2.00e-03\n",
      "[Fold 3] Ep 027 | train loss 0.1267 | val AP 0.9905 | lr 1.00e-03\n",
      "[Fold 3] Ep 028 | train loss 0.1256 | val AP 0.9946 | lr 1.00e-03\n",
      "[Fold 3] Ep 029 | train loss 0.1056 | val AP 0.9891 | lr 1.00e-03\n",
      "[Fold 3] Ep 030 | train loss 0.1032 | val AP 0.9934 | lr 1.00e-03\n",
      "[Fold 3] Ep 031 | train loss 0.1112 | val AP 0.9947 | lr 1.00e-03\n",
      "[Fold 3] Ep 032 | train loss 0.1004 | val AP 0.9968 | lr 1.00e-03\n",
      "[Fold 3] Ep 033 | train loss 0.0905 | val AP 0.9987 | lr 1.00e-03\n",
      "[Fold 3] Ep 034 | train loss 0.1011 | val AP 0.9969 | lr 1.00e-03\n",
      "[Fold 3] Ep 035 | train loss 0.0766 | val AP 0.9962 | lr 1.00e-03\n",
      "[Fold 3] Ep 036 | train loss 0.1006 | val AP 0.9987 | lr 1.00e-03\n",
      "[Fold 3] Ep 037 | train loss 0.0850 | val AP 0.9844 | lr 1.00e-03\n",
      "[Fold 3] Ep 038 | train loss 0.0873 | val AP 0.9952 | lr 1.00e-03\n",
      "[Fold 3] Ep 039 | train loss 0.0828 | val AP 0.9944 | lr 5.00e-04\n",
      "[Fold 3] Ep 040 | train loss 0.0735 | val AP 0.9981 | lr 5.00e-04\n",
      "[Fold 3] Ep 041 | train loss 0.0822 | val AP 0.9975 | lr 5.00e-04\n",
      "[Fold 3] Ep 042 | train loss 0.0769 | val AP 0.9975 | lr 5.00e-04\n",
      "[Fold 3] Ep 043 | train loss 0.0738 | val AP 0.9987 | lr 5.00e-04\n",
      "[Fold 3] Ep 044 | train loss 0.0782 | val AP 0.9987 | lr 5.00e-04\n",
      "[Fold 3] Ep 045 | train loss 0.0783 | val AP 0.9980 | lr 2.50e-04\n",
      "[Fold 3] Ep 046 | train loss 0.0676 | val AP 0.9973 | lr 2.50e-04\n",
      "[Fold 3] Ep 047 | train loss 0.0682 | val AP 0.9980 | lr 2.50e-04\n",
      "[Fold 3] Ep 048 | train loss 0.0645 | val AP 0.9974 | lr 2.50e-04\n",
      "[Fold 3] Early stop.\n",
      "[Fold 3] Test metrics: {'ROC-AUC': 0.9945676776822092, 'PRC-AUC': 0.9947051299578287, 'Accuracy': 0.9361702127659575, 'F1': 0.9347826086956522, 'MCC': 0.8731313054999178, 'Recall': 0.9148936170212766, 'Precision': 0.9555555555555556, 'False Positives': 4, 'False Positive Rate': np.float64(0.0425531914893617)}\n",
      "[Fold 4] Ep 001 | train loss 0.6325 | val AP 0.6648 | lr 2.00e-03\n",
      "[Fold 4] Ep 002 | train loss 0.4976 | val AP 0.5457 | lr 2.00e-03\n",
      "[Fold 4] Ep 003 | train loss 0.3985 | val AP 0.8889 | lr 2.00e-03\n",
      "[Fold 4] Ep 004 | train loss 0.4418 | val AP 0.6717 | lr 2.00e-03\n",
      "[Fold 4] Ep 005 | train loss 0.3765 | val AP 0.8878 | lr 2.00e-03\n",
      "[Fold 4] Ep 006 | train loss 0.3757 | val AP 0.8715 | lr 2.00e-03\n",
      "[Fold 4] Ep 007 | train loss 0.3718 | val AP 0.8633 | lr 2.00e-03\n",
      "[Fold 4] Ep 008 | train loss 0.3150 | val AP 0.9478 | lr 2.00e-03\n",
      "[Fold 4] Ep 009 | train loss 0.3347 | val AP 0.8534 | lr 2.00e-03\n",
      "[Fold 4] Ep 010 | train loss 0.3044 | val AP 0.9329 | lr 2.00e-03\n",
      "[Fold 4] Ep 011 | train loss 0.2688 | val AP 0.8279 | lr 2.00e-03\n",
      "[Fold 4] Ep 012 | train loss 0.2451 | val AP 0.7980 | lr 2.00e-03\n",
      "[Fold 4] Ep 013 | train loss 0.2838 | val AP 0.8865 | lr 2.00e-03\n",
      "[Fold 4] Ep 014 | train loss 0.2373 | val AP 0.8161 | lr 1.00e-03\n",
      "[Fold 4] Ep 015 | train loss 0.2200 | val AP 0.9115 | lr 1.00e-03\n",
      "[Fold 4] Ep 016 | train loss 0.2121 | val AP 0.9051 | lr 1.00e-03\n",
      "[Fold 4] Ep 017 | train loss 0.1834 | val AP 0.8807 | lr 1.00e-03\n",
      "[Fold 4] Ep 018 | train loss 0.1723 | val AP 0.8937 | lr 1.00e-03\n",
      "[Fold 4] Ep 019 | train loss 0.1838 | val AP 0.8466 | lr 1.00e-03\n",
      "[Fold 4] Ep 020 | train loss 0.1558 | val AP 0.9435 | lr 5.00e-04\n",
      "[Fold 4] Ep 021 | train loss 0.1402 | val AP 0.8740 | lr 5.00e-04\n",
      "[Fold 4] Ep 022 | train loss 0.1534 | val AP 0.9041 | lr 5.00e-04\n",
      "[Fold 4] Ep 023 | train loss 0.1581 | val AP 0.9305 | lr 5.00e-04\n",
      "[Fold 4] Ep 024 | train loss 0.1428 | val AP 0.8696 | lr 5.00e-04\n",
      "[Fold 4] Ep 025 | train loss 0.1415 | val AP 0.9276 | lr 5.00e-04\n",
      "[Fold 4] Ep 026 | train loss 0.1211 | val AP 0.9357 | lr 2.50e-04\n",
      "[Fold 4] Ep 027 | train loss 0.1504 | val AP 0.9423 | lr 2.50e-04\n",
      "[Fold 4] Ep 028 | train loss 0.1022 | val AP 0.9179 | lr 2.50e-04\n",
      "[Fold 4] Ep 029 | train loss 0.1294 | val AP 0.9264 | lr 2.50e-04\n",
      "[Fold 4] Ep 030 | train loss 0.1256 | val AP 0.9385 | lr 2.50e-04\n",
      "[Fold 4] Early stop.\n",
      "[Fold 4] Test metrics: {'ROC-AUC': 0.9757808963331824, 'PRC-AUC': 0.9802304675259312, 'Accuracy': 0.925531914893617, 'F1': 0.925531914893617, 'MCC': 0.851063829787234, 'Recall': 0.925531914893617, 'Precision': 0.925531914893617, 'False Positives': 7, 'False Positive Rate': np.float64(0.07446808510638298)}\n",
      "[Fold 5] Ep 001 | train loss 0.6314 | val AP 0.5642 | lr 2.00e-03\n",
      "[Fold 5] Ep 002 | train loss 0.4965 | val AP 0.8496 | lr 2.00e-03\n",
      "[Fold 5] Ep 003 | train loss 0.3988 | val AP 0.8681 | lr 2.00e-03\n",
      "[Fold 5] Ep 004 | train loss 0.3279 | val AP 0.8434 | lr 2.00e-03\n",
      "[Fold 5] Ep 005 | train loss 0.3096 | val AP 0.7988 | lr 2.00e-03\n",
      "[Fold 5] Ep 006 | train loss 0.2813 | val AP 0.9040 | lr 2.00e-03\n",
      "[Fold 5] Ep 007 | train loss 0.2726 | val AP 0.7188 | lr 2.00e-03\n",
      "[Fold 5] Ep 008 | train loss 0.2736 | val AP 0.9375 | lr 2.00e-03\n",
      "[Fold 5] Ep 009 | train loss 0.2635 | val AP 0.9655 | lr 2.00e-03\n",
      "[Fold 5] Ep 010 | train loss 0.2413 | val AP 0.9651 | lr 2.00e-03\n",
      "[Fold 5] Ep 011 | train loss 0.2155 | val AP 0.8456 | lr 2.00e-03\n",
      "[Fold 5] Ep 012 | train loss 0.2023 | val AP 0.9781 | lr 2.00e-03\n",
      "[Fold 5] Ep 013 | train loss 0.1892 | val AP 0.9222 | lr 2.00e-03\n",
      "[Fold 5] Ep 014 | train loss 0.2101 | val AP 0.9382 | lr 2.00e-03\n",
      "[Fold 5] Ep 015 | train loss 0.1699 | val AP 0.9670 | lr 2.00e-03\n",
      "[Fold 5] Ep 016 | train loss 0.1549 | val AP 0.9770 | lr 2.00e-03\n",
      "[Fold 5] Ep 017 | train loss 0.1384 | val AP 0.7849 | lr 2.00e-03\n",
      "[Fold 5] Ep 018 | train loss 0.1445 | val AP 0.9762 | lr 1.00e-03\n",
      "[Fold 5] Ep 019 | train loss 0.1178 | val AP 0.9605 | lr 1.00e-03\n",
      "[Fold 5] Ep 020 | train loss 0.1098 | val AP 0.9307 | lr 1.00e-03\n",
      "[Fold 5] Ep 021 | train loss 0.0965 | val AP 0.9494 | lr 1.00e-03\n",
      "[Fold 5] Ep 022 | train loss 0.1185 | val AP 0.9792 | lr 1.00e-03\n",
      "[Fold 5] Ep 023 | train loss 0.0982 | val AP 0.9609 | lr 1.00e-03\n",
      "[Fold 5] Ep 024 | train loss 0.0889 | val AP 0.9490 | lr 1.00e-03\n",
      "[Fold 5] Ep 025 | train loss 0.0852 | val AP 0.9805 | lr 1.00e-03\n",
      "[Fold 5] Ep 026 | train loss 0.0735 | val AP 0.9679 | lr 1.00e-03\n",
      "[Fold 5] Ep 027 | train loss 0.0679 | val AP 0.9392 | lr 1.00e-03\n",
      "[Fold 5] Ep 028 | train loss 0.0822 | val AP 0.9609 | lr 1.00e-03\n",
      "[Fold 5] Ep 029 | train loss 0.0764 | val AP 0.9690 | lr 1.00e-03\n",
      "[Fold 5] Ep 030 | train loss 0.0713 | val AP 0.9386 | lr 1.00e-03\n",
      "[Fold 5] Ep 031 | train loss 0.0571 | val AP 0.9574 | lr 5.00e-04\n",
      "[Fold 5] Ep 032 | train loss 0.0552 | val AP 0.8981 | lr 5.00e-04\n",
      "[Fold 5] Ep 033 | train loss 0.0725 | val AP 0.9654 | lr 5.00e-04\n",
      "[Fold 5] Ep 034 | train loss 0.0410 | val AP 0.9758 | lr 5.00e-04\n",
      "[Fold 5] Ep 035 | train loss 0.0619 | val AP 0.9713 | lr 5.00e-04\n",
      "[Fold 5] Ep 036 | train loss 0.0411 | val AP 0.9698 | lr 5.00e-04\n",
      "[Fold 5] Ep 037 | train loss 0.0529 | val AP 0.9682 | lr 2.50e-04\n",
      "[Fold 5] Ep 038 | train loss 0.0387 | val AP 0.9615 | lr 2.50e-04\n",
      "[Fold 5] Ep 039 | train loss 0.0448 | val AP 0.9620 | lr 2.50e-04\n",
      "[Fold 5] Ep 040 | train loss 0.0376 | val AP 0.9702 | lr 2.50e-04\n",
      "[Fold 5] Early stop.\n",
      "[Fold 5] Test metrics: {'ROC-AUC': 0.9022181982797647, 'PRC-AUC': 0.8656641001698933, 'Accuracy': 0.8457446808510638, 'F1': 0.8512820512820513, 'MCC': 0.6934146987650405, 'Recall': 0.8829787234042553, 'Precision': 0.8217821782178217, 'False Positives': 18, 'False Positive Rate': np.float64(0.19148936170212766)}\n",
      "\n",
      "========== Overall (5-fold aggregated) ==========\n",
      "ROC-AUC: 0.956419\n",
      "PRC-AUC: 0.936068\n",
      "Accuracy: 0.917021\n",
      "F1: 0.917197\n",
      "MCC: 0.834050\n",
      "Recall: 0.919149\n",
      "Precision: 0.915254\n",
      "False Positives: 40\n",
      "False Positive Rate: 0.085106\n",
      "Model: GINE-Classifier (AE init)\n",
      "Best fold model saved to: best_gine_model.pth\n",
      "\n",
      "Per-fold metrics -> cv_per_fold.csv\n",
      "Overall metrics -> cv_results.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "GCN( GINE ) + Graph AutoEncoder 欠采样管线\n",
    "------------------------------------------------\n",
    "流程：\n",
    "1) RDKit 将 SMILES -> 分子图 (节点+边的丰富特征)\n",
    "2) 训练 Graph Autoencoder (重构节点特征的 MSE) 得到图级嵌入\n",
    "3) 基于嵌入，按“与正类差异最大(最不相似)”的原则，从负类中选出与正类 1:1 的样本\n",
    "4) 用选出来的 1:1 数据做 Stratified 5-Fold 训练 GINE-Classifier\n",
    "5) 评估并输出：ROC-AUC, PRC-AUC, Accuracy, F1, MCC, Recall, Precision, False Positives, False Positive Rate\n",
    "6) 将每折与总体结果保存到 CSV；保存最佳折模型权重\n",
    "\n",
    "依赖：\n",
    "- rdkit\n",
    "- torch, torch_geometric (>=2.2)\n",
    "- scikit-learn, pandas, numpy\n",
    "\n",
    "数据要求：CSV 至少包含两列：'smiles', 'antibiotic_activity'(0/1)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "\n",
    "# PyG\n",
    "from torch_geometric.data import Data\n",
    "try:\n",
    "    from torch_geometric.loader import DataLoader\n",
    "except Exception:\n",
    "    from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool, GINEConv, BatchNorm\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, f1_score,\n",
    "                             accuracy_score, matthews_corrcoef, precision_score,\n",
    "                             recall_score, confusion_matrix)\n",
    "\n",
    "# -----------------------------\n",
    "# 配置区（可以改）\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "DEFAULT_CSV = 'E:/SyntheMol-main/data/Data/1_training_data/raw_data.csv'\n",
    "RESULT_CSV = 'cv_results.csv'\n",
    "FOLD_DETAIL_CSV = 'cv_per_fold.csv'\n",
    "BEST_MODEL_PATH = 'best_gine_model.pth'\n",
    "EMBED_CSV = 'graph_embeddings.csv'\n",
    "\n",
    "MAX_WORKERS = min(os.cpu_count() or 0, 30)   # 兼容你的“≤30 CPU”的要求\n",
    "BATCH_SIZE_AE = 64\n",
    "BATCH_SIZE_CLS = 64\n",
    "EPOCHS_AE = 60\n",
    "EPOCHS_CLS = 120\n",
    "PATIENCE = 15\n",
    "LR_AE = 1e-3\n",
    "LR_CLS = 2e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "DROPOUT = 0.2\n",
    "HIDDEN = 128\n",
    "NUM_GINE = 3                   # 编码层数\n",
    "VAL_SPLIT = 0.1                # 训练折里再切出一点做早停\n",
    "DIST_METRIC = 'cosine'         # 欠采样相异度度量：'cosine' 或 'euclidean'\n",
    "NEG_POS_RATIO = 1.0            # 负:正=1:1\n",
    "\n",
    "# 常见元素表（原子序号），其余归入 \"other\"\n",
    "COMMON_Z = [1,5,6,7,8,9,14,15,16,17,19,11,12,20,26,29,30,35,53]  # H,B,C,N,O,F,Si,P,S,Cl,K,Na,Mg,Ca,Fe,Cu,Zn,Br,I\n",
    "\n",
    "# -----------------------------\n",
    "# 工具函数\n",
    "# -----------------------------\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def one_hot(val, choices):\n",
    "    v = [0] * len(choices)\n",
    "    if val in choices:\n",
    "        v[choices.index(val)] = 1\n",
    "    return v\n",
    "\n",
    "\n",
    "def atom_features(atom: Chem.rdchem.Atom) -> List[float]:\n",
    "    \"\"\"\n",
    "    常见、有效的节点特征（比单一原子序号显著更强）：\n",
    "    - 原子序号 one-hot（COMMON_Z + other）\n",
    "    - 度(degree 0-5) one-hot\n",
    "    - 杂化态 one-hot\n",
    "    - 形式电荷 [-2..2] one-hot\n",
    "    - 是否芳香、是否在环\n",
    "    - 全替代氢原子数(0..4) one-hot\n",
    "    - 手性（R/S/None） one-hot\n",
    "    \"\"\"\n",
    "    z = atom.GetAtomicNum()\n",
    "    z_onehot = one_hot(z if z in COMMON_Z else -1, COMMON_Z + [-1])\n",
    "\n",
    "    degree = atom.GetTotalDegree()\n",
    "    degree_onehot = one_hot(min(degree, 5), list(range(6)))\n",
    "\n",
    "    hyb = atom.GetHybridization()\n",
    "    hyb_choices = [\n",
    "        Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2\n",
    "    ]\n",
    "    hyb_onehot = one_hot(hyb if hyb in hyb_choices else None, hyb_choices+[None])\n",
    "\n",
    "    charge = int(atom.GetFormalCharge())\n",
    "    charge = max(-2, min(2, charge))\n",
    "    charge_onehot = one_hot(charge, [-2,-1,0,1,2])\n",
    "\n",
    "    num_h = min(atom.GetTotalNumHs(), 4)\n",
    "    num_h_onehot = one_hot(num_h, [0,1,2,3,4])\n",
    "\n",
    "    aromatic = [int(atom.GetIsAromatic())]\n",
    "    ring = [int(atom.IsInRing())]\n",
    "\n",
    "    chiral_tag = atom.GetChiralTag()\n",
    "    chiral_choices = [\n",
    "        Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW\n",
    "    ]\n",
    "    chiral_onehot = one_hot(chiral_tag if chiral_tag in chiral_choices else Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "                            chiral_choices)\n",
    "\n",
    "    return z_onehot + degree_onehot + hyb_onehot + charge_onehot + num_h_onehot + aromatic + ring + chiral_onehot\n",
    "\n",
    "\n",
    "def bond_features(bond: Chem.rdchem.Bond) -> List[float]:\n",
    "    \"\"\"\n",
    "    边特征：\n",
    "    - 键类型 one-hot：SINGLE/DOUBLE/TRIPLE/AROMATIC\n",
    "    - 共轭(conjugated)、在环(in ring)\n",
    "    - 立体化 one-hot（简化）\n",
    "    \"\"\"\n",
    "    bt = bond.GetBondType()\n",
    "    bt_choices = [Chem.BondType.SINGLE, Chem.BondType.DOUBLE,\n",
    "                  Chem.BondType.TRIPLE, Chem.BondType.AROMATIC]\n",
    "    bt_onehot = one_hot(bt if bt in bt_choices else None, bt_choices+[None])\n",
    "\n",
    "    conj = [int(bond.GetIsConjugated())]\n",
    "    ring = [int(bond.IsInRing())]\n",
    "\n",
    "    stereo = bond.GetStereo()\n",
    "    stereo_choices = [Chem.rdchem.BondStereo.STEREONONE,\n",
    "                      Chem.rdchem.BondStereo.STEREOZ,\n",
    "                      Chem.rdchem.BondStereo.STEREOE]\n",
    "    stereo_onehot = one_hot(stereo if stereo in stereo_choices else Chem.rdchem.BondStereo.STEREONONE,\n",
    "                            stereo_choices)\n",
    "\n",
    "    return bt_onehot + conj + ring + stereo_onehot\n",
    "\n",
    "\n",
    "def smiles_to_graph(smiles: str):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: \n",
    "        return None\n",
    "    Chem.Kekulize(mol, clearAromaticFlags=False)\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "\n",
    "    x = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        x.append(atom_features(atom))\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        f = bond_features(b)\n",
    "        edge_index.append([i, j]); edge_attr.append(f)\n",
    "        edge_index.append([j, i]); edge_attr.append(f)\n",
    "\n",
    "    if len(edge_index) == 0:\n",
    "        edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "        edge_attr = torch.empty((0, len(bond_features(Chem.MolFromSmiles('CC').GetBonds()[0]))), dtype=torch.float)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 模型\n",
    "# -----------------------------\n",
    "def mlp(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, out_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(out_dim, out_dim)\n",
    "    )\n",
    "\n",
    "\n",
    "class GINEEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, edge_dim, hidden=HIDDEN, num_layers=NUM_GINE, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 第一层\n",
    "        self.convs.append(GINEConv(mlp(in_dim, hidden), edge_dim=edge_dim))\n",
    "        self.bns.append(BatchNorm(hidden))\n",
    "        # 其余层\n",
    "        for _ in range(num_layers-1):\n",
    "            self.convs.append(GINEConv(mlp(hidden, hidden), edge_dim=edge_dim))\n",
    "            self.bns.append(BatchNorm(hidden))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        h = x\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            h_res = h\n",
    "            h = conv(h, edge_index, edge_attr)\n",
    "            h = bn(h)\n",
    "            h = F.relu(h)\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            # 轻微残差（维度一致时）\n",
    "            if h_res.shape == h.shape:\n",
    "                h = h + 0.1 * h_res\n",
    "        return h  # node embeddings\n",
    "\n",
    "\n",
    "class GraphAE(nn.Module):\n",
    "    \"\"\" 简单 Graph Autoencoder：编码 node -> H，解码重构 node 特征 \"\"\"\n",
    "    def __init__(self, in_dim, edge_dim, hidden=HIDDEN, num_layers=NUM_GINE, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.encoder = GINEEncoder(in_dim, edge_dim, hidden, num_layers, dropout)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, in_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        h = self.encoder(x, edge_index, edge_attr)         # [N, hidden]\n",
    "        x_hat = self.decoder(h)                             # [N, in_dim]\n",
    "        g = global_mean_pool(h, batch)                      # graph embedding\n",
    "        return x_hat, g\n",
    "\n",
    "    def encode_nodes(self, x, edge_index, edge_attr):\n",
    "        return self.encoder(x, edge_index, edge_attr)\n",
    "\n",
    "\n",
    "class GINEClassifier(nn.Module):\n",
    "    \"\"\" 图级分类器，可从 AE.encoder 初始化参数 \"\"\"\n",
    "    def __init__(self, in_dim, edge_dim, hidden=HIDDEN, num_layers=NUM_GINE, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.encoder = GINEEncoder(in_dim, edge_dim, hidden, num_layers, dropout)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        h = self.encoder(x, edge_index, edge_attr)         # [N, hidden]\n",
    "        g = global_mean_pool(h, batch)                     # [B, hidden]\n",
    "        logit = self.head(g).view(-1)\n",
    "        return logit\n",
    "\n",
    "    def load_from_ae(self, ae: GraphAE):\n",
    "        self.encoder.load_state_dict(ae.encoder.state_dict(), strict=False)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 训练 / 评估函数\n",
    "# -----------------------------\n",
    "def train_ae(ae, loader, device, epochs=EPOCHS_AE, lr=LR_AE, wd=WEIGHT_DECAY, patience=PATIENCE):\n",
    "    ae = ae.to(device)\n",
    "    opt = torch.optim.Adam(ae.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    # 兼容不同 torch 版本：有的没有 verbose 参数\n",
    "    try:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            opt, mode='min', factor=0.5, patience=5, verbose=False\n",
    "        )\n",
    "    except TypeError:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            opt, mode='min', factor=0.5, patience=5\n",
    "        )\n",
    "\n",
    "    best_loss, bad = float('inf'), 0\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        ae.train()\n",
    "        total = 0.0\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            x_hat, g = ae(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            loss = F.mse_loss(x_hat, data.x)               # 节点特征重构损失\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(ae.parameters(), 2.0)\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "\n",
    "        mean_loss = total / max(len(loader), 1)\n",
    "        # 按验证指标(这里用训练均值替代)调整学习率\n",
    "        scheduler.step(mean_loss)\n",
    "\n",
    "        # 兼容无 verbose 场景的简单日志\n",
    "        lr_now = opt.param_groups[0][\"lr\"]\n",
    "        print(f'[AE] Epoch {ep:03d} | recon MSE {mean_loss:.5f} | lr {lr_now:.2e}')\n",
    "\n",
    "        if mean_loss < best_loss - 1e-5:\n",
    "            best_loss, bad = mean_loss, 0\n",
    "            torch.save(ae.state_dict(), 'best_graph_ae.pth')\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience and ep >= 20:\n",
    "                print('[AE] Early stop.')\n",
    "                break\n",
    "\n",
    "    ae.load_state_dict(torch.load('best_graph_ae.pth', map_location=device))\n",
    "    ae.eval()\n",
    "    return ae\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_graph_embeddings(ae: GraphAE, loader, device) -> np.ndarray:\n",
    "    ae.eval()\n",
    "    embs = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        h = ae.encode_nodes(data.x, data.edge_index, data.edge_attr)\n",
    "        g = global_mean_pool(h, data.batch)                # [B, hidden]\n",
    "        embs.append(g.cpu().numpy())\n",
    "    return np.concatenate(embs, axis=0)\n",
    "\n",
    "\n",
    "def select_negatives_farthest(embs: np.ndarray, labels: np.ndarray,\n",
    "                              metric: str = DIST_METRIC, ratio: float = NEG_POS_RATIO) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    embs: [N, d] 图嵌入；labels: [N] in {0,1}\n",
    "    返回一个布尔索引，表示被选中的样本（全部正类 + 选出来的负类）\n",
    "    规则：以“与正类中心 的相似度最低(或距离最大)”来选负类；负样本数≈正样本数*ratio\n",
    "    \"\"\"\n",
    "    pos_idx = np.where(labels == 1)[0]\n",
    "    neg_idx = np.where(labels == 0)[0]\n",
    "    if len(pos_idx) == 0 or len(neg_idx) == 0:\n",
    "        return np.ones_like(labels, dtype=bool)\n",
    "\n",
    "    pos_centroid = embs[pos_idx].mean(axis=0, keepdims=True)  # [1,d]\n",
    "    neg_embs = embs[neg_idx]                                  # [K,d]\n",
    "\n",
    "    if metric == 'cosine':\n",
    "        # 相似度越小越“相异”\n",
    "        # cos = <a,b> / (||a||*||b||)\n",
    "        a = neg_embs / (np.linalg.norm(neg_embs, axis=1, keepdims=True) + 1e-9)\n",
    "        b = pos_centroid / (np.linalg.norm(pos_centroid, axis=1, keepdims=True) + 1e-9)\n",
    "        sim = (a @ b.T).reshape(-1)                           # [K]\n",
    "        order = np.argsort(sim)                               # 从小到大\n",
    "    elif metric == 'euclidean':\n",
    "        dist = np.linalg.norm(neg_embs - pos_centroid, axis=1)\n",
    "        order = np.argsort(-dist)                             # 从大到小\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'cosine' or 'euclidean'\")\n",
    "\n",
    "    k = int(round(len(pos_idx) * ratio))\n",
    "    chosen_neg = neg_idx[order[:k]]\n",
    "    mask = np.zeros_like(labels, dtype=bool)\n",
    "    mask[pos_idx] = True\n",
    "    mask[chosen_neg] = True\n",
    "    return mask\n",
    "\n",
    "\n",
    "def train_one_epoch_cls(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        y = data.y.view(-1).to(device)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "    return total / max(len(loader),1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(model, loader, device):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        logits = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        prob = torch.sigmoid(logits)\n",
    "        ys.append(data.y.view(-1).cpu().numpy())\n",
    "        ps.append(prob.cpu().numpy())\n",
    "    y_true = np.concatenate(ys) if ys else np.array([])\n",
    "    y_pred = np.concatenate(ps) if ps else np.array([])\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def calc_metrics(y_true, y_prob, threshold=0.5):\n",
    "    if y_true.size == 0:\n",
    "        return {k: float('nan') for k in\n",
    "                ['ROC-AUC','PRC-AUC','Accuracy','F1','MCC','Recall','Precision','False Positives','False Positive Rate']}\n",
    "    y_hat = (y_prob >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat, labels=[0,1]).ravel()\n",
    "    fpr = fp / max((fp + tn), 1)\n",
    "    out = {\n",
    "        'ROC-AUC': roc_auc_score(y_true, y_prob),\n",
    "        'PRC-AUC': average_precision_score(y_true, y_prob),\n",
    "        'Accuracy': accuracy_score(y_true, y_hat),\n",
    "        'F1': f1_score(y_true, y_hat, zero_division=0),\n",
    "        'MCC': matthews_corrcoef(y_true, y_hat),\n",
    "        'Recall': recall_score(y_true, y_hat, zero_division=0),\n",
    "        'Precision': precision_score(y_true, y_hat, zero_division=0),\n",
    "        'False Positives': int(fp),\n",
    "        'False Positive Rate': fpr,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 主流程\n",
    "# -----------------------------\n",
    "def main(args):\n",
    "    set_seed(SEED)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    # 1) 读数据\n",
    "    df = pd.read_csv(args.csv)\n",
    "    assert 'smiles' in df.columns and 'antibiotic_activity' in df.columns, \\\n",
    "        \"CSV 必须包含 'smiles' 与 'antibiotic_activity' 两列\"\n",
    "    smiles = df['smiles'].astype(str).tolist()\n",
    "    labels = df['antibiotic_activity'].astype(int).to_numpy()\n",
    "\n",
    "    # 2) 构图\n",
    "    data_list: List[Data] = []\n",
    "    drop_idx = []\n",
    "    for i, smi in enumerate(tqdm(smiles, desc='SMILES->Graph')):\n",
    "        g = smiles_to_graph(smi)\n",
    "        if g is None or g.x.numel() == 0:\n",
    "            drop_idx.append(i); continue\n",
    "        g.y = torch.tensor([labels[i]], dtype=torch.float32)\n",
    "        data_list.append(g)\n",
    "    if drop_idx:\n",
    "        print(f'Warning: {len(drop_idx)} SMILES 转换失败，已跳过。')\n",
    "        labels = np.delete(labels, drop_idx, axis=0)\n",
    "\n",
    "    # 维度信息\n",
    "    in_dim = data_list[0].x.size(1)\n",
    "    edge_dim = data_list[0].edge_attr.size(1) if data_list[0].edge_attr is not None else 0\n",
    "    print(f'Node feat dim = {in_dim} | Edge feat dim = {edge_dim} | N graphs = {len(data_list)}')\n",
    "\n",
    "    # 3) 训练 Graph AE（全部样本，不分标签）\n",
    "    ae_loader = DataLoader(data_list, batch_size=BATCH_SIZE_AE, shuffle=True, num_workers=0)\n",
    "    ae = GraphAE(in_dim, edge_dim, hidden=HIDDEN, num_layers=NUM_GINE, dropout=DROPOUT)\n",
    "    ae = train_ae(ae, ae_loader, device)\n",
    "\n",
    "    # 4) 提取图嵌入，并做“差异最大”的负类选择\n",
    "    eval_loader = DataLoader(data_list, batch_size=BATCH_SIZE_AE, shuffle=False, num_workers=0)\n",
    "    embs = get_graph_embeddings(ae, eval_loader, device)         # [N, hidden]\n",
    "    pd.DataFrame(embs).to_csv(EMBED_CSV, index=False)\n",
    "\n",
    "    mask = select_negatives_farthest(embs, labels, metric=DIST_METRIC, ratio=NEG_POS_RATIO)\n",
    "    data_balanced = [d for d, m in zip(data_list, mask) if m]\n",
    "    labels_balanced = labels[mask]\n",
    "    print(f'After undersampling by dissimilarity: pos={labels_balanced.sum()} '\n",
    "          f'neg={(1-labels_balanced).sum()} total={len(data_balanced)}')\n",
    "\n",
    "    # 5) 5-fold 训练分类器（encoder 从 AE 迁移初始化）\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    fold_metrics = []\n",
    "    all_true, all_prob = [], []\n",
    "    best_key, best_score = None, -1.0\n",
    "\n",
    "    for fold, (tr_idx, te_idx) in enumerate(skf.split(np.arange(len(data_balanced)), labels_balanced), 1):\n",
    "        train_subset = [data_balanced[i] for i in tr_idx]\n",
    "        test_subset  = [data_balanced[i] for i in te_idx]\n",
    "        y_train = labels_balanced[tr_idx]\n",
    "        y_test  = labels_balanced[te_idx]\n",
    "\n",
    "        # 再从训练里切出 VAL 做早停\n",
    "        tr_part, val_part, _, _ = train_test_split(\n",
    "            train_subset, y_train, test_size=VAL_SPLIT, stratify=y_train, random_state=SEED)\n",
    "\n",
    "        train_loader = DataLoader(tr_part, batch_size=BATCH_SIZE_CLS, shuffle=True,  num_workers=0)\n",
    "        val_loader   = DataLoader(val_part, batch_size=BATCH_SIZE_CLS, shuffle=False, num_workers=0)\n",
    "        test_loader  = DataLoader(test_subset, batch_size=BATCH_SIZE_CLS, shuffle=False, num_workers=0)\n",
    "\n",
    "        model = GINEClassifier(in_dim, edge_dim, hidden=HIDDEN, num_layers=NUM_GINE, dropout=DROPOUT).to(device)\n",
    "        model.load_from_ae(ae)  # 迁移初始化\n",
    "\n",
    "        # 正负不均衡依然可能存在（抽样后一般接近1:1），保险起见计算 pos_weight\n",
    "        pos_count = float((y_train==1).sum()); neg_count = float((y_train==0).sum())\n",
    "        pos_weight = torch.tensor([(neg_count / max(pos_count, 1.0))], device=device, dtype=torch.float32)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR_CLS, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "        best_val, bad = -1.0, 0\n",
    "        best_state = None\n",
    "\n",
    "        for ep in range(1, EPOCHS_CLS+1):\n",
    "            loss = train_one_epoch_cls(model, train_loader, optimizer, criterion, device)\n",
    "            yv, pv = infer(model, val_loader, device)\n",
    "            # 用 PRC-AUC 作为早停指标（在极不均衡时更鲁棒）\n",
    "            val_ap = average_precision_score(yv, pv) if yv.size > 0 else 0.0\n",
    "            scheduler.step(val_ap)\n",
    "            print(f'[Fold {fold}] Ep {ep:03d} | train loss {loss:.4f} | val AP {val_ap:.4f} | lr {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "\n",
    "            if val_ap > best_val + 1e-5:\n",
    "                best_val, bad = val_ap, 0\n",
    "                best_state = model.state_dict()\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= PATIENCE and ep >= 30:\n",
    "                    print(f'[Fold {fold}] Early stop.')\n",
    "                    break\n",
    "\n",
    "        # 加载最佳参数并测试\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        yt, pt = infer(model, test_loader, device)\n",
    "        all_true.append(yt); all_prob.append(pt)\n",
    "        m = calc_metrics(yt, pt, threshold=0.5)\n",
    "        m_row = {'Fold': fold, **m}\n",
    "        fold_metrics.append(m_row)\n",
    "        print(f'[Fold {fold}] Test metrics: {m}')\n",
    "\n",
    "        # 记录一个“最佳折”模型（以 ROC-AUC 决定）\n",
    "        if m['ROC-AUC'] > best_score:\n",
    "            best_score = m['ROC-AUC']; best_key = fold\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "\n",
    "    # 6) 汇总并保存结果\n",
    "    fold_df = pd.DataFrame(fold_metrics)\n",
    "    fold_df.to_csv(FOLD_DETAIL_CSV, index=False)\n",
    "\n",
    "    all_true = np.concatenate(all_true); all_prob = np.concatenate(all_prob)\n",
    "    overall = calc_metrics(all_true, all_prob, threshold=0.5)\n",
    "    overall['Model'] = 'GINE-Classifier (AE init)'\n",
    "    print('\\n========== Overall (5-fold aggregated) ==========')\n",
    "    for k,v in overall.items():\n",
    "        print(f'{k}: {v:.6f}' if isinstance(v, float) else f'{k}: {v}')\n",
    "    print(f'Best fold model saved to: {BEST_MODEL_PATH}')\n",
    "\n",
    "    # 也保存一份总表（便于你和其它模型对比拼表）\n",
    "    out_row = {\n",
    "        'Model': 'GINE(AE init)+Dissimilar undersampling',\n",
    "        **overall\n",
    "    }\n",
    "    pd.DataFrame([out_row]).to_csv(RESULT_CSV, index=False)\n",
    "    print(f'\\nPer-fold metrics -> {FOLD_DETAIL_CSV}')\n",
    "    print(f'Overall metrics -> {RESULT_CSV}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse, sys\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--csv', type=str, default=DEFAULT_CSV,\n",
    "                        help=\"包含 'smiles' 与 'antibiotic_activity' 的CSV路径\")\n",
    "    # 在 Notebook/IDE 等环境里会有多余的参数，使用 parse_known_args 忽略它们\n",
    "    args, _ = parser.parse_known_args()\n",
    "    # 也可以在这里打印一下以确认：\n",
    "    # print('Parsed args:', args)\n",
    "    main(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "英文版选nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SMILES->Graph: 100%|██████████| 13524/13524 [00:11<00:00, 1203.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feat dim = 47 | Edge feat dim = 10 | N graphs = 13524\n",
      "[AE] Epoch 001 | recon MSE 0.02076 | lr 1.00e-03\n",
      "[AE] Epoch 002 | recon MSE 0.00463 | lr 1.00e-03\n",
      "[AE] Epoch 003 | recon MSE 0.00259 | lr 1.00e-03\n",
      "[AE] Epoch 004 | recon MSE 0.00179 | lr 1.00e-03\n",
      "[AE] Epoch 005 | recon MSE 0.00137 | lr 1.00e-03\n",
      "[AE] Epoch 006 | recon MSE 0.00114 | lr 1.00e-03\n",
      "[AE] Epoch 007 | recon MSE 0.00101 | lr 1.00e-03\n",
      "[AE] Epoch 008 | recon MSE 0.00093 | lr 1.00e-03\n",
      "[AE] Epoch 009 | recon MSE 0.00088 | lr 1.00e-03\n",
      "[AE] Epoch 010 | recon MSE 0.00094 | lr 1.00e-03\n",
      "[AE] Epoch 011 | recon MSE 0.00084 | lr 1.00e-03\n",
      "[AE] Epoch 012 | recon MSE 0.00081 | lr 1.00e-03\n",
      "[AE] Epoch 013 | recon MSE 0.00085 | lr 1.00e-03\n",
      "[AE] Epoch 014 | recon MSE 0.00078 | lr 1.00e-03\n",
      "[AE] Epoch 015 | recon MSE 0.00076 | lr 1.00e-03\n",
      "[AE] Epoch 016 | recon MSE 0.00076 | lr 1.00e-03\n",
      "[AE] Epoch 017 | recon MSE 0.00076 | lr 1.00e-03\n",
      "[AE] Epoch 018 | recon MSE 0.00075 | lr 1.00e-03\n",
      "[AE] Epoch 019 | recon MSE 0.00074 | lr 1.00e-03\n",
      "[AE] Epoch 020 | recon MSE 0.00074 | lr 1.00e-03\n",
      "[AE] Epoch 021 | recon MSE 0.00086 | lr 1.00e-03\n",
      "[AE] Epoch 022 | recon MSE 0.00074 | lr 1.00e-03\n",
      "[AE] Epoch 023 | recon MSE 0.00072 | lr 1.00e-03\n",
      "[AE] Epoch 024 | recon MSE 0.00072 | lr 1.00e-03\n",
      "[AE] Epoch 025 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 026 | recon MSE 0.00072 | lr 1.00e-03\n",
      "[AE] Epoch 027 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 028 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 029 | recon MSE 0.00071 | lr 1.00e-03\n",
      "[AE] Epoch 030 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 031 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 032 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 033 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 034 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 035 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 036 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 037 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 038 | recon MSE 0.00073 | lr 1.00e-03\n",
      "[AE] Epoch 039 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 040 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 041 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 042 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 043 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 044 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 045 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 046 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 047 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 048 | recon MSE 0.00075 | lr 1.00e-03\n",
      "[AE] Epoch 049 | recon MSE 0.00070 | lr 1.00e-03\n",
      "[AE] Epoch 050 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 051 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 052 | recon MSE 0.00069 | lr 1.00e-03\n",
      "[AE] Epoch 053 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 054 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 055 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 056 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 057 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 058 | recon MSE 0.00068 | lr 1.00e-03\n",
      "[AE] Epoch 059 | recon MSE 0.00067 | lr 1.00e-03\n",
      "[AE] Epoch 060 | recon MSE 0.00067 | lr 1.00e-03\n",
      "After undersampling by 'nearest' (cosine) similarity: pos=470 neg=470 total=940\n",
      "[Fold 1] Ep 001 | train loss 0.6449 | val AP 0.9519 | lr 2.00e-03\n",
      "[Fold 1] Ep 002 | train loss 0.4425 | val AP 0.8957 | lr 2.00e-03\n",
      "[Fold 1] Ep 003 | train loss 0.3317 | val AP 0.9455 | lr 2.00e-03\n",
      "[Fold 1] Ep 004 | train loss 0.3259 | val AP 0.9562 | lr 2.00e-03\n",
      "[Fold 1] Ep 005 | train loss 0.2732 | val AP 0.9662 | lr 2.00e-03\n",
      "[Fold 1] Ep 006 | train loss 0.2855 | val AP 0.9499 | lr 2.00e-03\n",
      "[Fold 1] Ep 007 | train loss 0.2741 | val AP 0.8777 | lr 2.00e-03\n",
      "[Fold 1] Ep 008 | train loss 0.2363 | val AP 0.9364 | lr 2.00e-03\n",
      "[Fold 1] Ep 009 | train loss 0.2535 | val AP 0.9432 | lr 2.00e-03\n",
      "[Fold 1] Ep 010 | train loss 0.2505 | val AP 0.9241 | lr 2.00e-03\n",
      "[Fold 1] Ep 011 | train loss 0.2229 | val AP 0.9244 | lr 1.00e-03\n",
      "[Fold 1] Ep 012 | train loss 0.2201 | val AP 0.9361 | lr 1.00e-03\n",
      "[Fold 1] Ep 013 | train loss 0.1992 | val AP 0.9430 | lr 1.00e-03\n",
      "[Fold 1] Ep 014 | train loss 0.1986 | val AP 0.9497 | lr 1.00e-03\n",
      "[Fold 1] Ep 015 | train loss 0.1619 | val AP 0.9541 | lr 1.00e-03\n",
      "[Fold 1] Ep 016 | train loss 0.1580 | val AP 0.9542 | lr 1.00e-03\n",
      "[Fold 1] Ep 017 | train loss 0.1769 | val AP 0.9618 | lr 5.00e-04\n",
      "[Fold 1] Ep 018 | train loss 0.1558 | val AP 0.9534 | lr 5.00e-04\n",
      "[Fold 1] Ep 019 | train loss 0.1381 | val AP 0.9565 | lr 5.00e-04\n",
      "[Fold 1] Ep 020 | train loss 0.1336 | val AP 0.9567 | lr 5.00e-04\n",
      "[Fold 1] Ep 021 | train loss 0.1621 | val AP 0.9548 | lr 5.00e-04\n",
      "[Fold 1] Ep 022 | train loss 0.1483 | val AP 0.9522 | lr 5.00e-04\n",
      "[Fold 1] Ep 023 | train loss 0.1373 | val AP 0.9527 | lr 2.50e-04\n",
      "[Fold 1] Ep 024 | train loss 0.1340 | val AP 0.9570 | lr 2.50e-04\n",
      "[Fold 1] Ep 025 | train loss 0.1249 | val AP 0.9455 | lr 2.50e-04\n",
      "[Fold 1] Ep 026 | train loss 0.1189 | val AP 0.9559 | lr 2.50e-04\n",
      "[Fold 1] Ep 027 | train loss 0.1053 | val AP 0.9530 | lr 2.50e-04\n",
      "[Fold 1] Ep 028 | train loss 0.1253 | val AP 0.9569 | lr 2.50e-04\n",
      "[Fold 1] Ep 029 | train loss 0.1133 | val AP 0.9514 | lr 1.25e-04\n",
      "[Fold 1] Ep 030 | train loss 0.0999 | val AP 0.9554 | lr 1.25e-04\n",
      "[Fold 1] Early stop.\n",
      "[Fold 1] Test metrics: {'ROC-AUC': 0.9580126754187415, 'PRC-AUC': 0.9497681064353753, 'Accuracy': 0.9361702127659575, 'F1': 0.9361702127659575, 'MCC': 0.8723404255319149, 'Recall': 0.9361702127659575, 'Precision': 0.9361702127659575, 'False Positives': 6, 'False Positive Rate': np.float64(0.06382978723404255)}\n",
      "[Fold 2] Ep 001 | train loss 0.6479 | val AP 0.6986 | lr 2.00e-03\n",
      "[Fold 2] Ep 002 | train loss 0.4665 | val AP 0.8958 | lr 2.00e-03\n",
      "[Fold 2] Ep 003 | train loss 0.3352 | val AP 0.8958 | lr 2.00e-03\n",
      "[Fold 2] Ep 004 | train loss 0.2670 | val AP 0.9216 | lr 2.00e-03\n",
      "[Fold 2] Ep 005 | train loss 0.2964 | val AP 0.7691 | lr 2.00e-03\n",
      "[Fold 2] Ep 006 | train loss 0.3085 | val AP 0.9668 | lr 2.00e-03\n",
      "[Fold 2] Ep 007 | train loss 0.2368 | val AP 0.9019 | lr 2.00e-03\n",
      "[Fold 2] Ep 008 | train loss 0.2654 | val AP 0.9266 | lr 2.00e-03\n",
      "[Fold 2] Ep 009 | train loss 0.2212 | val AP 0.9488 | lr 2.00e-03\n",
      "[Fold 2] Ep 010 | train loss 0.2184 | val AP 0.8884 | lr 2.00e-03\n",
      "[Fold 2] Ep 011 | train loss 0.2155 | val AP 0.9496 | lr 2.00e-03\n",
      "[Fold 2] Ep 012 | train loss 0.2299 | val AP 0.9620 | lr 1.00e-03\n",
      "[Fold 2] Ep 013 | train loss 0.2006 | val AP 0.9610 | lr 1.00e-03\n",
      "[Fold 2] Ep 014 | train loss 0.2152 | val AP 0.9507 | lr 1.00e-03\n",
      "[Fold 2] Ep 015 | train loss 0.1580 | val AP 0.9728 | lr 1.00e-03\n",
      "[Fold 2] Ep 016 | train loss 0.1574 | val AP 0.9628 | lr 1.00e-03\n",
      "[Fold 2] Ep 017 | train loss 0.1728 | val AP 0.9623 | lr 1.00e-03\n",
      "[Fold 2] Ep 018 | train loss 0.1484 | val AP 0.9545 | lr 1.00e-03\n",
      "[Fold 2] Ep 019 | train loss 0.1634 | val AP 0.9594 | lr 1.00e-03\n",
      "[Fold 2] Ep 020 | train loss 0.1315 | val AP 0.9454 | lr 1.00e-03\n",
      "[Fold 2] Ep 021 | train loss 0.1371 | val AP 0.9691 | lr 5.00e-04\n",
      "[Fold 2] Ep 022 | train loss 0.1203 | val AP 0.9686 | lr 5.00e-04\n",
      "[Fold 2] Ep 023 | train loss 0.1161 | val AP 0.9587 | lr 5.00e-04\n",
      "[Fold 2] Ep 024 | train loss 0.1009 | val AP 0.9706 | lr 5.00e-04\n",
      "[Fold 2] Ep 025 | train loss 0.1031 | val AP 0.9647 | lr 5.00e-04\n",
      "[Fold 2] Ep 026 | train loss 0.1054 | val AP 0.9707 | lr 5.00e-04\n",
      "[Fold 2] Ep 027 | train loss 0.0870 | val AP 0.9718 | lr 2.50e-04\n",
      "[Fold 2] Ep 028 | train loss 0.0992 | val AP 0.9735 | lr 2.50e-04\n",
      "[Fold 2] Ep 029 | train loss 0.0931 | val AP 0.9731 | lr 2.50e-04\n",
      "[Fold 2] Ep 030 | train loss 0.0876 | val AP 0.9731 | lr 2.50e-04\n",
      "[Fold 2] Ep 031 | train loss 0.0803 | val AP 0.9739 | lr 2.50e-04\n",
      "[Fold 2] Ep 032 | train loss 0.0984 | val AP 0.9735 | lr 2.50e-04\n",
      "[Fold 2] Ep 033 | train loss 0.0874 | val AP 0.9708 | lr 2.50e-04\n",
      "[Fold 2] Ep 034 | train loss 0.0771 | val AP 0.9740 | lr 2.50e-04\n",
      "[Fold 2] Ep 035 | train loss 0.0799 | val AP 0.9754 | lr 2.50e-04\n",
      "[Fold 2] Ep 036 | train loss 0.0735 | val AP 0.9720 | lr 2.50e-04\n",
      "[Fold 2] Ep 037 | train loss 0.0951 | val AP 0.9715 | lr 2.50e-04\n",
      "[Fold 2] Ep 038 | train loss 0.0813 | val AP 0.9715 | lr 2.50e-04\n",
      "[Fold 2] Ep 039 | train loss 0.0810 | val AP 0.9706 | lr 2.50e-04\n",
      "[Fold 2] Ep 040 | train loss 0.0672 | val AP 0.9766 | lr 2.50e-04\n",
      "[Fold 2] Ep 041 | train loss 0.0739 | val AP 0.9719 | lr 2.50e-04\n",
      "[Fold 2] Ep 042 | train loss 0.0736 | val AP 0.9686 | lr 2.50e-04\n",
      "[Fold 2] Ep 043 | train loss 0.0969 | val AP 0.9736 | lr 2.50e-04\n",
      "[Fold 2] Ep 044 | train loss 0.0915 | val AP 0.9699 | lr 2.50e-04\n",
      "[Fold 2] Ep 045 | train loss 0.0668 | val AP 0.9707 | lr 2.50e-04\n",
      "[Fold 2] Ep 046 | train loss 0.0859 | val AP 0.9743 | lr 1.25e-04\n",
      "[Fold 2] Ep 047 | train loss 0.0716 | val AP 0.9723 | lr 1.25e-04\n",
      "[Fold 2] Ep 048 | train loss 0.0686 | val AP 0.9693 | lr 1.25e-04\n",
      "[Fold 2] Ep 049 | train loss 0.0688 | val AP 0.9707 | lr 1.25e-04\n",
      "[Fold 2] Ep 050 | train loss 0.0643 | val AP 0.9686 | lr 1.25e-04\n",
      "[Fold 2] Ep 051 | train loss 0.0749 | val AP 0.9709 | lr 1.25e-04\n",
      "[Fold 2] Ep 052 | train loss 0.0799 | val AP 0.9777 | lr 1.25e-04\n",
      "[Fold 2] Ep 053 | train loss 0.0682 | val AP 0.9776 | lr 1.25e-04\n",
      "[Fold 2] Ep 054 | train loss 0.0723 | val AP 0.9720 | lr 1.25e-04\n",
      "[Fold 2] Ep 055 | train loss 0.0771 | val AP 0.9699 | lr 1.25e-04\n",
      "[Fold 2] Ep 056 | train loss 0.0539 | val AP 0.9722 | lr 1.25e-04\n",
      "[Fold 2] Ep 057 | train loss 0.0827 | val AP 0.9748 | lr 1.25e-04\n",
      "[Fold 2] Ep 058 | train loss 0.0634 | val AP 0.9755 | lr 6.25e-05\n",
      "[Fold 2] Ep 059 | train loss 0.0796 | val AP 0.9743 | lr 6.25e-05\n",
      "[Fold 2] Ep 060 | train loss 0.0713 | val AP 0.9732 | lr 6.25e-05\n",
      "[Fold 2] Ep 061 | train loss 0.0738 | val AP 0.9731 | lr 6.25e-05\n",
      "[Fold 2] Ep 062 | train loss 0.0727 | val AP 0.9740 | lr 6.25e-05\n",
      "[Fold 2] Ep 063 | train loss 0.0596 | val AP 0.9749 | lr 6.25e-05\n",
      "[Fold 2] Ep 064 | train loss 0.0925 | val AP 0.9749 | lr 3.13e-05\n",
      "[Fold 2] Ep 065 | train loss 0.0839 | val AP 0.9739 | lr 3.13e-05\n",
      "[Fold 2] Ep 066 | train loss 0.0654 | val AP 0.9747 | lr 3.13e-05\n",
      "[Fold 2] Ep 067 | train loss 0.0687 | val AP 0.9747 | lr 3.13e-05\n",
      "[Fold 2] Early stop.\n",
      "[Fold 2] Test metrics: {'ROC-AUC': 0.9425079221367134, 'PRC-AUC': 0.954169996040974, 'Accuracy': 0.851063829787234, 'F1': 0.8372093023255814, 'MCC': 0.7125253031944253, 'Recall': 0.7659574468085106, 'Precision': 0.9230769230769231, 'False Positives': 6, 'False Positive Rate': np.float64(0.06382978723404255)}\n",
      "[Fold 3] Ep 001 | train loss 0.6285 | val AP 0.8954 | lr 2.00e-03\n",
      "[Fold 3] Ep 002 | train loss 0.4083 | val AP 0.9029 | lr 2.00e-03\n",
      "[Fold 3] Ep 003 | train loss 0.3126 | val AP 0.9378 | lr 2.00e-03\n",
      "[Fold 3] Ep 004 | train loss 0.3185 | val AP 0.8520 | lr 2.00e-03\n",
      "[Fold 3] Ep 005 | train loss 0.2617 | val AP 0.8503 | lr 2.00e-03\n",
      "[Fold 3] Ep 006 | train loss 0.3288 | val AP 0.8846 | lr 2.00e-03\n",
      "[Fold 3] Ep 007 | train loss 0.2449 | val AP 0.9154 | lr 2.00e-03\n",
      "[Fold 3] Ep 008 | train loss 0.2380 | val AP 0.9389 | lr 2.00e-03\n",
      "[Fold 3] Ep 009 | train loss 0.2591 | val AP 0.9077 | lr 2.00e-03\n",
      "[Fold 3] Ep 010 | train loss 0.2110 | val AP 0.9339 | lr 2.00e-03\n",
      "[Fold 3] Ep 011 | train loss 0.2153 | val AP 0.9544 | lr 2.00e-03\n",
      "[Fold 3] Ep 012 | train loss 0.2114 | val AP 0.9403 | lr 2.00e-03\n",
      "[Fold 3] Ep 013 | train loss 0.1849 | val AP 0.9577 | lr 2.00e-03\n",
      "[Fold 3] Ep 014 | train loss 0.1886 | val AP 0.9498 | lr 2.00e-03\n",
      "[Fold 3] Ep 015 | train loss 0.2290 | val AP 0.9305 | lr 2.00e-03\n",
      "[Fold 3] Ep 016 | train loss 0.1772 | val AP 0.9399 | lr 2.00e-03\n",
      "[Fold 3] Ep 017 | train loss 0.1899 | val AP 0.9475 | lr 2.00e-03\n",
      "[Fold 3] Ep 018 | train loss 0.1652 | val AP 0.9409 | lr 2.00e-03\n",
      "[Fold 3] Ep 019 | train loss 0.1560 | val AP 0.9158 | lr 1.00e-03\n",
      "[Fold 3] Ep 020 | train loss 0.1483 | val AP 0.9631 | lr 1.00e-03\n",
      "[Fold 3] Ep 021 | train loss 0.1763 | val AP 0.9424 | lr 1.00e-03\n",
      "[Fold 3] Ep 022 | train loss 0.1277 | val AP 0.9547 | lr 1.00e-03\n",
      "[Fold 3] Ep 023 | train loss 0.1404 | val AP 0.9562 | lr 1.00e-03\n",
      "[Fold 3] Ep 024 | train loss 0.1186 | val AP 0.9336 | lr 1.00e-03\n",
      "[Fold 3] Ep 025 | train loss 0.1177 | val AP 0.9502 | lr 1.00e-03\n",
      "[Fold 3] Ep 026 | train loss 0.1036 | val AP 0.9379 | lr 5.00e-04\n",
      "[Fold 3] Ep 027 | train loss 0.1100 | val AP 0.9503 | lr 5.00e-04\n",
      "[Fold 3] Ep 028 | train loss 0.1002 | val AP 0.9389 | lr 5.00e-04\n",
      "[Fold 3] Ep 029 | train loss 0.0851 | val AP 0.9344 | lr 5.00e-04\n",
      "[Fold 3] Ep 030 | train loss 0.1061 | val AP 0.9339 | lr 5.00e-04\n",
      "[Fold 3] Ep 031 | train loss 0.0906 | val AP 0.9401 | lr 5.00e-04\n",
      "[Fold 3] Ep 032 | train loss 0.0916 | val AP 0.9352 | lr 2.50e-04\n",
      "[Fold 3] Ep 033 | train loss 0.0765 | val AP 0.9404 | lr 2.50e-04\n",
      "[Fold 3] Ep 034 | train loss 0.0852 | val AP 0.9426 | lr 2.50e-04\n",
      "[Fold 3] Ep 035 | train loss 0.0980 | val AP 0.9401 | lr 2.50e-04\n",
      "[Fold 3] Early stop.\n",
      "[Fold 3] Test metrics: {'ROC-AUC': 0.9689904934359439, 'PRC-AUC': 0.9651921301716772, 'Accuracy': 0.9042553191489362, 'F1': 0.898876404494382, 'MCC': 0.8131249357707345, 'Recall': 0.851063829787234, 'Precision': 0.9523809523809523, 'False Positives': 4, 'False Positive Rate': np.float64(0.0425531914893617)}\n",
      "[Fold 4] Ep 001 | train loss 0.6299 | val AP 0.5181 | lr 2.00e-03\n",
      "[Fold 4] Ep 002 | train loss 0.4660 | val AP 0.9422 | lr 2.00e-03\n",
      "[Fold 4] Ep 003 | train loss 0.3805 | val AP 0.9369 | lr 2.00e-03\n",
      "[Fold 4] Ep 004 | train loss 0.3105 | val AP 0.8578 | lr 2.00e-03\n",
      "[Fold 4] Ep 005 | train loss 0.2739 | val AP 0.8848 | lr 2.00e-03\n",
      "[Fold 4] Ep 006 | train loss 0.2596 | val AP 0.9587 | lr 2.00e-03\n",
      "[Fold 4] Ep 007 | train loss 0.2675 | val AP 0.8680 | lr 2.00e-03\n",
      "[Fold 4] Ep 008 | train loss 0.2637 | val AP 0.9409 | lr 2.00e-03\n",
      "[Fold 4] Ep 009 | train loss 0.2488 | val AP 0.9552 | lr 2.00e-03\n",
      "[Fold 4] Ep 010 | train loss 0.2236 | val AP 0.9692 | lr 2.00e-03\n",
      "[Fold 4] Ep 011 | train loss 0.2158 | val AP 0.9710 | lr 2.00e-03\n",
      "[Fold 4] Ep 012 | train loss 0.2469 | val AP 0.9837 | lr 2.00e-03\n",
      "[Fold 4] Ep 013 | train loss 0.2113 | val AP 0.9335 | lr 2.00e-03\n",
      "[Fold 4] Ep 014 | train loss 0.2394 | val AP 0.9722 | lr 2.00e-03\n",
      "[Fold 4] Ep 015 | train loss 0.2164 | val AP 0.9496 | lr 2.00e-03\n",
      "[Fold 4] Ep 016 | train loss 0.1965 | val AP 0.9799 | lr 2.00e-03\n",
      "[Fold 4] Ep 017 | train loss 0.2052 | val AP 0.9064 | lr 2.00e-03\n",
      "[Fold 4] Ep 018 | train loss 0.2212 | val AP 0.9585 | lr 1.00e-03\n",
      "[Fold 4] Ep 019 | train loss 0.1870 | val AP 0.9884 | lr 1.00e-03\n",
      "[Fold 4] Ep 020 | train loss 0.1711 | val AP 0.9700 | lr 1.00e-03\n",
      "[Fold 4] Ep 021 | train loss 0.1850 | val AP 0.9933 | lr 1.00e-03\n",
      "[Fold 4] Ep 022 | train loss 0.1610 | val AP 0.9503 | lr 1.00e-03\n",
      "[Fold 4] Ep 023 | train loss 0.1529 | val AP 0.9849 | lr 1.00e-03\n",
      "[Fold 4] Ep 024 | train loss 0.1519 | val AP 0.9541 | lr 1.00e-03\n",
      "[Fold 4] Ep 025 | train loss 0.1495 | val AP 0.9944 | lr 1.00e-03\n",
      "[Fold 4] Ep 026 | train loss 0.1443 | val AP 0.9746 | lr 1.00e-03\n",
      "[Fold 4] Ep 027 | train loss 0.1511 | val AP 0.9702 | lr 1.00e-03\n",
      "[Fold 4] Ep 028 | train loss 0.1620 | val AP 0.9874 | lr 1.00e-03\n",
      "[Fold 4] Ep 029 | train loss 0.1457 | val AP 0.9859 | lr 1.00e-03\n",
      "[Fold 4] Ep 030 | train loss 0.1233 | val AP 0.9927 | lr 1.00e-03\n",
      "[Fold 4] Ep 031 | train loss 0.1362 | val AP 0.9877 | lr 5.00e-04\n",
      "[Fold 4] Ep 032 | train loss 0.1240 | val AP 0.9818 | lr 5.00e-04\n",
      "[Fold 4] Ep 033 | train loss 0.1074 | val AP 0.9803 | lr 5.00e-04\n",
      "[Fold 4] Ep 034 | train loss 0.1238 | val AP 0.9863 | lr 5.00e-04\n",
      "[Fold 4] Ep 035 | train loss 0.1027 | val AP 0.9877 | lr 5.00e-04\n",
      "[Fold 4] Ep 036 | train loss 0.1002 | val AP 0.9913 | lr 5.00e-04\n",
      "[Fold 4] Ep 037 | train loss 0.1048 | val AP 0.9903 | lr 2.50e-04\n",
      "[Fold 4] Ep 038 | train loss 0.0950 | val AP 0.9869 | lr 2.50e-04\n",
      "[Fold 4] Ep 039 | train loss 0.0827 | val AP 0.9861 | lr 2.50e-04\n",
      "[Fold 4] Ep 040 | train loss 0.0853 | val AP 0.9866 | lr 2.50e-04\n",
      "[Fold 4] Early stop.\n",
      "[Fold 4] Test metrics: {'ROC-AUC': 0.9701222272521502, 'PRC-AUC': 0.9781530793338535, 'Accuracy': 0.9521276595744681, 'F1': 0.9518716577540107, 'MCC': 0.9043064923087151, 'Recall': 0.9468085106382979, 'Precision': 0.956989247311828, 'False Positives': 4, 'False Positive Rate': np.float64(0.0425531914893617)}\n",
      "[Fold 5] Ep 001 | train loss 0.6603 | val AP 0.8641 | lr 2.00e-03\n",
      "[Fold 5] Ep 002 | train loss 0.4813 | val AP 0.9298 | lr 2.00e-03\n",
      "[Fold 5] Ep 003 | train loss 0.3483 | val AP 0.9404 | lr 2.00e-03\n",
      "[Fold 5] Ep 004 | train loss 0.3215 | val AP 0.8916 | lr 2.00e-03\n",
      "[Fold 5] Ep 005 | train loss 0.3043 | val AP 0.9043 | lr 2.00e-03\n",
      "[Fold 5] Ep 006 | train loss 0.2852 | val AP 0.9092 | lr 2.00e-03\n",
      "[Fold 5] Ep 007 | train loss 0.2710 | val AP 0.9453 | lr 2.00e-03\n",
      "[Fold 5] Ep 008 | train loss 0.2415 | val AP 0.9282 | lr 2.00e-03\n",
      "[Fold 5] Ep 009 | train loss 0.2744 | val AP 0.9708 | lr 2.00e-03\n",
      "[Fold 5] Ep 010 | train loss 0.2645 | val AP 0.7711 | lr 2.00e-03\n",
      "[Fold 5] Ep 011 | train loss 0.2150 | val AP 0.9639 | lr 2.00e-03\n",
      "[Fold 5] Ep 012 | train loss 0.2153 | val AP 0.9495 | lr 2.00e-03\n",
      "[Fold 5] Ep 013 | train loss 0.2098 | val AP 0.9575 | lr 2.00e-03\n",
      "[Fold 5] Ep 014 | train loss 0.2139 | val AP 0.9791 | lr 2.00e-03\n",
      "[Fold 5] Ep 015 | train loss 0.2011 | val AP 0.9495 | lr 2.00e-03\n",
      "[Fold 5] Ep 016 | train loss 0.1835 | val AP 0.9690 | lr 2.00e-03\n",
      "[Fold 5] Ep 017 | train loss 0.1751 | val AP 0.9476 | lr 2.00e-03\n",
      "[Fold 5] Ep 018 | train loss 0.1928 | val AP 0.9608 | lr 2.00e-03\n",
      "[Fold 5] Ep 019 | train loss 0.1756 | val AP 0.9670 | lr 2.00e-03\n",
      "[Fold 5] Ep 020 | train loss 0.1628 | val AP 0.9282 | lr 1.00e-03\n",
      "[Fold 5] Ep 021 | train loss 0.1531 | val AP 0.9388 | lr 1.00e-03\n",
      "[Fold 5] Ep 022 | train loss 0.1454 | val AP 0.9413 | lr 1.00e-03\n",
      "[Fold 5] Ep 023 | train loss 0.1364 | val AP 0.9387 | lr 1.00e-03\n",
      "[Fold 5] Ep 024 | train loss 0.1362 | val AP 0.9612 | lr 1.00e-03\n",
      "[Fold 5] Ep 025 | train loss 0.1378 | val AP 0.9533 | lr 1.00e-03\n",
      "[Fold 5] Ep 026 | train loss 0.1451 | val AP 0.9651 | lr 5.00e-04\n",
      "[Fold 5] Ep 027 | train loss 0.1177 | val AP 0.9539 | lr 5.00e-04\n",
      "[Fold 5] Ep 028 | train loss 0.1191 | val AP 0.9477 | lr 5.00e-04\n",
      "[Fold 5] Ep 029 | train loss 0.1196 | val AP 0.9636 | lr 5.00e-04\n",
      "[Fold 5] Ep 030 | train loss 0.1210 | val AP 0.9623 | lr 5.00e-04\n",
      "[Fold 5] Early stop.\n",
      "[Fold 5] Test metrics: {'ROC-AUC': 0.9660479855138072, 'PRC-AUC': 0.9723779608130719, 'Accuracy': 0.9148936170212766, 'F1': 0.9139784946236559, 'MCC': 0.8299751174897799, 'Recall': 0.9042553191489362, 'Precision': 0.9239130434782609, 'False Positives': 7, 'False Positive Rate': np.float64(0.07446808510638298)}\n",
      "\n",
      "========== Overall (5-fold aggregated) ==========\n",
      "ROC-AUC: 0.955220\n",
      "PRC-AUC: 0.960153\n",
      "Accuracy: 0.911702\n",
      "F1: 0.908891\n",
      "MCC: 0.824976\n",
      "Recall: 0.880851\n",
      "Precision: 0.938776\n",
      "False Positives: 27\n",
      "False Positive Rate: 0.057447\n",
      "Best fold model saved to: best_gine_model.pth\n",
      "\n",
      "Per-fold metrics -> cv_per_fold.csv\n",
      "Overall metrics -> cv_results.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "GINE + Graph Autoencoder (GAE) undersampling pipeline for molecular activity prediction.\n",
    "\n",
    "Pipeline\n",
    "--------\n",
    "1) RDKit: convert SMILES -> molecular graphs with rich node/edge features\n",
    "2) Train a Graph Autoencoder (reconstruct node features) to obtain graph embeddings\n",
    "3) Undersample negatives by similarity to the positive centroid in embedding space:\n",
    "   - 'nearest'  : pick the most similar negatives (smallest difference)  [DEFAULT]\n",
    "   - 'farthest' : pick the most dissimilar negatives (largest difference)\n",
    "   Supports cosine (default) or euclidean distance; negative:positive ~ 1:1 (configurable)\n",
    "4) Train a GINE-based graph classifier with Stratified 5-Fold cross-validation\n",
    "5) Report and save metrics per fold and overall:\n",
    "   ROC-AUC, PRC-AUC, Accuracy, F1, MCC, Recall, Precision, False Positives, False Positive Rate\n",
    "6) Save best-fold model weights and optional embeddings CSV\n",
    "\n",
    "Requirements\n",
    "------------\n",
    "- rdkit\n",
    "- torch, torch_geometric (>= 2.2 recommended)\n",
    "- scikit-learn, pandas, numpy, tqdm\n",
    "\n",
    "Input\n",
    "-----\n",
    "A CSV with at least two columns:\n",
    "- 'smiles' : SMILES string\n",
    "- 'antibiotic_activity' : binary label {0,1}\n",
    "\n",
    "Usage\n",
    "-----\n",
    "Command line:\n",
    "    python train_gcn_gae_pipeline.py --csv /path/to/data.csv \\\n",
    "        --pick nearest --metric cosine --ratio 1.0\n",
    "\n",
    "Jupyter / IDE:\n",
    "    This script uses `parse_known_args()` to ignore extra kernel args.\n",
    "    Alternatively, call:\n",
    "        from types import SimpleNamespace\n",
    "        main(SimpleNamespace(csv='/path/to/data.csv', pick='nearest', metric='cosine', ratio=1.0))\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# PyG\n",
    "from torch_geometric.data import Data\n",
    "try:\n",
    "    from torch_geometric.loader import DataLoader\n",
    "except Exception:  # backward compatibility with very old PyG versions\n",
    "    from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool, GINEConv, BatchNorm\n",
    "\n",
    "# RDKit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    matthews_corrcoef,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration (edit as needed)\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "DEFAULT_CSV = \"E:/SyntheMol-main/data/Data/1_training_data/raw_data.csv\"\n",
    "RESULT_CSV = \"cv_results.csv\"\n",
    "FOLD_DETAIL_CSV = \"cv_per_fold.csv\"\n",
    "BEST_MODEL_PATH = \"best_gine_model.pth\"\n",
    "EMBED_CSV = \"graph_embeddings.csv\"\n",
    "\n",
    "MAX_WORKERS = min(os.cpu_count() or 0, 30)  # keep <= 30 CPUs if you parallelize elsewhere\n",
    "BATCH_SIZE_AE = 64\n",
    "BATCH_SIZE_CLS = 64\n",
    "EPOCHS_AE = 60\n",
    "EPOCHS_CLS = 120\n",
    "PATIENCE = 15\n",
    "LR_AE = 1e-3\n",
    "LR_CLS = 2e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "DROPOUT = 0.2\n",
    "HIDDEN = 128\n",
    "NUM_GINE = 3                  # number of encoder layers\n",
    "VAL_SPLIT = 0.10              # split from the training fold for early stopping\n",
    "DIST_METRIC = \"cosine\"        # 'cosine' or 'euclidean' (default used if not overriden by CLI)\n",
    "NEG_POS_RATIO = 1.0           # negative:positive = 1:1 by default\n",
    "DEFAULT_PICK = \"nearest\"      # 'nearest' (most similar negatives) or 'farthest' (most dissimilar)\n",
    "\n",
    "# Common atomic numbers; everything else goes to \"other\"\n",
    "COMMON_Z = [1, 5, 6, 7, 8, 9, 14, 15, 16, 17, 19, 11, 12, 20, 26, 29, 30, 35, 53]\n",
    "# H,B,C,N,O,F,Si,P,S,Cl,K,Na,Mg,Ca,Fe,Cu,Zn,Br,I\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def set_seed(seed: int = SEED) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def one_hot(val, choices):\n",
    "    vec = [0] * len(choices)\n",
    "    if val in choices:\n",
    "        vec[choices.index(val)] = 1\n",
    "    return vec\n",
    "\n",
    "\n",
    "def atom_features(atom: Chem.rdchem.Atom) -> List[float]:\n",
    "    \"\"\"\n",
    "    Node features:\n",
    "      - atomic number: one-hot (COMMON_Z + 'other')\n",
    "      - degree: one-hot [0..5]\n",
    "      - hybridization: one-hot {sp, sp2, sp3, sp3d, sp3d2, other}\n",
    "      - formal charge: one-hot [-2..2]\n",
    "      - total hydrogens: one-hot [0..4]\n",
    "      - aromatic (bool)\n",
    "      - in ring (bool)\n",
    "      - chirality tag: one-hot {unspecified, CW, CCW}\n",
    "    \"\"\"\n",
    "    z = atom.GetAtomicNum()\n",
    "    z_onehot = one_hot(z if z in COMMON_Z else -1, COMMON_Z + [-1])\n",
    "\n",
    "    degree = atom.GetTotalDegree()\n",
    "    degree_onehot = one_hot(min(degree, 5), list(range(6)))\n",
    "\n",
    "    hyb = atom.GetHybridization()\n",
    "    hyb_choices = [\n",
    "        Chem.rdchem.HybridizationType.SP,\n",
    "        Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3,\n",
    "        Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2,\n",
    "    ]\n",
    "    hyb_onehot = one_hot(hyb if hyb in hyb_choices else None, hyb_choices + [None])\n",
    "\n",
    "    charge = int(atom.GetFormalCharge())\n",
    "    charge = max(-2, min(2, charge))\n",
    "    charge_onehot = one_hot(charge, [-2, -1, 0, 1, 2])\n",
    "\n",
    "    num_h = min(atom.GetTotalNumHs(), 4)\n",
    "    num_h_onehot = one_hot(num_h, [0, 1, 2, 3, 4])\n",
    "\n",
    "    aromatic = [int(atom.GetIsAromatic())]\n",
    "    ring = [int(atom.IsInRing())]\n",
    "\n",
    "    chiral_tag = atom.GetChiralTag()\n",
    "    chiral_choices = [\n",
    "        Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "    ]\n",
    "    chiral_onehot = one_hot(\n",
    "        chiral_tag if chiral_tag in chiral_choices else Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "        chiral_choices,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        z_onehot\n",
    "        + degree_onehot\n",
    "        + hyb_onehot\n",
    "        + charge_onehot\n",
    "        + num_h_onehot\n",
    "        + aromatic\n",
    "        + ring\n",
    "        + chiral_onehot\n",
    "    )\n",
    "\n",
    "\n",
    "def bond_features(bond: Chem.rdchem.Bond) -> List[float]:\n",
    "    \"\"\"\n",
    "    Edge features:\n",
    "      - bond type: one-hot {single,double,triple,aromatic,other}\n",
    "      - conjugated (bool)\n",
    "      - in ring (bool)\n",
    "      - stereo: one-hot {none, Z, E}\n",
    "    \"\"\"\n",
    "    bt = bond.GetBondType()\n",
    "    bt_choices = [\n",
    "        Chem.BondType.SINGLE,\n",
    "        Chem.BondType.DOUBLE,\n",
    "        Chem.BondType.TRIPLE,\n",
    "        Chem.BondType.AROMATIC,\n",
    "    ]\n",
    "    bt_onehot = one_hot(bt if bt in bt_choices else None, bt_choices + [None])\n",
    "\n",
    "    conj = [int(bond.GetIsConjugated())]\n",
    "    ring = [int(bond.IsInRing())]\n",
    "\n",
    "    stereo = bond.GetStereo()\n",
    "    stereo_choices = [\n",
    "        Chem.rdchem.BondStereo.STEREONONE,\n",
    "        Chem.rdchem.BondStereo.STEREOZ,\n",
    "        Chem.rdchem.BondStereo.STEREOE,\n",
    "    ]\n",
    "    stereo_onehot = one_hot(\n",
    "        stereo if stereo in stereo_choices else Chem.rdchem.BondStereo.STEREONONE, stereo_choices\n",
    "    )\n",
    "\n",
    "    return bt_onehot + conj + ring + stereo_onehot\n",
    "\n",
    "\n",
    "def smiles_to_graph(smiles: str):\n",
    "    \"\"\"Build a PyG `Data` object from a SMILES string.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    # Keep aromatic flags; 2D coords are sufficient for this pipeline\n",
    "    Chem.Kekulize(mol, clearAromaticFlags=False)\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "\n",
    "    # Node features\n",
    "    x = [atom_features(a) for a in mol.GetAtoms()]\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "    # Edges + edge features (bidirectional)\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        f = bond_features(b)\n",
    "        edge_index.append([i, j]); edge_attr.append(f)\n",
    "        edge_index.append([j, i]); edge_attr.append(f)\n",
    "\n",
    "    if len(edge_index) == 0:\n",
    "        # Rare edge-less molecule\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        # Build an empty edge_attr with correct feature width by probing a dummy bond\n",
    "        dummy = Chem.MolFromSmiles(\"CC\").GetBonds()[0]\n",
    "        feat_w = len(bond_features(dummy))\n",
    "        edge_attr = torch.empty((0, feat_w), dtype=torch.float)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "# -----------------------------\n",
    "# Models\n",
    "# -----------------------------\n",
    "def mlp(in_dim: int, out_dim: int) -> nn.Sequential:\n",
    "    return nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Linear(out_dim, out_dim))\n",
    "\n",
    "\n",
    "class GINEEncoder(nn.Module):\n",
    "    \"\"\"GINE encoder with BatchNorm, dropout and a light residual connection.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, edge_dim: int, hidden: int = HIDDEN, num_layers: int = NUM_GINE, dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # First layer\n",
    "        self.convs.append(GINEConv(mlp(in_dim, hidden), edge_dim=edge_dim))\n",
    "        self.bns.append(BatchNorm(hidden))\n",
    "\n",
    "        # Subsequent layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GINEConv(mlp(hidden, hidden), edge_dim=edge_dim))\n",
    "            self.bns.append(BatchNorm(hidden))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        h = x\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            h_res = h\n",
    "            h = conv(h, edge_index, edge_attr)\n",
    "            h = bn(h)\n",
    "            h = F.relu(h)\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "            # Tiny residual when shape matches\n",
    "            if h_res.shape == h.shape:\n",
    "                h = h + 0.1 * h_res\n",
    "        return h  # node embeddings\n",
    "\n",
    "\n",
    "class GraphAE(nn.Module):\n",
    "    \"\"\"Graph Autoencoder: node encoder -> reconstruct node features; returns graph embedding.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, edge_dim: int, hidden: int = HIDDEN, num_layers: int = NUM_GINE, dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.encoder = GINEEncoder(in_dim, edge_dim, hidden, num_layers, dropout)\n",
    "        self.decoder = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, in_dim))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        h = self.encoder(x, edge_index, edge_attr)  # [N, hidden]\n",
    "        x_hat = self.decoder(h)                     # [N, in_dim]\n",
    "        g = global_mean_pool(h, batch)              # [B, hidden] graph embedding\n",
    "        return x_hat, g\n",
    "\n",
    "    def encode_nodes(self, x, edge_index, edge_attr):\n",
    "        return self.encoder(x, edge_index, edge_attr)\n",
    "\n",
    "\n",
    "class GINEClassifier(nn.Module):\n",
    "    \"\"\"Graph-level classifier; encoder can be initialized from a trained AE.\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, edge_dim: int, hidden: int = HIDDEN, num_layers: int = NUM_GINE, dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.encoder = GINEEncoder(in_dim, edge_dim, hidden, num_layers, dropout)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        h = self.encoder(x, edge_index, edge_attr)\n",
    "        g = global_mean_pool(h, batch)\n",
    "        logit = self.head(g).view(-1)\n",
    "        return logit\n",
    "\n",
    "    def load_from_ae(self, ae: GraphAE):\n",
    "        self.encoder.load_state_dict(ae.encoder.state_dict(), strict=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Training & evaluation helpers\n",
    "# -----------------------------\n",
    "def train_ae(ae: GraphAE, loader: DataLoader, device, epochs: int = EPOCHS_AE, lr: float = LR_AE,\n",
    "             wd: float = WEIGHT_DECAY, patience: int = PATIENCE) -> GraphAE:\n",
    "    \"\"\"Train GraphAE with node feature reconstruction loss (MSE).\"\"\"\n",
    "    ae = ae.to(device)\n",
    "    opt = torch.optim.Adam(ae.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    # ReduceLROnPlateau: some torch versions don't support 'verbose'\n",
    "    try:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=5, verbose=False)\n",
    "    except TypeError:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=5)\n",
    "\n",
    "    best_loss, bad = float(\"inf\"), 0\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        ae.train()\n",
    "        total = 0.0\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            x_hat, _ = ae(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            loss = F.mse_loss(x_hat, data.x)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(ae.parameters(), 2.0)\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "\n",
    "        mean_loss = total / max(len(loader), 1)\n",
    "        scheduler.step(mean_loss)\n",
    "        lr_now = opt.param_groups[0][\"lr\"]\n",
    "        print(f\"[AE] Epoch {ep:03d} | recon MSE {mean_loss:.5f} | lr {lr_now:.2e}\")\n",
    "\n",
    "        if mean_loss < best_loss - 1e-5:\n",
    "            best_loss, bad = mean_loss, 0\n",
    "            torch.save(ae.state_dict(), \"best_graph_ae.pth\")\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience and ep >= 20:\n",
    "                print(\"[AE] Early stop.\")\n",
    "                break\n",
    "\n",
    "    ae.load_state_dict(torch.load(\"best_graph_ae.pth\", map_location=device))\n",
    "    ae.eval()\n",
    "    return ae\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_graph_embeddings(ae: GraphAE, loader: DataLoader, device) -> np.ndarray:\n",
    "    \"\"\"Return graph embeddings [N_graphs, hidden] from a trained AE encoder.\"\"\"\n",
    "    ae.eval()\n",
    "    embs = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        h = ae.encode_nodes(data.x, data.edge_index, data.edge_attr)\n",
    "        g = global_mean_pool(h, data.batch)  # [B, hidden]\n",
    "        embs.append(g.cpu().numpy())\n",
    "    return np.concatenate(embs, axis=0)\n",
    "\n",
    "\n",
    "def select_negatives_by_similarity(\n",
    "    embs: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    metric: str = DIST_METRIC,     # 'cosine' or 'euclidean'\n",
    "    ratio: float = NEG_POS_RATIO,  # negatives : positives\n",
    "    pick: str = DEFAULT_PICK       # 'nearest' (most similar) or 'farthest' (most dissimilar)\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Select negatives relative to the positive centroid in the embedding space.\n",
    "\n",
    "    Behavior\n",
    "    --------\n",
    "    - cosine:    higher similarity -> more similar; lower -> more dissimilar\n",
    "    - euclidean: smaller distance  -> more similar; larger -> more dissimilar\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mask : (N,) bool array indicating retained samples (all positives + selected negatives)\n",
    "    \"\"\"\n",
    "    assert pick in (\"nearest\", \"farthest\"), \"pick must be 'nearest' or 'farthest'\"\n",
    "\n",
    "    pos_idx = np.where(labels == 1)[0]\n",
    "    neg_idx = np.where(labels == 0)[0]\n",
    "    if len(pos_idx) == 0 or len(neg_idx) == 0:\n",
    "        return np.ones_like(labels, dtype=bool)\n",
    "\n",
    "    pos_centroid = embs[pos_idx].mean(axis=0, keepdims=True)  # [1, d]\n",
    "    neg_embs = embs[neg_idx]                                  # [K, d]\n",
    "\n",
    "    if metric == \"cosine\":\n",
    "        a = neg_embs / (np.linalg.norm(neg_embs, axis=1, keepdims=True) + 1e-9)\n",
    "        b = pos_centroid / (np.linalg.norm(pos_centroid, axis=1, keepdims=True) + 1e-9)\n",
    "        sim = (a @ b.T).reshape(-1)  # larger = more similar\n",
    "        if pick == \"nearest\":\n",
    "            order = np.argsort(-sim)   # descending  -> most similar first\n",
    "        else:\n",
    "            order = np.argsort(sim)    # ascending   -> most dissimilar first\n",
    "    elif metric == \"euclidean\":\n",
    "        dist = np.linalg.norm(neg_embs - pos_centroid, axis=1)  # smaller = more similar\n",
    "        if pick == \"nearest\":\n",
    "            order = np.argsort(dist)   # ascending  -> most similar first\n",
    "        else:\n",
    "            order = np.argsort(-dist)  # descending -> most dissimilar first\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'cosine' or 'euclidean'\")\n",
    "\n",
    "    k = int(round(len(pos_idx) * ratio))\n",
    "    chosen_neg = neg_idx[order[:k]]\n",
    "\n",
    "    mask = np.zeros_like(labels, dtype=bool)\n",
    "    mask[pos_idx] = True\n",
    "    mask[chosen_neg] = True\n",
    "    return mask\n",
    "\n",
    "\n",
    "def train_one_epoch_cls(model: nn.Module, loader: DataLoader, optimizer, criterion, device) -> float:\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        y = data.y.view(-1).to(device)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "    return total / max(len(loader), 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(model: nn.Module, loader: DataLoader, device):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        logits = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        prob = torch.sigmoid(logits)\n",
    "        ys.append(data.y.view(-1).cpu().numpy())\n",
    "        ps.append(prob.cpu().numpy())\n",
    "    y_true = np.concatenate(ys) if ys else np.array([])\n",
    "    y_pred = np.concatenate(ps) if ps else np.array([])\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def calc_metrics(y_true: np.ndarray, y_prob: np.ndarray, threshold: float = 0.5):\n",
    "    \"\"\"Return all required metrics, including FP and FPR.\"\"\"\n",
    "    if y_true.size == 0:\n",
    "        keys = [\n",
    "            \"ROC-AUC\", \"PRC-AUC\", \"Accuracy\", \"F1\", \"MCC\",\n",
    "            \"Recall\", \"Precision\", \"False Positives\", \"False Positive Rate\",\n",
    "        ]\n",
    "        return {k: float(\"nan\") for k in keys}\n",
    "\n",
    "    y_hat = (y_prob >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat, labels=[0, 1]).ravel()\n",
    "    fpr = fp / max((fp + tn), 1)\n",
    "\n",
    "    return {\n",
    "        \"ROC-AUC\": roc_auc_score(y_true, y_prob),\n",
    "        \"PRC-AUC\": average_precision_score(y_true, y_prob),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_hat),\n",
    "        \"F1\": f1_score(y_true, y_hat, zero_division=0),\n",
    "        \"MCC\": matthews_corrcoef(y_true, y_hat),\n",
    "        \"Recall\": recall_score(y_true, y_hat, zero_division=0),\n",
    "        \"Precision\": precision_score(y_true, y_hat, zero_division=0),\n",
    "        \"False Positives\": int(fp),\n",
    "        \"False Positive Rate\": fpr,\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main(args) -> None:\n",
    "    set_seed(SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 1) Load data\n",
    "    df = pd.read_csv(args.csv)\n",
    "    assert \"smiles\" in df.columns and \"antibiotic_activity\" in df.columns, \\\n",
    "        \"CSV must contain columns 'smiles' and 'antibiotic_activity'.\"\n",
    "    smiles = df[\"smiles\"].astype(str).tolist()\n",
    "    labels = df[\"antibiotic_activity\"].astype(int).to_numpy()\n",
    "\n",
    "    # 2) Build graphs\n",
    "    data_list: List[Data] = []\n",
    "    drop_idx = []\n",
    "    for i, smi in enumerate(tqdm(smiles, desc=\"SMILES->Graph\")):\n",
    "        g = smiles_to_graph(smi)\n",
    "        if g is None or g.x.numel() == 0:\n",
    "            drop_idx.append(i)\n",
    "            continue\n",
    "        g.y = torch.tensor([labels[i]], dtype=torch.float32)\n",
    "        data_list.append(g)\n",
    "\n",
    "    if len(data_list) == 0:\n",
    "        raise RuntimeError(\"No valid molecules after SMILES->graph conversion.\")\n",
    "\n",
    "    if drop_idx:\n",
    "        print(f\"Warning: {len(drop_idx)} SMILES failed to convert and were skipped.\")\n",
    "        labels = np.delete(labels, drop_idx, axis=0)\n",
    "\n",
    "    in_dim = data_list[0].x.size(1)\n",
    "    edge_dim = data_list[0].edge_attr.size(1) if data_list[0].edge_attr is not None else 0\n",
    "    print(f\"Node feat dim = {in_dim} | Edge feat dim = {edge_dim} | N graphs = {len(data_list)}\")\n",
    "\n",
    "    # 3) Train Graph AE on ALL samples\n",
    "    ae_loader = DataLoader(data_list, batch_size=BATCH_SIZE_AE, shuffle=True, num_workers=0)\n",
    "    ae = GraphAE(in_dim, edge_dim, hidden=HIDDEN, num_layers=NUM_GINE, dropout=DROPOUT)\n",
    "    ae = train_ae(ae, ae_loader, device)\n",
    "\n",
    "    # 4) Get embeddings and perform similarity-based undersampling\n",
    "    eval_loader = DataLoader(data_list, batch_size=BATCH_SIZE_AE, shuffle=False, num_workers=0)\n",
    "    embs = get_graph_embeddings(ae, eval_loader, device)  # [N, hidden]\n",
    "    pd.DataFrame(embs).to_csv(EMBED_CSV, index=False)\n",
    "\n",
    "    pick_mode = getattr(args, \"pick\", DEFAULT_PICK)\n",
    "    dist_metric = getattr(args, \"metric\", DIST_METRIC)\n",
    "    ratio = float(getattr(args, \"ratio\", NEG_POS_RATIO))\n",
    "\n",
    "    mask = select_negatives_by_similarity(\n",
    "        embs, labels, metric=dist_metric, ratio=ratio, pick=pick_mode\n",
    "    )\n",
    "    data_balanced = [d for d, m in zip(data_list, mask) if m]\n",
    "    labels_balanced = labels[mask]\n",
    "    print(\n",
    "        f\"After undersampling by '{pick_mode}' ({dist_metric}) similarity: \"\n",
    "        f\"pos={labels_balanced.sum()} neg={(1 - labels_balanced).sum()} total={len(data_balanced)}\"\n",
    "    )\n",
    "\n",
    "    # 5) 5-fold CV training\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    fold_metrics = []\n",
    "    all_true, all_prob = [], []\n",
    "    best_score = -1.0\n",
    "\n",
    "    for fold, (tr_idx, te_idx) in enumerate(skf.split(np.arange(len(data_balanced)), labels_balanced), 1):\n",
    "        train_subset = [data_balanced[i] for i in tr_idx]\n",
    "        test_subset = [data_balanced[i] for i in te_idx]\n",
    "        y_train = labels_balanced[tr_idx]\n",
    "        y_test = labels_balanced[te_idx]\n",
    "\n",
    "        # validation split from training fold for early stopping\n",
    "        tr_part, val_part, _, _ = train_test_split(\n",
    "            train_subset, y_train, test_size=VAL_SPLIT, stratify=y_train, random_state=SEED\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(tr_part, batch_size=BATCH_SIZE_CLS, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_part, batch_size=BATCH_SIZE_CLS, shuffle=False, num_workers=0)\n",
    "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE_CLS, shuffle=False, num_workers=0)\n",
    "\n",
    "        model = GINEClassifier(in_dim, edge_dim, hidden=HIDDEN, num_layers=NUM_GINE, dropout=DROPOUT).to(device)\n",
    "        model.load_from_ae(ae)\n",
    "\n",
    "        # class imbalance guard (should be close to 1:1 after undersampling)\n",
    "        pos_count = float((y_train == 1).sum())\n",
    "        neg_count = float((y_train == 0).sum())\n",
    "        pos_weight = torch.tensor([(neg_count / max(pos_count, 1.0))], device=device, dtype=torch.float32)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR_CLS, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=5)\n",
    "\n",
    "        best_val, bad = -1.0, 0\n",
    "        best_state = None\n",
    "\n",
    "        for ep in range(1, EPOCHS_CLS + 1):\n",
    "            loss = train_one_epoch_cls(model, train_loader, optimizer, criterion, device)\n",
    "            yv, pv = infer(model, val_loader, device)\n",
    "            val_ap = average_precision_score(yv, pv) if yv.size > 0 else 0.0  # PRC-AUC for early stopping\n",
    "            scheduler.step(val_ap)\n",
    "            print(\n",
    "                f\"[Fold {fold}] Ep {ep:03d} | train loss {loss:.4f} | val AP {val_ap:.4f} \"\n",
    "                f\"| lr {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "            )\n",
    "\n",
    "            if val_ap > best_val + 1e-5:\n",
    "                best_val, bad = val_ap, 0\n",
    "                best_state = model.state_dict()\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= PATIENCE and ep >= 30:\n",
    "                    print(f\"[Fold {fold}] Early stop.\")\n",
    "                    break\n",
    "\n",
    "        # test with best validation state\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "        yt, pt = infer(model, test_loader, device)\n",
    "        all_true.append(yt)\n",
    "        all_prob.append(pt)\n",
    "        m = calc_metrics(yt, pt, threshold=0.5)\n",
    "        fold_metrics.append({\"Fold\": fold, **m})\n",
    "        print(f\"[Fold {fold}] Test metrics: {m}\")\n",
    "\n",
    "        # keep best model by ROC-AUC\n",
    "        if m[\"ROC-AUC\"] > best_score:\n",
    "            best_score = m[\"ROC-AUC\"]\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "\n",
    "    # 6) Save per-fold and overall results\n",
    "    pd.DataFrame(fold_metrics).to_csv(FOLD_DETAIL_CSV, index=False)\n",
    "\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_prob = np.concatenate(all_prob)\n",
    "    overall_metrics = calc_metrics(all_true, all_prob, threshold=0.5)\n",
    "\n",
    "    print(\"\\n========== Overall (5-fold aggregated) ==========\")\n",
    "    for k, v in overall_metrics.items():\n",
    "        print(f\"{k}: {v:.6f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "    print(f\"Best fold model saved to: {BEST_MODEL_PATH}\")\n",
    "\n",
    "    # Single-row summary for easy comparison across models\n",
    "    out_row = {\n",
    "        \"Model\": f\"GINE(AE init) + {pick_mode} undersampling ({dist_metric}), ratio={ratio}\",\n",
    "        **overall_metrics,\n",
    "    }\n",
    "    pd.DataFrame([out_row]).to_csv(RESULT_CSV, index=False)\n",
    "    print(f\"\\nPer-fold metrics -> {FOLD_DETAIL_CSV}\")\n",
    "    print(f\"Overall metrics -> {RESULT_CSV}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--csv\",\n",
    "        type=str,\n",
    "        default=DEFAULT_CSV,\n",
    "        help=\"Path to CSV with columns 'smiles' and 'antibiotic_activity'.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pick\",\n",
    "        type=str,\n",
    "        choices=[\"nearest\", \"farthest\"],\n",
    "        default=DEFAULT_PICK,\n",
    "        help=\"Negative selection mode relative to positive centroid (default: nearest).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--metric\",\n",
    "        type=str,\n",
    "        choices=[\"cosine\", \"euclidean\"],\n",
    "        default=DIST_METRIC,\n",
    "        help=\"Similarity metric in embedding space (default: cosine).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ratio\",\n",
    "        type=float,\n",
    "        default=NEG_POS_RATIO,\n",
    "        help=\"Negative:positive ratio for undersampling (default: 1.0).\",\n",
    "    )\n",
    "    # In notebook/IDE environments extra args like --f=... may be injected.\n",
    "    args, _ = parser.parse_known_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[训练集] 原始正样本数=422, 负样本数=8595\n",
      "[训练集] 欠采样后负样本数=844, 总训练样本数=1266\n",
      "[测试集] 原始正样本=112, 负样本=5264\n",
      "[测试集] 筛选后(最近邻=正)的正样本数 = 16\n",
      "[测试集] 筛选后负样本数 = 32 (簇数=32)\n",
      "[测试集] 最终用于验证的样本数 = 48\n",
      "\n",
      ">>> Training model: LogisticRegression\n",
      "Best Params: {'C': 0.01}\n",
      "ROC-AUC: 0.826171875\n",
      "PRC-AUC: 0.7974591953277466\n",
      "Accuracy: 0.8333333333333334\n",
      "F1 Score: 0.6923076923076923\n",
      "MCC: 0.61665481259351\n",
      "Recall: 0.5625\n",
      "Precision: 0.9\n",
      "False Positives: 1, False Positive Rate: 0.03125\n",
      "\n",
      ">>> Training model: SVM\n",
      "Best Params: {'C': 1, 'kernel': 'rbf'}\n",
      "ROC-AUC: 0.857421875\n",
      "PRC-AUC: 0.8678075485024499\n",
      "Accuracy: 0.8125\n",
      "F1 Score: 0.6086956521739131\n",
      "MCC: 0.5843487097907776\n",
      "Recall: 0.4375\n",
      "Precision: 1.0\n",
      "False Positives: 0, False Positive Rate: 0.0\n",
      "\n",
      ">>> Training model: RandomForest\n",
      "Best Params: {'max_depth': None, 'n_estimators': 100}\n",
      "ROC-AUC: 0.8212890625\n",
      "PRC-AUC: 0.8127170396701646\n",
      "Accuracy: 0.8541666666666666\n",
      "F1 Score: 0.7407407407407407\n",
      "MCC: 0.6659496553711425\n",
      "Recall: 0.625\n",
      "Precision: 0.9090909090909091\n",
      "False Positives: 1, False Positive Rate: 0.03125\n",
      "\n",
      ">>> Training model: DecisionTree\n",
      "Best Params: {'max_depth': None, 'min_samples_split': 10}\n",
      "ROC-AUC: 0.666015625\n",
      "PRC-AUC: 0.5725747726626882\n",
      "Accuracy: 0.6875\n",
      "F1 Score: 0.6153846153846154\n",
      "MCC: 0.38334908600273254\n",
      "Recall: 0.75\n",
      "Precision: 0.5217391304347826\n",
      "False Positives: 11, False Positive Rate: 0.34375\n",
      "\n",
      ">>> Training model: KNN\n",
      "Best Params: {'n_neighbors': 5, 'weights': 'distance'}\n",
      "ROC-AUC: 0.865234375\n",
      "PRC-AUC: 0.8906322843822844\n",
      "Accuracy: 0.8333333333333334\n",
      "F1 Score: 0.6666666666666666\n",
      "MCC: 0.6324555320336759\n",
      "Recall: 0.5\n",
      "Precision: 1.0\n",
      "False Positives: 0, False Positive Rate: 0.0\n",
      "\n",
      ">>> Training model: XGBoost\n",
      "Best Params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 50}\n",
      "ROC-AUC: 0.82421875\n",
      "PRC-AUC: 0.7124051664511282\n",
      "Accuracy: 0.7291666666666666\n",
      "F1 Score: 0.38095238095238093\n",
      "MCC: 0.33756997551928847\n",
      "Recall: 0.25\n",
      "Precision: 0.8\n",
      "False Positives: 1, False Positive Rate: 0.03125\n",
      "\n",
      "所有模型训练和评估完成。结果已保存到 model_evaluation_results_final128.csv。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier  # 决策树\n",
    "from sklearn.neighbors import KNeighborsClassifier  # KNN\n",
    "from xgboost import XGBClassifier  # XGBoost\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============== 1. 读取训练集 (library_1 + library_2) 和 测试集 (library_3) ==============\n",
    "train_data_1 = pd.read_csv(\n",
    "    r'C:\\Users\\DuYih\\Desktop\\github\\DL Microbiology Antibiotics\\SyntheMol-main\\data\\Data\\1_training_data\\library_1_binarized.csv'\n",
    ")\n",
    "train_data_2 = pd.read_csv(\n",
    "    r'C:\\Users\\DuYih\\Desktop\\github\\DL Microbiology Antibiotics\\SyntheMol-main\\data\\Data\\1_training_data\\library_2_binarized.csv'\n",
    ")\n",
    "train_data = pd.concat([train_data_1, train_data_2], ignore_index=True)\n",
    "\n",
    "test_data = pd.read_csv(\n",
    "    r'C:\\Users\\DuYih\\Desktop\\github\\DL Microbiology Antibiotics\\SyntheMol-main\\data\\Data\\1_training_data\\library_3_binarized.csv'\n",
    ")\n",
    "\n",
    "# ============== 2. 如果需要，从 SMILES 计算 Morgan 指纹 ==============\n",
    "def get_fingerprint(smiles, n_bits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits)\n",
    "        return np.array(fp)\n",
    "    else:\n",
    "        return np.zeros(n_bits)\n",
    "\n",
    "# 如果 CSV 中有 'smiles' 列，就计算指纹(0/1)。否则可删除以下赋值。\n",
    "if 'smiles' in train_data.columns:\n",
    "    train_data['fingerprint'] = train_data['smiles'].apply(get_fingerprint)\n",
    "if 'smiles' in test_data.columns:\n",
    "    test_data['fingerprint'] = test_data['smiles'].apply(get_fingerprint)\n",
    "\n",
    "# ============== 3. 将训练集分为正负样本 ==============\n",
    "train_positive = train_data[train_data['antibiotic_activity'] == 1]\n",
    "train_negative = train_data[train_data['antibiotic_activity'] == 0]\n",
    "\n",
    "print(f\"[训练集] 原始正样本数={len(train_positive)}, 负样本数={len(train_negative)}\")\n",
    "\n",
    "# ============== 4. 对训练负样本做KMeans，欠采样 ==============\n",
    "#    簇数 = 2 × (训练正样本数)，选取各簇中心附近的负样本\n",
    "num_clusters_train_neg = 2 * len(train_positive)\n",
    "num_clusters_train_neg = min(num_clusters_train_neg, len(train_negative))  # 不超过负样本数\n",
    "num_clusters_train_neg = max(num_clusters_train_neg, 1)  # 至少1\n",
    "\n",
    "train_neg_fps = np.vstack(train_negative['fingerprint'].values)\n",
    "kmeans_train_neg = KMeans(n_clusters=num_clusters_train_neg, random_state=42)\n",
    "kmeans_train_neg.fit(train_neg_fps)\n",
    "\n",
    "closest_train_neg, _ = pairwise_distances_argmin_min(kmeans_train_neg.cluster_centers_, train_neg_fps)\n",
    "selected_train_negative = train_negative.iloc[closest_train_neg]\n",
    "\n",
    "final_train_data = pd.concat([train_positive, selected_train_negative], ignore_index=True)\n",
    "print(f\"[训练集] 欠采样后负样本数={len(selected_train_negative)}, 总训练样本数={len(final_train_data)}\")\n",
    "\n",
    "# ============== 5. 构建用于“最近邻”检索的训练集特征矩阵 ==============\n",
    "#    我们要对测试正样本做最近邻查询，看其最近的训练样本是正还是负\n",
    "train_fp_matrix = np.vstack(final_train_data['fingerprint'].values)\n",
    "train_labels = final_train_data['antibiotic_activity'].values\n",
    "\n",
    "# ============== 6. 测试集分为正负样本 ==============\n",
    "test_positive = test_data[test_data['antibiotic_activity'] == 1]\n",
    "test_negative = test_data[test_data['antibiotic_activity'] == 0]\n",
    "\n",
    "print(f\"[测试集] 原始正样本={len(test_positive)}, 负样本={len(test_negative)}\")\n",
    "\n",
    "# ============== 7. 筛选测试正样本：保持最近邻训练样本为正的测试样本 ==============\n",
    "def get_nearest_train_label(fp, X_train, y_train):\n",
    "    # 计算 fp 到 X_train 所有向量的欧氏距离，并找最小距离的index\n",
    "    dists = np.linalg.norm(X_train - fp, axis=1)\n",
    "    idx_min = np.argmin(dists)\n",
    "    return y_train[idx_min]\n",
    "\n",
    "selected_test_positive_list = []\n",
    "test_pos_fp = np.vstack(test_positive['fingerprint'].values) if len(test_positive)>0 else np.empty((0,))\n",
    "\n",
    "for i in range(len(test_positive)):\n",
    "    fp = test_pos_fp[i]\n",
    "    label_of_nearest = get_nearest_train_label(fp, train_fp_matrix, train_labels)\n",
    "    if label_of_nearest == 1:\n",
    "        # 最近邻在训练集中是正样本 => 保留\n",
    "        selected_test_positive_list.append(test_positive.iloc[i])\n",
    "        \n",
    "selected_test_positive = pd.DataFrame(selected_test_positive_list)\n",
    "print(f\"[测试集] 筛选后(最近邻=正)的正样本数 = {len(selected_test_positive)}\")\n",
    "\n",
    "# ============== 8. 筛选测试负样本：欠采样, 簇数= 2 × (测试正样本数) ==============\n",
    "final_test_pos_count = len(selected_test_positive)\n",
    "num_clusters_test_neg = 2 * final_test_pos_count\n",
    "num_clusters_test_neg = min(num_clusters_test_neg, len(test_negative))  # 不超过负样本数\n",
    "num_clusters_test_neg = max(num_clusters_test_neg, 1) if len(test_negative)>0 else 0\n",
    "\n",
    "selected_test_negative = pd.DataFrame(columns=test_negative.columns)\n",
    "\n",
    "if num_clusters_test_neg > 0:\n",
    "    test_neg_fps = np.vstack(test_negative['fingerprint'].values)\n",
    "    kmeans_test_neg = KMeans(n_clusters=num_clusters_test_neg, random_state=42)\n",
    "    kmeans_test_neg.fit(test_neg_fps)\n",
    "    \n",
    "    closest_test_neg, _ = pairwise_distances_argmin_min(kmeans_test_neg.cluster_centers_, test_neg_fps)\n",
    "    selected_test_negative = test_negative.iloc[closest_test_neg]\n",
    "\n",
    "print(f\"[测试集] 筛选后负样本数 = {len(selected_test_negative)} (簇数={num_clusters_test_neg})\")\n",
    "\n",
    "# ============== 9. 最终测试集 = 上述保留的正样本 + 筛选后的负样本 ==============\n",
    "final_test_data = pd.concat([selected_test_positive, selected_test_negative], ignore_index=True)\n",
    "print(f\"[测试集] 最终用于验证的样本数 = {len(final_test_data)}\")\n",
    "\n",
    "# ============== 10. 准备特征与标签，并做标准化 ==============\n",
    "X_train = np.vstack(final_train_data['fingerprint'].values)\n",
    "y_train = final_train_data['antibiotic_activity'].values\n",
    "\n",
    "if len(final_test_data) > 0:\n",
    "    X_test = np.vstack(final_test_data['fingerprint'].values)\n",
    "    y_test = final_test_data['antibiotic_activity'].values\n",
    "else:\n",
    "    X_test = np.empty((0, X_train.shape[1]))\n",
    "    y_test = np.array([])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test) if len(final_test_data) > 0 else X_test\n",
    "\n",
    "# ============== 11. 定义模型与超参数网格 + 训练 & 评估 ==============\n",
    "models_params = {\n",
    "    'LogisticRegression': (\n",
    "        LogisticRegression(max_iter=1000),\n",
    "        {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "    ),\n",
    "    'SVM': (\n",
    "        SVC(probability=True),\n",
    "        {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "    ),\n",
    "    'RandomForest': (\n",
    "        RandomForestClassifier(),\n",
    "        {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20]}\n",
    "    ),\n",
    "    'DecisionTree': (\n",
    "        DecisionTreeClassifier(),\n",
    "        {'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10]}\n",
    "    ),\n",
    "    'KNN': (\n",
    "        KNeighborsClassifier(),\n",
    "        {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}\n",
    "    ),\n",
    "    'XGBoost': (\n",
    "        XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        {'n_estimators': [50, 100], 'max_depth': [3, 6], 'learning_rate': [0.01, 0.1]}\n",
    "    )\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "if len(final_test_data) == 0:\n",
    "    print(\"\\n[警告] 最终测试集为空，无法评估模型！\")\n",
    "else:\n",
    "    for model_name, (model, param_grid) in models_params.items():\n",
    "        print(f\"\\n>>> Training model: {model_name}\")\n",
    "        clf = GridSearchCV(model, param_grid, scoring='roc_auc', cv=5)\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        best_model = clf.best_estimator_\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        \n",
    "        # 预测概率\n",
    "        if hasattr(best_model, \"predict_proba\"):\n",
    "            y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "        else:\n",
    "            decision_vals = best_model.decision_function(X_test_scaled)\n",
    "            # 归一化到[0,1]\n",
    "            y_pred_proba = (\n",
    "                (decision_vals - decision_vals.min()) / (decision_vals.max() - decision_vals.min())\n",
    "                if decision_vals.max() != decision_vals.min()\n",
    "                else np.zeros_like(decision_vals)\n",
    "            )\n",
    "        \n",
    "        # 计算指标\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba) if len(y_test)>0 else None\n",
    "        prec, rec, _ = precision_recall_curve(y_test, y_pred_proba) if len(y_test)>0 else (None, None, None)\n",
    "        prc_auc = auc(rec, prec) if prec is not None else None\n",
    "        accuracy = accuracy_score(y_test, y_pred) if len(y_test)>0 else None\n",
    "        f1 = f1_score(y_test, y_pred) if len(y_test)>0 else None\n",
    "        mcc = matthews_corrcoef(y_test, y_pred) if len(y_test)>0 else None\n",
    "        recall_val = recall_score(y_test, y_pred) if len(y_test)>0 else None\n",
    "        precision_val = precision_score(y_test, y_pred) if len(y_test)>0 else None\n",
    "        \n",
    "        if len(y_test) > 0:\n",
    "            tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "            fp_rate = fp / (fp + tn) if (fp + tn) else 0\n",
    "        else:\n",
    "            tn = fp = fn = tp = 0\n",
    "            fp_rate = None\n",
    "        \n",
    "        print(f\"Best Params: {clf.best_params_}\")\n",
    "        print(f\"ROC-AUC: {roc_auc}\")\n",
    "        print(f\"PRC-AUC: {prc_auc}\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "        print(f\"MCC: {mcc}\")\n",
    "        print(f\"Recall: {recall_val}\")\n",
    "        print(f\"Precision: {precision_val}\")\n",
    "        print(f\"False Positives: {fp}, False Positive Rate: {fp_rate}\")\n",
    "\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Best Params': clf.best_params_,\n",
    "            'ROC-AUC': roc_auc,\n",
    "            'PRC-AUC': prc_auc,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1 Score': f1,\n",
    "            'MCC': mcc,\n",
    "            'Recall': recall_val,\n",
    "            'Precision': precision_val,\n",
    "            'False Positives': fp,\n",
    "            'False Positive Rate': fp_rate\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('model_evaluation_results_final128.csv', index=False)\n",
    "print(\"\\n所有模型训练和评估完成。结果已保存到 model_evaluation_results_final128.csv。\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters_test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\tau = \\alpha\\times\\frac{a}{b}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
